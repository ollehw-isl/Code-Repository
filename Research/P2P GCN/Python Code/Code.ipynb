{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from scipy.sparse import csgraph\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from Code.Graph_composition import *\n",
    "from Code.Modeling import *\n",
    "import easydict\n",
    "from sklearn import metrics    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_dataset = pd.read_csv('Data/Sample_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset1 = sample_dataset.drop('loan_status', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_graph = Numerical_min_max(sample_dataset1)\n",
    "all_feature_graph_discreted = Discretization(all_feature_graph)\n",
    "all_feature_name, all_feature_distance_by_factor_exercise = Algo_distance(all_feature_graph_discreted)\n",
    "all_feature_weight_vector_exercise = Weight(all_feature_distance_by_factor_exercise)\n",
    "all_feature_distance_matrix_exercise = Distance(all_feature_graph, all_feature_weight_vector_exercise, all_feature_distance_by_factor_exercise, all_feature_name)\n",
    "all_feature_distance_matrix_exercise.tofile('all_feature_distance_matrix.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_feature_distance_matrix_exercise = np.fromfile('loan_feature_distance_matrix.dat', dtype=float)\n",
    "history_feature_distance_matrix_exercise = np.fromfile('history_feature_distance_matrix.dat', dtype=float)\n",
    "soft_feature_distance_matrix_exercise = np.fromfile('soft_feature_distance_matrix.dat', dtype=float)\n",
    "\n",
    "loan_feature_distance_matrix_exercise = loan_feature_distance_matrix_exercise.reshape(14000,14000)\n",
    "history_feature_distance_matrix_exercise = history_feature_distance_matrix_exercise.reshape(14000,14000)\n",
    "soft_feature_distance_matrix_exercise = soft_feature_distance_matrix_exercise.reshape(14000,14000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, cv = 1):\n",
    "        ### Train and Test indexing for Fully Paid Observations \n",
    "        total_Fully_paid_index = list(range(7000))\n",
    "        test_Fully_paid_index = list(range((cv-1)*1000, cv*1000))\n",
    "        train_Fully_paid_index = list(set(total_Fully_paid_index)-set(test_Fully_paid_index))\n",
    "        \n",
    "        ### Train and Test indexing for Defualt Observations\n",
    "        total_Default_index = list(range(7000,14000))\n",
    "        test_Default_index = list(range((7000 + (cv-1)*1000), (7000 + cv*1000)))\n",
    "        train_Default_index = list(set(total_Default_index)-set(test_Default_index))\n",
    "        \n",
    "        ### Train and Test dataset partition\n",
    "        train_dataset = sample_dataset.iloc[(train_Fully_paid_index + train_Default_index), :].reset_index(drop = True)\n",
    "        test_dataset = sample_dataset.iloc[(test_Fully_paid_index + test_Default_index), :].reset_index(drop = True)\n",
    "        \n",
    "        ### Train distance matrix\n",
    "        loan_feature_distance_matrix_train = loan_feature_distance_matrix_exercise[(train_Fully_paid_index + train_Default_index), :]\n",
    "        loan_feature_distance_matrix_train = loan_feature_distance_matrix_train[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        history_feature_distance_matrix_train = history_feature_distance_matrix_exercise[(train_Fully_paid_index + train_Default_index), :]\n",
    "        history_feature_distance_matrix_train = history_feature_distance_matrix_train[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        soft_feature_distance_matrix_train = soft_feature_distance_matrix_exercise[(train_Fully_paid_index + train_Default_index), :]\n",
    "        soft_feature_distance_matrix_train = soft_feature_distance_matrix_train[:, (train_Fully_paid_index + train_Default_index)]\n",
    " \n",
    "        ### Test distance matrix\n",
    "        loan_feature_distance_matrix_test = loan_feature_distance_matrix_exercise[(test_Fully_paid_index + test_Default_index), :]\n",
    "        loan_feature_distance_matrix_test = loan_feature_distance_matrix_test[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        history_feature_distance_matrix_test = history_feature_distance_matrix_exercise[(test_Fully_paid_index + test_Default_index), :]\n",
    "        history_feature_distance_matrix_test = history_feature_distance_matrix_test[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        soft_feature_distance_matrix_test = soft_feature_distance_matrix_exercise[(test_Fully_paid_index + test_Default_index), :]\n",
    "        soft_feature_distance_matrix_test = soft_feature_distance_matrix_test[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        \n",
    "        return(train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test) \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distance matrix to Adjancency matrix and Node feature matrix Generation\n",
    "def Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, bandwidth = 10):\n",
    "    ### Transformation for Train dataset\n",
    "    loan_feature_adjacency_matrix_train = Distance_Weight(loan_feature_distance_matrix_train, bandwidth)\n",
    "    history_feature_adjacency_matrix_train = Distance_Weight(history_feature_distance_matrix_train, bandwidth)\n",
    "    soft_feature_adjacency_matrix_train = Distance_Weight(soft_feature_distance_matrix_train, bandwidth)\n",
    "    \n",
    "    ### Transformation for Test dataset\n",
    "    loan_feature_adjacency_matrix_test = Distance_Weight_Test(loan_feature_distance_matrix_test, bandwidth)\n",
    "    history_feature_adjacency_matrix_test = Distance_Weight_Test(history_feature_distance_matrix_test, bandwidth)\n",
    "    soft_feature_adjacency_matrix_test = Distance_Weight_Test(soft_feature_distance_matrix_test, bandwidth)\n",
    "\n",
    "    ### Normalize for train dataset\n",
    "    train_loan_adj = normalize(loan_feature_adjacency_matrix_train + sp.eye(loan_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_history_adj = normalize(history_feature_adjacency_matrix_train + sp.eye(history_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_soft_adj = normalize(soft_feature_adjacency_matrix_train + sp.eye(soft_feature_adjacency_matrix_train.shape[0]))\n",
    "\n",
    "    ### Matrix generation for Test dataset\n",
    "    loan_feature_adjacency_matrix_test = np.concatenate([loan_feature_adjacency_matrix_train,loan_feature_adjacency_matrix_test])\n",
    "    history_feature_adjacency_matrix_test = np.concatenate([history_feature_adjacency_matrix_train,history_feature_adjacency_matrix_test])\n",
    "    soft_feature_adjacency_matrix_test = np.concatenate([soft_feature_adjacency_matrix_train,soft_feature_adjacency_matrix_test])\n",
    "\n",
    "    loan_feature_adjacency_matrix_test = np.concatenate((loan_feature_adjacency_matrix_test, np.zeros((loan_feature_adjacency_matrix_test.shape[0], loan_feature_distance_matrix_test.shape[0]))), axis = 1)\n",
    "    history_feature_adjacency_matrix_test = np.concatenate((history_feature_adjacency_matrix_test, np.zeros((history_feature_adjacency_matrix_test.shape[0], history_feature_distance_matrix_test.shape[0]))), axis = 1)\n",
    "    soft_feature_adjacency_matrix_test = np.concatenate((soft_feature_adjacency_matrix_test, np.zeros((soft_feature_adjacency_matrix_test.shape[0], soft_feature_distance_matrix_test.shape[0]))), axis = 1)\n",
    "\n",
    "    test_loan_adj = normalize(loan_feature_adjacency_matrix_test + sp.eye(loan_feature_adjacency_matrix_test.shape[0]))\n",
    "    test_history_adj = normalize(history_feature_adjacency_matrix_test + sp.eye(history_feature_adjacency_matrix_test.shape[0]))\n",
    "    test_soft_adj = normalize(soft_feature_adjacency_matrix_test + sp.eye(soft_feature_adjacency_matrix_test.shape[0]))\n",
    "\n",
    "\n",
    "    ### One-hot Encoding and Train X, Train Y   \n",
    "    temp_dummies = pd.get_dummies(train_dataset)\n",
    "    train_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "    base_category = temp_dummies.columns[[11, 13, 18, 19, 21, 24, 26, 29, 32, 34, 40, 43]]\n",
    "    train_x = Model_matrix(train_dataset, base_category)\n",
    "    train_x = train_x.drop('loan_status_Charged Off', axis = 1)\n",
    "\n",
    "    ### Train Node feature Matrix\n",
    "    loan_features = train_dataset.columns[0:7]\n",
    "    train_loan_features_dataset = train_dataset[loan_features] \n",
    "    history_features = train_dataset.columns[7:16]\n",
    "    train_history_features_dataset = train_dataset[history_features] \n",
    "    soft_features = train_dataset.columns[16:21]\n",
    "    train_soft_features_dataset = train_dataset[soft_features] \n",
    "    \n",
    "    loan_x = Model_matrix(train_loan_features_dataset, base_category)\n",
    "    history_x = Model_matrix(train_history_features_dataset, base_category)\n",
    "    soft_x = Model_matrix(train_soft_features_dataset, base_category)\n",
    "    DN_count_loan = Default_Neighbor_count(loan_feature_adjacency_matrix_train, train_y)\n",
    "    train_loan_node_feature = np.concatenate((DN_count_loan, loan_x), axis= 1)\n",
    "    DN_count_history = Default_Neighbor_count(history_feature_adjacency_matrix_train, train_y)\n",
    "    train_history_node_feature = np.concatenate((DN_count_history, history_x), axis= 1)\n",
    "    DN_count_soft = Default_Neighbor_count(soft_feature_adjacency_matrix_train, train_y)\n",
    "    train_soft_node_feature = np.concatenate((DN_count_soft, soft_x), axis= 1)\n",
    "\n",
    "    all_dataset = pd.concat([train_dataset, test_dataset])\n",
    "    all_dataset = all_dataset.reset_index()\n",
    "    all_dataset = all_dataset.drop(columns=['index'])\n",
    "\n",
    "    temp_dummies = pd.get_dummies(all_dataset)\n",
    "    test_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "\n",
    "    ### Test Node feature matrix\n",
    "    loan_features = all_dataset.columns[0:7]\n",
    "    test_loan_features_dataset = all_dataset[loan_features] \n",
    "    history_features = all_dataset.columns[7:16]\n",
    "    test_history_features_dataset = all_dataset[history_features] \n",
    "    soft_features = all_dataset.columns[16:21]\n",
    "    test_soft_features_dataset = all_dataset[soft_features] \n",
    "    \n",
    "    loan_x = Model_matrix(test_loan_features_dataset, base_category)\n",
    "    history_x = Model_matrix(test_history_features_dataset, base_category)\n",
    "    soft_x = Model_matrix(test_soft_features_dataset, base_category)\n",
    "    \n",
    "    DN_count_loan = Default_Neighbor_count(loan_feature_adjacency_matrix_test, test_y)\n",
    "    test_loan_node_feature = np.concatenate((DN_count_loan, loan_x), axis= 1)\n",
    "    \n",
    "    DN_count_history = Default_Neighbor_count(history_feature_adjacency_matrix_test, test_y)\n",
    "    test_history_node_feature = np.concatenate((DN_count_history, history_x), axis= 1)\n",
    "    \n",
    "    DN_count_soft = Default_Neighbor_count(soft_feature_adjacency_matrix_test, test_y)\n",
    "    test_soft_node_feature = np.concatenate((DN_count_soft, soft_x), axis= 1)\n",
    "    \n",
    "    return(train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics    \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from Code.Test_sampling import *\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 1)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft = Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loan_node_feature = torch.from_numpy(test_loan_node_feature).float().cuda()\n",
    "test_history_node_feature = torch.from_numpy(test_history_node_feature).float().cuda()\n",
    "test_soft_node_feature = torch.from_numpy(test_soft_node_feature).float().cuda()\n",
    "test_loan_adj = torch.from_numpy(test_loan_adj).float().cuda()\n",
    "test_history_adj = torch.from_numpy(test_history_adj).float().cuda()\n",
    "test_soft_adj = torch.from_numpy(test_soft_adj).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(47137)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 3210, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 2830 acc_train: 0.8071 Test set results: acc_test: 0.8105 F1_test: 0.8300 auc_test: 0.8815\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(97213)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 2646, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3377 acc_train: 0.8085 Test set results: acc_test: 0.8025 F1_test: 0.8228 auc_test: 0.8721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(214831)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 2264, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3808 acc_train: 0.8064 Test set results: acc_test: 0.8000 F1_test: 0.8236 auc_test: 0.8640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(8731147)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.001, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "\n",
    "### Epoch: 4783 acc_train: 0.8162 Test set results: acc_test: 0.7980 F1_test: 0.8233 auc_test: 0.8690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(722)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "#\n",
    "### Epoch: 1409 acc_train: 0.8008 Test set results: acc_test: 0.8125 F1_test: 0.8312 auc_test: 0.8804\n",
    "\n",
    "### Epoch: 2815 acc_train: 0.8033 Test set results: acc_test: 0.8095 F1_test: 0.8331 auc_test: 0.8815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17314)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 4065, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "\n",
    "### Epoch: 3299 acc_train: 0.8066 Test set results: acc_test: 0.7995 F1_test: 0.8209 auc_test: 0.8600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5122)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 3998, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code.Test_sampling import *\n",
    "train_x, train_y_save = X_Y_split(train_dataset)\n",
    "test_x, test_y_save = X_Y_split(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "train_x.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in train_x.columns.values]\n",
    "\n",
    "test_x.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in test_x.columns.values]\n",
    "train_x.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in train_x.columns.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(model, Test_x, Test_y):\n",
    "    pred = model.predict(Test_x)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Test_y, model.predict_proba(Test_x)[:, 1], pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print(\"Accuracy = {}\".format(metrics.accuracy_score(Test_y, pred)))\n",
    "    print(\"Precision = {}\".format(metrics.precision_score(Test_y, pred, pos_label=1)))\n",
    "    print(\"Recall = {}\".format(metrics.recall_score(Test_y, pred, pos_label=1)))\n",
    "    print(\"F1 score = {}\".format(metrics.f1_score(Test_y, pred, pos_label=1)))\n",
    "    print(\"AUC = {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=500, max_depth = 3, max_features = 4)\n",
    "rf_model.fit(train_x, train_y_save)\n",
    "Evaluation(rf_model, test_x, test_y_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(C = 3, gamma = 'scale', probability = True)\n",
    "svm_model.fit(train_x, train_y_save)\n",
    "Evaluation(svm_model, test_x, test_y_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(n_estimator = 500, learning_rate = 0.001, max_depth = 5, gamma = 0.1)\n",
    "xgb_model.fit(train_x, train_y_save)\n",
    "Evaluation(xgb_model, test_x, test_y_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 20, 20, 20, 20), random_state=9054, max_iter = 5000, early_stopping = True)\n",
    "mlp_model.fit(train_x, train_y_save)\n",
    "Evaluation(mlp_model, test_x, test_y_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 20, 20, 20, 20), random_state=54, max_iter = 5000, early_stopping = True)\n",
    "mlp_model.fit(train_x, train_y_save)\n",
    "Evaluation(mlp_model, test_x, test_y_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 20, 20, 20, 20), random_state=1254, max_iter = 5000, early_stopping = True)\n",
    "mlp_model.fit(train_x, train_y_save)\n",
    "Evaluation(mlp_model, test_x, test_y_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 20, 20, 20, 20), random_state=19127)\n",
    "mlp_model.fit(train_x, train_y_save)\n",
    "Evaluation(mlp_model, test_x, test_y_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, args, criterion):\n",
    "    t = time.time()\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(test_loan_node_feature[0:12000, :], test_history_node_feature[0:12000, :], test_soft_node_feature[0:12000, :], \n",
    "                  test_loan_adj[0:12000, 0:12000], test_history_adj[0:12000, 0:12000], test_soft_adj[0:12000, 0:12000])\n",
    "    loss = criterion(outputs, test_y[0:12000].float())\n",
    "    \n",
    "    pred = (outputs > 0.5).float()\n",
    "    correct = (pred.transpose(0,1) == test_y[0:12000].float()).float().sum()\n",
    "    acc_train = correct / len(test_y[0:12000])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    net.eval()\n",
    "    outputs = net(test_loan_node_feature, test_history_node_feature, test_soft_node_feature, \n",
    "                  test_loan_adj, test_history_adj, test_soft_adj)\n",
    "    outputs = outputs[range(12000,len(outputs))]\n",
    "\n",
    "    outputs_list = outputs.tolist()\n",
    "    test_y_for_test = test_y[range(12000,len(test_y))].float()\n",
    "    test_y_for_test_list = test_y_for_test.tolist()\n",
    "    loss_test = criterion(outputs, test_y_for_test)\n",
    "    pred = (outputs > 0.5).float()\n",
    "    correct = (pred.transpose(0,1) == test_y_for_test).float().sum()\n",
    "    acc_test = correct / len(test_y_for_test)\n",
    "    pred_list = pred.tolist()\n",
    "    f1_score = metrics.f1_score(test_y_for_test_list, pred_list, pos_label=1)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_y_for_test_list, outputs_list, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "  \n",
    "    outputs_list = outputs.tolist()\n",
    "    test_y_for_test = test_y[range(12000,len(test_y))].float()\n",
    "    test_y_for_test_list = test_y_for_test.tolist()\n",
    "    loss_test = criterion(outputs, test_y_for_test)\n",
    "    pred = (outputs > 0.5).float()\n",
    "    pred = pred.tolist()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_y_for_test_list, outputs_list, pos_label=1)\n",
    "  \n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          \"Test set results:\",\n",
    "          'acc: {:.4f}'.format(acc_test),\n",
    "          \"Pre = {}\".format(metrics.precision_score(test_y_for_test_list, pred, pos_label=1)),\n",
    "          \"Recall = {}\".format(metrics.recall_score(test_y_for_test_list, pred, pos_label=1)),\n",
    "          'F1: {:.4f}'.format(f1_score),\n",
    "          'auc: {:.4f}'.format(roc_auc))\n",
    "\n",
    "    \n",
    "def test():\n",
    "    net.eval()\n",
    "    outputs = net(test_loan_node_feature, test_history_node_feature, test_soft_node_feature, \n",
    "               test_loan_adj, test_history_adj, test_soft_adj)\n",
    " \n",
    "    outputs = outputs[range(12000,len(outputs))]\n",
    "\n",
    "    outputs_list = outputs.tolist()\n",
    "    test_y_for_test = test_y[range(12000,len(test_y))].float()\n",
    "    test_y_for_test_list = test_y_for_test.tolist()\n",
    "    loss_test = criterion(outputs, test_y_for_test)\n",
    "    pred = (outputs > 0.5).float()\n",
    "    pred = pred.tolist()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_y_for_test_list, outputs_list, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "  \n",
    "    print(\"Accuracy = {}\".format(metrics.accuracy_score(test_y_for_test_list, pred)))\n",
    "    print(\"Precision = {}\".format(metrics.precision_score(test_y_for_test_list, pred, pos_label=1)))\n",
    "    print(\"Recall = {}\".format(metrics.recall_score(test_y_for_test_list, pred, pos_label=1)))\n",
    "    print(\"F1 score = {}\".format(metrics.f1_score(test_y_for_test_list, pred, pos_label=1)))\n",
    "    print(\"AUC = {}\".format(roc_auc))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks_loan = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_loan.append(GCNBlock(args.n_layer,\n",
    "                                        9 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.blocks_history = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_history.append(GCNBlock(args.n_layer,\n",
    "                                        13 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.blocks_soft = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_soft.append(GCNBlock(args.n_layer,\n",
    "                                        12 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.pred1 = Classifier(3 * args.hidden_dim, 1,act = nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, loan_node_feature, history_node_feature, soft_node_feature, loan_adj, history_adj, soft_adj):\n",
    "        for i, block in enumerate(self.blocks_loan):\n",
    "            out1, loan_adj = block((loan_node_feature if i==0 else out1), loan_adj)\n",
    "        for i, block in enumerate(self.blocks_history):\n",
    "            out2, history_adj = block((history_node_feature if i==0 else out2), history_adj)\n",
    "        for i, block in enumerate(self.blocks_soft):\n",
    "            out3, soft_adj = block((soft_node_feature if i==0 else out3), soft_adj)\n",
    "        out4 = torch.cat([out1, out2, out3], dim = 1)\n",
    "        out = self.pred1(out4)\n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks_loan = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_loan.append(GCNBlock(args.n_layer,\n",
    "                                        9 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.blocks_history = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_history.append(GCNBlock(args.n_layer,\n",
    "                                        13 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.blocks_soft = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_soft.append(GCNBlock(args.n_layer,\n",
    "                                        12 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.pred1 = Classifier(3 * args.hidden_dim, args.pred_dim1,act = nn.ReLU())\n",
    "        self.pred2 = Classifier(args.pred_dim1, 1,act = nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, loan_node_feature, history_node_feature, soft_node_feature, loan_adj, history_adj, soft_adj):\n",
    "        for i, block in enumerate(self.blocks_loan):\n",
    "            out1, loan_adj = block((loan_node_feature if i==0 else out1), loan_adj)\n",
    "        for i, block in enumerate(self.blocks_history):\n",
    "            out2, history_adj = block((history_node_feature if i==0 else out2), history_adj)\n",
    "        for i, block in enumerate(self.blocks_soft):\n",
    "            out3, soft_adj = block((soft_node_feature if i==0 else out3), soft_adj)\n",
    "        out4 = torch.cat([out1, out2, out3], dim = 1)\n",
    "        out = self.pred1(out4)\n",
    "        out = self.pred2(out)\n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks_loan = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_loan.append(GCNBlock(args.n_layer,\n",
    "                                        9 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.blocks_history = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_history.append(GCNBlock(args.n_layer,\n",
    "                                        13 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.blocks_soft = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_soft.append(GCNBlock(args.n_layer,\n",
    "                                        12 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.pred1 = Classifier(3 * args.hidden_dim, args.pred_dim1,act = nn.ReLU())\n",
    "        self.pred2 = Classifier(args.pred_dim1, args.pred_dim2,act = nn.ReLU())\n",
    "        self.pred3 = Classifier(args.pred_dim2, 1,act = nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, loan_node_feature, history_node_feature, soft_node_feature, loan_adj, history_adj, soft_adj):\n",
    "        for i, block in enumerate(self.blocks_loan):\n",
    "            out1, loan_adj = block((loan_node_feature if i==0 else out1), loan_adj)\n",
    "        for i, block in enumerate(self.blocks_history):\n",
    "            out2, history_adj = block((history_node_feature if i==0 else out2), history_adj)\n",
    "        for i, block in enumerate(self.blocks_soft):\n",
    "            out3, soft_adj = block((soft_node_feature if i==0 else out3), soft_adj)\n",
    "        out4 = torch.cat([out1, out2, out3], dim = 1)\n",
    "        out = self.pred1(out4)\n",
    "        out = self.pred2(out)\n",
    "        out = self.pred3(out)\n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks_loan = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_loan.append(GCNBlock(args.n_layer,\n",
    "                                        9 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.blocks_history = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_history.append(GCNBlock(args.n_layer,\n",
    "                                        13 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.blocks_soft = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_soft.append(GCNBlock(args.n_layer,\n",
    "                                        12 if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.pred1 = Classifier(3 * args.hidden_dim, args.pred_dim1,act = nn.ReLU())\n",
    "        self.pred2 = Classifier(args.pred_dim1, args.pred_dim2, act = nn.ReLU())\n",
    "        self.pred3 = Classifier(args.pred_dim2, args.pred_dim3, act = nn.ReLU())\n",
    "        self.pred4 = Classifier(args.pred_dim3, 1,act = nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, loan_node_feature, history_node_feature, soft_node_feature, loan_adj, history_adj, soft_adj):\n",
    "        for i, block in enumerate(self.blocks_loan):\n",
    "            out1, loan_adj = block((loan_node_feature if i==0 else out1), loan_adj)\n",
    "        for i, block in enumerate(self.blocks_history):\n",
    "            out2, history_adj = block((history_node_feature if i==0 else out2), history_adj)\n",
    "        for i, block in enumerate(self.blocks_soft):\n",
    "            out3, soft_adj = block((soft_node_feature if i==0 else out3), soft_adj)\n",
    "        out4 = torch.cat([out1, out2, out3], dim = 1)\n",
    "        out = self.pred1(out4)\n",
    "        out = self.pred2(out)\n",
    "        out = self.pred3(out)\n",
    "        out = self.pred4(out)\n",
    "        \n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks_loan = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_loan.append(GCNBlock(args.n_layer,\n",
    "                                        9,\n",
    "                                        9,\n",
    "                                        9,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.blocks_history = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_history.append(GCNBlock(args.n_layer,\n",
    "                                        13,\n",
    "                                        13,\n",
    "                                        13,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.blocks_soft = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_soft.append(GCNBlock(args.n_layer,\n",
    "                                        12,\n",
    "                                        12,\n",
    "                                        12,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "        self.pred1 = Classifier(34, args.pred_dim1,act = nn.ReLU())\n",
    "        self.pred2 = Classifier(args.pred_dim1, args.pred_dim2, act = nn.ReLU())\n",
    "        self.pred3 = Classifier(args.pred_dim2, args.pred_dim3, act = nn.ReLU())\n",
    "        self.pred4 = Classifier(args.pred_dim3, 1,act = nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, loan_node_feature, history_node_feature, soft_node_feature, loan_adj, history_adj, soft_adj):\n",
    "        for i, block in enumerate(self.blocks_loan):\n",
    "            out1, loan_adj = block((loan_node_feature if i==0 else out1), loan_adj)\n",
    "        for i, block in enumerate(self.blocks_history):\n",
    "            out2, history_adj = block((history_node_feature if i==0 else out2), history_adj)\n",
    "        for i, block in enumerate(self.blocks_soft):\n",
    "            out3, soft_adj = block((soft_node_feature if i==0 else out3), soft_adj)\n",
    "        out4 = torch.cat([out1, out2, out3], dim = 1)\n",
    "        out = self.pred1(out4)\n",
    "        out = self.pred2(out)\n",
    "        out = self.pred3(out)\n",
    "        out = self.pred4(out)\n",
    "        \n",
    "        return out\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seed : 4199 -> Test Acc : 0.795, F1: 0.814 Auc : 0.864\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loan_node_feature = torch.from_numpy(test_loan_node_feature).float().cuda()\n",
    "test_history_node_feature = torch.from_numpy(test_history_node_feature).float().cuda()\n",
    "test_soft_node_feature = torch.from_numpy(test_soft_node_feature).float().cuda()\n",
    "test_loan_adj = torch.from_numpy(test_loan_adj).float().cuda()\n",
    "test_history_adj = torch.from_numpy(test_history_adj).float().cuda()\n",
    "test_soft_adj = torch.from_numpy(test_soft_adj).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CV 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(122147)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(484312384)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(214831)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 3808, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3808 acc_train: 0.8064 Test set results: acc_test: 0.8000 F1_test: 0.8236 auc_test: 0.8640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(8731147)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 4783, \"lr\": 0.001, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "\n",
    "### Epoch: 4783 acc_train: 0.8162 Test set results: acc_test: 0.7980 F1_test: 0.8233 auc_test: 0.8690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5871379)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 2316, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(62912271)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 3023, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3023 acc_train: 0.8077 Test set results: acc_test: 0.8150 F1_test: 0.8369 auc_test: 0.8836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17314)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 3299, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "\n",
    "### Epoch: 3299 acc_train: 0.8066 Test set results: acc_test: 0.7995 F1_test: 0.8209 auc_test: 0.8600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5122)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 3792, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(122147)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(233258)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(391814)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(488147)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(522122)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(695122)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(724414)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(822222)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(914781)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2338471)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3981472)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4813579)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(9182347)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7513587)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(821357138)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(821357138)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(187318)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(348731)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(513843)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(683184)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(219831218)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(484312384)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6843123157)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(86313185131)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(997312584)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"hidden_dim\": 16 , \"pred_dim1\": 16 , \"pred_dim2\": 24, \"pred_dim3\": 8,\n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 3792 acc_train: 0.8101 Test set results: acc_test: 0.7915 F1_test: 0.8131 auc_test: 0.8731"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All node feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = pd.read_csv('Data/Sample_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_distance_matrix_exercise = np.fromfile('all_feature_distance_matrix.dat', dtype=float)\n",
    "all_feature_distance_matrix_exercise = all_feature_distance_matrix_exercise.reshape(14000,14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_dataset_Extraction(sample_dataset, all_feature_distance_matrix_exercise, cv = 1):\n",
    "        ### Train and Test indexing for Fully Paid Observations \n",
    "        total_Fully_paid_index = list(range(7000))\n",
    "        test_Fully_paid_index = list(range((cv-1)*1000, cv*1000))\n",
    "        train_Fully_paid_index = list(set(total_Fully_paid_index)-set(test_Fully_paid_index))\n",
    "        \n",
    "        ### Train and Test indexing for Defualt Observations\n",
    "        total_Default_index = list(range(7000,14000))\n",
    "        test_Default_index = list(range((7000 + (cv-1)*1000), (7000 + cv*1000)))\n",
    "        train_Default_index = list(set(total_Default_index)-set(test_Default_index))\n",
    "        \n",
    "        ### Train and Test dataset partition\n",
    "        train_dataset = sample_dataset.iloc[(train_Fully_paid_index + train_Default_index), :].reset_index(drop = True)\n",
    "        test_dataset = sample_dataset.iloc[(test_Fully_paid_index + test_Default_index), :].reset_index(drop = True)\n",
    "        \n",
    "        ### Train distance matrix\n",
    "        all_feature_distance_matrix_train = all_feature_distance_matrix_exercise[(train_Fully_paid_index + train_Default_index), :]\n",
    "        all_feature_distance_matrix_train = all_feature_distance_matrix_train[:, (train_Fully_paid_index + train_Default_index)]\n",
    " \n",
    "        ### Test distance matrix\n",
    "        all_feature_distance_matrix_test = all_feature_distance_matrix_exercise[(test_Fully_paid_index + test_Default_index), :]\n",
    "        all_feature_distance_matrix_test = all_feature_distance_matrix_test[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        \n",
    "        return(train_dataset, test_dataset, all_feature_distance_matrix_train, all_feature_distance_matrix_test) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distance matrix to Adjancency matrix and Node feature matrix Generation\n",
    "def Matrix_Generation(all_feature_distance_matrix_train, all_feature_distance_matrix_test, train_dataset, test_dataset, bandwidth = 10):\n",
    "    ### Transformation for Train dataset\n",
    "    all_feature_adjacency_matrix_train = Distance_Weight(all_feature_distance_matrix_train, bandwidth)\n",
    "    \n",
    "    ### Transformation for Test dataset\n",
    "    all_feature_adjacency_matrix_test = Distance_Weight_Test(all_feature_distance_matrix_test, bandwidth)\n",
    " \n",
    "    ### Normalize for train dataset\n",
    "    train_all_adj = normalize(all_feature_adjacency_matrix_train + sp.eye(all_feature_adjacency_matrix_train.shape[0]))\n",
    "    \n",
    "    ### Matrix generation for Test dataset\n",
    "    all_feature_adjacency_matrix_test = np.concatenate([all_feature_adjacency_matrix_train,all_feature_adjacency_matrix_test]) \n",
    "    all_feature_adjacency_matrix_test = np.concatenate((all_feature_adjacency_matrix_test, np.zeros((all_feature_adjacency_matrix_test.shape[0], all_feature_distance_matrix_test.shape[0]))), axis = 1)   \n",
    "    test_all_adj = normalize(all_feature_adjacency_matrix_test + sp.eye(all_feature_adjacency_matrix_test.shape[0]))\n",
    "\n",
    "    ### One-hot Encoding and Train X, Train Y   \n",
    "    temp_dummies = pd.get_dummies(train_dataset)\n",
    "    train_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "    base_category = temp_dummies.columns[[11, 13, 18, 19, 21, 24, 26, 29, 32, 34, 40, 43]]\n",
    "    train_x = Model_matrix(train_dataset, base_category)\n",
    "    train_x = train_x.drop('loan_status_Charged Off', axis = 1)\n",
    "\n",
    "    ### Train Node feature Matrix\n",
    "    DN_count_all = Default_Neighbor_count(all_feature_adjacency_matrix_train, train_y)\n",
    "    train_all_node_feature = np.concatenate((DN_count_all, train_x), axis= 1)\n",
    "\n",
    "    all_dataset = pd.concat([train_dataset, test_dataset])\n",
    "    all_dataset = all_dataset.reset_index()\n",
    "    all_dataset = all_dataset.drop(columns=['index'])\n",
    "\n",
    "    temp_dummies = pd.get_dummies(all_dataset)\n",
    "    test_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "\n",
    "    ### Test Node feature matrix\n",
    "    all_x = Model_matrix(all_dataset, base_category)\n",
    "    all_x = all_x.drop('loan_status_Charged Off', axis = 1)\n",
    "\n",
    "    DN_count_all = Default_Neighbor_count(all_feature_adjacency_matrix_test, test_y)\n",
    "    test_all_node_feature = np.concatenate((DN_count_all, all_x), axis= 1)\n",
    "\n",
    "    \n",
    "    return(train_all_adj, test_all_adj, train_all_node_feature, test_all_node_feature, base_category, train_y, test_y, DN_count_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, all_feature_distance_matrix_train, all_feature_distance_matrix_test  = CV_dataset_Extraction(sample_dataset, all_feature_distance_matrix_exercise,  \n",
    "            cv = 1)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_all_adj, test_all_adj, train_all_node_feature, test_all_node_feature, base_category, train_y, test_y, DN_count_all = Matrix_Generation(all_feature_distance_matrix_train, all_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_block_GCNNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(One_block_GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks_gcn = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks_gcn.append(GCNBlock(args.n_layer,\n",
    "                                        args.in_dim if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        \n",
    "\n",
    "        \n",
    "        self.pred1 = Classifier(args.hidden_dim, 1,act = nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, node_feature, adj):\n",
    "        for i, block in enumerate(self.blocks_gcn):\n",
    "            out, adj = block((node_feature if i==0 else out), adj)\n",
    "       \n",
    "        out = self.pred1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, args, criterion):\n",
    "    t = time.time()\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(test_all_node_feature[0:12000, :], \n",
    "                  test_all_adj[0:12000, 0:12000])\n",
    "    loss = criterion(outputs, test_y[0:12000].float())\n",
    "    \n",
    "    pred = (outputs > 0.5).float()\n",
    "    correct = (pred.transpose(0,1) == test_y[0:12000].float()).float().sum()\n",
    "    acc_train = correct / len(test_y[0:12000])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    net.eval()\n",
    "    outputs = net(test_all_node_feature,\n",
    "                  test_all_adj)\n",
    "    outputs = outputs[range(12000,len(outputs))]\n",
    "\n",
    "    outputs_list = outputs.tolist()\n",
    "    test_y_for_test = test_y[range(12000,len(test_y))].float()\n",
    "    test_y_for_test_list = test_y_for_test.tolist()\n",
    "    loss_test = criterion(outputs, test_y_for_test)\n",
    "    pred = (outputs > 0.5).float()\n",
    "    correct = (pred.transpose(0,1) == test_y_for_test).float().sum()\n",
    "    acc_test = correct / len(test_y_for_test)\n",
    "    pred_list = pred.tolist()\n",
    "    f1_score = metrics.f1_score(test_y_for_test_list, pred_list, pos_label=1)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_y_for_test_list, outputs_list, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    outputs_list = outputs.tolist()\n",
    "    test_y_for_test = test_y[range(12000,len(test_y))].float()\n",
    "    test_y_for_test_list = test_y_for_test.tolist()\n",
    "    loss_test = criterion(outputs, test_y_for_test)\n",
    "    pred = (outputs > 0.5).float()\n",
    "    pred = pred.tolist()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_y_for_test_list, outputs_list, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "  \n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          \"Test set results:\",\n",
    "          'acc: {:.4f}'.format(acc_test),\n",
    "          \"Pre = {}\".format(metrics.precision_score(test_y_for_test_list, pred, pos_label=1)),\n",
    "          \"Recall = {}\".format(metrics.recall_score(test_y_for_test_list, pred, pos_label=1)),\n",
    "          'F1: {:.4f}'.format(f1_score),\n",
    "          'auc: {:.4f}'.format(roc_auc))\n",
    "\n",
    "    \n",
    "def test():\n",
    "    net.eval()\n",
    "    outputs = net(test_all_node_feature, \n",
    "               test_all_adj)\n",
    " \n",
    "    outputs = outputs[range(12000,len(outputs))]\n",
    "\n",
    "    outputs_list = outputs.tolist()\n",
    "    test_y_for_test = test_y[range(12000,len(test_y))].float()\n",
    "    test_y_for_test_list = test_y_for_test.tolist()\n",
    "    loss_test = criterion(outputs, test_y_for_test)\n",
    "    pred = (outputs > 0.5).float()\n",
    "    pred = pred.tolist()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_y_for_test_list, outputs_list, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "  \n",
    "    print(\"Accuracy = {}\".format(metrics.accuracy_score(test_y_for_test_list, pred)))\n",
    "    print(\"Precision = {}\".format(metrics.precision_score(test_y_for_test_list, pred, pos_label=1)))\n",
    "    print(\"Recall = {}\".format(metrics.recall_score(test_y_for_test_list, pred, pos_label=1)))\n",
    "    print(\"F1 score = {}\".format(metrics.f1_score(test_y_for_test_list, pred, pos_label=1)))\n",
    "    print(\"AUC = {}\".format(roc_auc))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, all_feature_distance_matrix_train, all_feature_distance_matrix_test  = CV_dataset_Extraction(sample_dataset, all_feature_distance_matrix_exercise,  \n",
    "            cv = 7)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_all_adj, test_all_adj, train_all_node_feature, test_all_node_feature, base_category, train_y, test_y, DN_count_all = Matrix_Generation(all_feature_distance_matrix_train, all_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_node_feature = torch.from_numpy(test_all_node_feature).float().cuda()\n",
    "test_all_adj = torch.from_numpy(test_all_adj).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(47137)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 2830 acc_train: 0.8071 Test set results: acc_test: 0.8105 F1_test: 0.8300 auc_test: 0.8815\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(97213)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(214831)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5871379)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(62912271)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17314)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(5122)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only one node feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, cv = 1):\n",
    "        ### Train and Test indexing for Fully Paid Observations \n",
    "        total_Fully_paid_index = list(range(7000))\n",
    "        test_Fully_paid_index = list(range((cv-1)*1000, cv*1000))\n",
    "        train_Fully_paid_index = list(set(total_Fully_paid_index)-set(test_Fully_paid_index))\n",
    "        \n",
    "        ### Train and Test indexing for Defualt Observations\n",
    "        total_Default_index = list(range(7000,14000))\n",
    "        test_Default_index = list(range((7000 + (cv-1)*1000), (7000 + cv*1000)))\n",
    "        train_Default_index = list(set(total_Default_index)-set(test_Default_index))\n",
    "        \n",
    "        ### Train and Test dataset partition\n",
    "        train_dataset = sample_dataset.iloc[(train_Fully_paid_index + train_Default_index), :].reset_index(drop = True)\n",
    "        test_dataset = sample_dataset.iloc[(test_Fully_paid_index + test_Default_index), :].reset_index(drop = True)\n",
    "        \n",
    "        ### Train distance matrix\n",
    "        loan_feature_distance_matrix_train = loan_feature_distance_matrix_exercise[(train_Fully_paid_index + train_Default_index), :]\n",
    "        loan_feature_distance_matrix_train = loan_feature_distance_matrix_train[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        history_feature_distance_matrix_train = history_feature_distance_matrix_exercise[(train_Fully_paid_index + train_Default_index), :]\n",
    "        history_feature_distance_matrix_train = history_feature_distance_matrix_train[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        soft_feature_distance_matrix_train = soft_feature_distance_matrix_exercise[(train_Fully_paid_index + train_Default_index), :]\n",
    "        soft_feature_distance_matrix_train = soft_feature_distance_matrix_train[:, (train_Fully_paid_index + train_Default_index)]\n",
    " \n",
    "        ### Test distance matrix\n",
    "        loan_feature_distance_matrix_test = loan_feature_distance_matrix_exercise[(test_Fully_paid_index + test_Default_index), :]\n",
    "        loan_feature_distance_matrix_test = loan_feature_distance_matrix_test[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        history_feature_distance_matrix_test = history_feature_distance_matrix_exercise[(test_Fully_paid_index + test_Default_index), :]\n",
    "        history_feature_distance_matrix_test = history_feature_distance_matrix_test[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        soft_feature_distance_matrix_test = soft_feature_distance_matrix_exercise[(test_Fully_paid_index + test_Default_index), :]\n",
    "        soft_feature_distance_matrix_test = soft_feature_distance_matrix_test[:, (train_Fully_paid_index + train_Default_index)]\n",
    "        \n",
    "        return(train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distance matrix to Adjancency matrix and Node feature matrix Generation\n",
    "def Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, bandwidth = 10):\n",
    "    ### Transformation for Train dataset\n",
    "    loan_feature_adjacency_matrix_train = Distance_Weight(loan_feature_distance_matrix_train, bandwidth)\n",
    "    history_feature_adjacency_matrix_train = Distance_Weight(history_feature_distance_matrix_train, bandwidth)\n",
    "    soft_feature_adjacency_matrix_train = Distance_Weight(soft_feature_distance_matrix_train, bandwidth)\n",
    "    \n",
    "    ### Transformation for Test dataset\n",
    "    loan_feature_adjacency_matrix_test = Distance_Weight_Test(loan_feature_distance_matrix_test, bandwidth)\n",
    "    history_feature_adjacency_matrix_test = Distance_Weight_Test(history_feature_distance_matrix_test, bandwidth)\n",
    "    soft_feature_adjacency_matrix_test = Distance_Weight_Test(soft_feature_distance_matrix_test, bandwidth)\n",
    "\n",
    "    ### Normalize for train dataset\n",
    "    train_loan_adj = normalize(loan_feature_adjacency_matrix_train + sp.eye(loan_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_history_adj = normalize(history_feature_adjacency_matrix_train + sp.eye(history_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_soft_adj = normalize(soft_feature_adjacency_matrix_train + sp.eye(soft_feature_adjacency_matrix_train.shape[0]))\n",
    "\n",
    "    ### Matrix generation for Test dataset\n",
    "    loan_feature_adjacency_matrix_test = np.concatenate([loan_feature_adjacency_matrix_train,loan_feature_adjacency_matrix_test])\n",
    "    history_feature_adjacency_matrix_test = np.concatenate([history_feature_adjacency_matrix_train,history_feature_adjacency_matrix_test])\n",
    "    soft_feature_adjacency_matrix_test = np.concatenate([soft_feature_adjacency_matrix_train,soft_feature_adjacency_matrix_test])\n",
    "\n",
    "    loan_feature_adjacency_matrix_test = np.concatenate((loan_feature_adjacency_matrix_test, np.zeros((loan_feature_adjacency_matrix_test.shape[0], loan_feature_distance_matrix_test.shape[0]))), axis = 1)\n",
    "    history_feature_adjacency_matrix_test = np.concatenate((history_feature_adjacency_matrix_test, np.zeros((history_feature_adjacency_matrix_test.shape[0], history_feature_distance_matrix_test.shape[0]))), axis = 1)\n",
    "    soft_feature_adjacency_matrix_test = np.concatenate((soft_feature_adjacency_matrix_test, np.zeros((soft_feature_adjacency_matrix_test.shape[0], soft_feature_distance_matrix_test.shape[0]))), axis = 1)\n",
    "\n",
    "    test_loan_adj = normalize(loan_feature_adjacency_matrix_test + sp.eye(loan_feature_adjacency_matrix_test.shape[0]))\n",
    "    test_history_adj = normalize(history_feature_adjacency_matrix_test + sp.eye(history_feature_adjacency_matrix_test.shape[0]))\n",
    "    test_soft_adj = normalize(soft_feature_adjacency_matrix_test + sp.eye(soft_feature_adjacency_matrix_test.shape[0]))\n",
    "\n",
    "\n",
    "    ### One-hot Encoding and Train X, Train Y   \n",
    "    temp_dummies = pd.get_dummies(train_dataset)\n",
    "    train_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "    base_category = temp_dummies.columns[[11, 13, 18, 19, 21, 24, 26, 29, 32, 34, 40, 43]]\n",
    "    train_x = Model_matrix(train_dataset, base_category)\n",
    "    train_x = train_x.drop('loan_status_Charged Off', axis = 1)\n",
    "\n",
    "    ### Train Node feature Matrix\n",
    "    DN_count_loan = Default_Neighbor_count(loan_feature_adjacency_matrix_train, train_y)\n",
    "    train_loan_node_feature = np.concatenate((DN_count_loan, train_x), axis= 1)\n",
    "    DN_count_history = Default_Neighbor_count(history_feature_adjacency_matrix_train, train_y)\n",
    "    train_history_node_feature = np.concatenate((DN_count_history, train_x), axis= 1)\n",
    "    DN_count_soft = Default_Neighbor_count(soft_feature_adjacency_matrix_train, train_y)\n",
    "    train_soft_node_feature = np.concatenate((DN_count_soft, train_x), axis= 1)\n",
    "\n",
    "    all_dataset = pd.concat([train_dataset, test_dataset])\n",
    "    all_dataset = all_dataset.reset_index()\n",
    "    all_dataset = all_dataset.drop(columns=['index'])\n",
    "\n",
    "    temp_dummies = pd.get_dummies(all_dataset)\n",
    "    test_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "\n",
    "    ### Test Node feature matrix\n",
    "    all_x = Model_matrix(all_dataset, base_category)\n",
    "    all_x = all_x.drop('loan_status_Charged Off', axis = 1)\n",
    "\n",
    "    DN_count_loan = Default_Neighbor_count(loan_feature_adjacency_matrix_test, test_y)\n",
    "    test_loan_node_feature = np.concatenate((DN_count_loan, all_x), axis= 1)\n",
    "    \n",
    "    DN_count_history = Default_Neighbor_count(history_feature_adjacency_matrix_test, test_y)\n",
    "    test_history_node_feature = np.concatenate((DN_count_history, all_x), axis= 1)\n",
    "    \n",
    "    DN_count_soft = Default_Neighbor_count(soft_feature_adjacency_matrix_test, test_y)\n",
    "    test_soft_node_feature = np.concatenate((DN_count_soft, all_x), axis= 1)\n",
    "    \n",
    "    return(train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 1)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft = Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, args, criterion):\n",
    "    t = time.time()\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(test_soft_node_feature[0:12000, :], \n",
    "                  test_soft_adj[0:12000, 0:12000])\n",
    "    loss = criterion(outputs, test_y[0:12000].float())\n",
    "    \n",
    "    pred = (outputs > 0.5).float()\n",
    "    correct = (pred.transpose(0,1) == test_y[0:12000].float()).float().sum()\n",
    "    acc_train = correct / len(test_y[0:12000])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    net.eval()\n",
    "    outputs = net(test_soft_node_feature,\n",
    "                  test_soft_adj)\n",
    "    outputs = outputs[range(12000,len(outputs))]\n",
    "\n",
    "    outputs_list = outputs.tolist()\n",
    "    test_y_for_test = test_y[range(12000,len(test_y))].float()\n",
    "    test_y_for_test_list = test_y_for_test.tolist()\n",
    "    loss_test = criterion(outputs, test_y_for_test)\n",
    "    pred = (outputs > 0.5).float()\n",
    "    correct = (pred.transpose(0,1) == test_y_for_test).float().sum()\n",
    "    acc_test = correct / len(test_y_for_test)\n",
    "    pred_list = pred.tolist()\n",
    "    f1_score = metrics.f1_score(test_y_for_test_list, pred_list, pos_label=1)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_y_for_test_list, outputs_list, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    outputs_list = outputs.tolist()\n",
    "    test_y_for_test = test_y[range(12000,len(test_y))].float()\n",
    "    test_y_for_test_list = test_y_for_test.tolist()\n",
    "    loss_test = criterion(outputs, test_y_for_test)\n",
    "    pred = (outputs > 0.5).float()\n",
    "    pred = pred.tolist()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_y_for_test_list, outputs_list, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "  \n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          \"Test set results:\",\n",
    "          'acc: {:.4f}'.format(acc_test),\n",
    "          \"Pre = {}\".format(metrics.precision_score(test_y_for_test_list, pred, pos_label=1)),\n",
    "          \"Recall = {}\".format(metrics.recall_score(test_y_for_test_list, pred, pos_label=1)),\n",
    "          'F1: {:.4f}'.format(f1_score),\n",
    "          'auc: {:.4f}'.format(roc_auc))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 1)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft = Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n",
    "\n",
    "\n",
    "test_soft_node_feature = torch.from_numpy(test_soft_node_feature).float().cuda()\n",
    "test_soft_adj = torch.from_numpy(test_soft_adj).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(47137)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 2830 acc_train: 0.8071 Test set results: acc_test: 0.8105 F1_test: 0.8300 auc_test: 0.8815\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 2)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft = Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n",
    "\n",
    "\n",
    "test_soft_node_feature = torch.from_numpy(test_soft_node_feature).float().cuda()\n",
    "test_soft_adj = torch.from_numpy(test_soft_adj).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(97213)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 2830 acc_train: 0.8071 Test set results: acc_test: 0.8105 F1_test: 0.8300 auc_test: 0.8815\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 3)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft = Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n",
    "\n",
    "\n",
    "test_soft_node_feature = torch.from_numpy(test_soft_node_feature).float().cuda()\n",
    "test_soft_adj = torch.from_numpy(test_soft_adj).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(214831)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 2830 acc_train: 0.8071 Test set results: acc_test: 0.8105 F1_test: 0.8300 auc_test: 0.8815\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 4)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft = Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n",
    "\n",
    "\n",
    "test_soft_node_feature = torch.from_numpy(test_soft_node_feature).float().cuda()\n",
    "test_soft_adj = torch.from_numpy(test_soft_adj).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5871379)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 2830 acc_train: 0.8071 Test set results: acc_test: 0.8105 F1_test: 0.8300 auc_test: 0.8815\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 5)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft = Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n",
    "\n",
    "\n",
    "test_soft_node_feature = torch.from_numpy(test_soft_node_feature).float().cuda()\n",
    "test_soft_adj = torch.from_numpy(test_soft_adj).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(62912271)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 2830 acc_train: 0.8071 Test set results: acc_test: 0.8105 F1_test: 0.8300 auc_test: 0.8815\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 6)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft = Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n",
    "\n",
    "\n",
    "test_soft_node_feature = torch.from_numpy(test_soft_node_feature).float().cuda()\n",
    "test_soft_adj = torch.from_numpy(test_soft_adj).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17314)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 2830 acc_train: 0.8071 Test set results: acc_test: 0.8105 F1_test: 0.8300 auc_test: 0.8815\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 7)\n",
    "\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "\n",
    "train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft = Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n",
    "\n",
    "\n",
    "test_soft_node_feature = torch.from_numpy(test_soft_node_feature).float().cuda()\n",
    "test_soft_adj = torch.from_numpy(test_soft_adj).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5122)\n",
    "    \n",
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 5000, \"lr\": 0.0005, \"weight_decay\": 5e-4, \"n_layer\": 3, \n",
    "                                         \"n_block\": 1 , \"in_dim\": 32, \"hidden_dim\": 16 , \n",
    "                                         \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n",
    "args.act = nn.ReLU()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "net = One_block_GCNNet(args)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)\n",
    "    \n",
    "### Epoch: 2830 acc_train: 0.8071 Test set results: acc_test: 0.8105 F1_test: 0.8300 auc_test: 0.8815\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code.Graph_composition import *\n",
    "\n",
    "### Data load\n",
    "sample_dataset = pd.read_csv('Data/Sample_dataset.csv')\n",
    "\n",
    "\n",
    "# Loan feature definition\n",
    "loan_features = sample_dataset.columns[0:7]\n",
    "loan_features_dataset = sample_dataset[loan_features] \n",
    "\n",
    "soft_features = sample_dataset.columns[16:21]\n",
    "soft_features_dataset = sample_dataset[soft_features] \n",
    "\n",
    "# History feature definition\n",
    "history_features = sample_dataset.columns[7:16]\n",
    "history_features_dataset = sample_dataset[history_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_loan_feature = []\n",
    "\n",
    "for i in range(len(loan_features)):\n",
    "    loan_features_dataset = sample_dataset[loan_features] \n",
    "    del loan_features_dataset[loan_features[i]]\n",
    "    loan_feature_graph = Numerical_min_max(loan_features_dataset)\n",
    "    loan_feature_graph_discreted = Discretization(loan_feature_graph)\n",
    "    loan_feature_name, loan_feature_distance_by_factor_exercise = Algo_distance(loan_feature_graph_discreted)\n",
    "    loan_feature_weight_vector_exercise = Weight(loan_feature_distance_by_factor_exercise)\n",
    "    loan_feature_distance_matrix_exercise = Distance(loan_feature_graph, loan_feature_weight_vector_exercise, loan_feature_distance_by_factor_exercise, loan_feature_name)\n",
    "    xai_loan_feature = [xai_loan_feature, loan_feature_distance_matrix_exercise]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('xai_loan_feature', xai_loan_feature1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_history_feature = []\n",
    "\n",
    "for i in range(len(history_features)):\n",
    "    history_features_dataset = sample_dataset[history_features] \n",
    "    del history_features_dataset[history_features[i]]\n",
    "    history_feature_graph = Numerical_min_max(history_features_dataset)\n",
    "    history_feature_graph_discreted = Discretization(history_feature_graph)\n",
    "    history_feature_name, history_feature_distance_by_factor_exercise = Algo_distance(history_feature_graph_discreted)\n",
    "    history_feature_weight_vector_exercise = Weight(history_feature_distance_by_factor_exercise)\n",
    "    history_feature_distance_matrix_exercise = Distance(history_feature_graph, history_feature_weight_vector_exercise, history_feature_distance_by_factor_exercise, history_feature_name)\n",
    "    xai_history_feature = [xai_history_feature, history_feature_distance_matrix_exercise]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('xai_history_feature', xai_history_feature1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_soft_feature = []\n",
    "\n",
    "for i in range(0,2):\n",
    "    soft_features_dataset = sample_dataset[soft_features] \n",
    "    del soft_features_dataset[soft_features[i]]\n",
    "    soft_feature_graph = Numerical_min_max(soft_features_dataset)\n",
    "    soft_feature_graph_discreted = Discretization(soft_feature_graph)\n",
    "    soft_feature_name, soft_feature_distance_by_factor_exercise = Algo_distance(soft_feature_graph_discreted)\n",
    "    soft_feature_weight_vector_exercise = Weight(soft_feature_distance_by_factor_exercise)\n",
    "    soft_feature_distance_matrix_exercise = Distance(soft_feature_graph, soft_feature_weight_vector_exercise, soft_feature_distance_by_factor_exercise, soft_feature_name)\n",
    "    xai_soft_feature = [xai_soft_feature, soft_feature_distance_matrix_exercise]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_features_dataset = sample_dataset[soft_features] \n",
    "del soft_features_dataset[soft_features[2]]    \n",
    "soft_feature_name, soft_feature_distance_by_factor_exercise = Algo_distance(soft_features_dataset)\n",
    "soft_feature_distance_matrix_exercise = Distance_only_category(soft_features_dataset, soft_feature_distance_by_factor_exercise, soft_feature_name)\n",
    "xai_soft_feature = [xai_soft_feature, soft_feature_distance_matrix_exercise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3,5):\n",
    "    soft_features_dataset = sample_dataset[soft_features] \n",
    "    del soft_features_dataset[soft_features[i]]\n",
    "    soft_feature_graph = Numerical_min_max(soft_features_dataset)\n",
    "    soft_feature_graph_discreted = Discretization(soft_feature_graph)\n",
    "    soft_feature_name, soft_feature_distance_by_factor_exercise = Algo_distance(soft_feature_graph_discreted)\n",
    "    soft_feature_weight_vector_exercise = Weight(soft_feature_distance_by_factor_exercise)\n",
    "    soft_feature_distance_matrix_exercise = Distance(soft_feature_graph, soft_feature_weight_vector_exercise, soft_feature_distance_by_factor_exercise, soft_feature_name)\n",
    "    xai_soft_feature = [xai_soft_feature, soft_feature_distance_matrix_exercise]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('xai_soft_feature', xai_soft_feature1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_soft_feature[0][0][0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_loan_feature[0][0][0][0][0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_loan_feature1 = [xai_loan_feature[0][0][0][0][0][0][1],xai_loan_feature[0][0][0][0][0][1], xai_loan_feature[0][0][0][0][1], xai_loan_feature[0][0][0][1], xai_loan_feature[0][0][1], xai_loan_feature[0][1], xai_loan_feature[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_history_feature1 = [xai_history_feature[0][0][0][0][0][0][0][0][1], xai_history_feature[0][0][0][0][0][0][0][1],xai_history_feature[0][0][0][0][0][0][1],xai_history_feature[0][0][0][0][0][1], xai_history_feature[0][0][0][0][1], xai_history_feature[0][0][0][1], xai_history_feature[0][0][1], xai_history_feature[0][1], xai_history_feature[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_soft_feature1 = [xai_soft_feature[0][0][0][0][1], xai_soft_feature[0][0][0][1], xai_soft_feature[0][0][1], xai_soft_feature[0][1], xai_soft_feature[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(test_loan_node_feature[0:12000, :], test_history_node_feature[0:12000, :], test_soft_node_feature[0:12000, :], \n",
    "              test_loan_adj[0:12000, 0:12000], test_history_adj[0:12000, 0:12000], test_soft_adj[0:12000, 0:12000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_matrix_without_one_feature(dataset, base_category, feature1, feature2):\n",
    "    temp_dummies1 = pd.get_dummies(dataset)\n",
    "    temp_dummies2 = pd.get_dummies(dataset[[feature2, feature1]])\n",
    "    del temp_dummies2[feature2]\n",
    "    base_category_for_dataset1 = temp_dummies1.columns & base_category\n",
    "    temp_dummies1 = temp_dummies1.drop(columns = base_category_for_dataset1)\n",
    "    base_category_for_dataset2 = temp_dummies2.columns & base_category\n",
    "    temp_dummies2 = temp_dummies2.drop(columns = base_category_for_dataset2)    \n",
    "    total_colnames = temp_dummies1.columns\n",
    "    colnames = temp_dummies2.columns\n",
    "    \n",
    "    return(temp_dummies1, colnames, total_colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distance matrix to Adjancency matrix and Node feature matrix Generation\n",
    "def XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, bandwidth = 10, xai_index = 0):\n",
    "    ### Transformation for Train dataset\n",
    "    loan_feature_adjacency_matrix_train = Distance_Weight(loan_feature_distance_matrix_train, bandwidth)\n",
    "    history_feature_adjacency_matrix_train = Distance_Weight(history_feature_distance_matrix_train, bandwidth)\n",
    "    soft_feature_adjacency_matrix_train = Distance_Weight(soft_feature_distance_matrix_train, bandwidth)\n",
    "    \n",
    "    ### Transformation for Test dataset\n",
    "    loan_feature_adjacency_matrix_test = Distance_Weight_Test(loan_feature_distance_matrix_test, bandwidth)\n",
    "    history_feature_adjacency_matrix_test = Distance_Weight_Test(history_feature_distance_matrix_test, bandwidth)\n",
    "    soft_feature_adjacency_matrix_test = Distance_Weight_Test(soft_feature_distance_matrix_test, bandwidth)\n",
    "\n",
    "    ### Normalize for train dataset\n",
    "    train_loan_adj = normalize(loan_feature_adjacency_matrix_train + sp.eye(loan_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_history_adj = normalize(history_feature_adjacency_matrix_train + sp.eye(history_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_soft_adj = normalize(soft_feature_adjacency_matrix_train + sp.eye(soft_feature_adjacency_matrix_train.shape[0]))\n",
    "\n",
    "    ### One-hot Encoding and Train X, Train Y   \n",
    "    temp_dummies = pd.get_dummies(train_dataset)\n",
    "    train_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "    base_category = temp_dummies.columns[[11, 13, 18, 19, 21, 24, 26, 29, 32, 34, 40, 43]]\n",
    "    train_x = Model_matrix(train_dataset, base_category)\n",
    "    train_x = train_x.drop('loan_status_Charged Off', axis = 1)\n",
    "\n",
    "    ### Train Node feature Matrix\n",
    "    loan_features = train_dataset.columns[0:7]\n",
    "    train_loan_features_dataset = train_dataset[loan_features] \n",
    "    masked_feature = loan_features[xai_index]\n",
    "    if xai_index == 0:\n",
    "        numeric_feature = loan_features[2]\n",
    "    else:\n",
    "        numeric_feature = loan_features[0]\n",
    "    \n",
    "    history_features = train_dataset.columns[7:16]\n",
    "    train_history_features_dataset = train_dataset[history_features] \n",
    "    soft_features = train_dataset.columns[16:21]\n",
    "    train_soft_features_dataset = train_dataset[soft_features] \n",
    "    \n",
    "    loan_x, colnames, total_colnames = Model_matrix_without_one_feature(train_loan_features_dataset, base_category, masked_feature, numeric_feature)\n",
    "    col_index = [list(loan_x.columns).index(x) + 1 for x in colnames]\n",
    "    history_x = Model_matrix(train_history_features_dataset, base_category)\n",
    "    soft_x = Model_matrix(train_soft_features_dataset, base_category)\n",
    "    DN_count_loan = Default_Neighbor_count(loan_feature_adjacency_matrix_train, train_y)\n",
    "    train_loan_node_feature = np.concatenate((DN_count_loan, loan_x), axis= 1)\n",
    "    DN_count_history = Default_Neighbor_count(history_feature_adjacency_matrix_train, train_y)\n",
    "    train_history_node_feature = np.concatenate((DN_count_history, history_x), axis= 1)\n",
    "    DN_count_soft = Default_Neighbor_count(soft_feature_adjacency_matrix_train, train_y)\n",
    "    train_soft_node_feature = np.concatenate((DN_count_soft, soft_x), axis= 1)\n",
    "\n",
    "    \n",
    "    \n",
    "    return(train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature, total_colnames, col_index)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_loan_feature = np.load('xai_loan_feature.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(xai_loan_feature)):\n",
    "    criterion = nn.L1Loss()\n",
    "    train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, xai_loan_feature[i], history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 5)\n",
    "    col_order = train_dataset.columns\n",
    "\n",
    "    ### normalize\n",
    "    test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "    train_dataset = Numerical_min_max(train_dataset)\n",
    "    train_dataset = train_dataset[col_order]\n",
    "    test_dataset = test_dataset[col_order]\n",
    "    del col_order\n",
    "    \n",
    "    train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature, total_colnames, col_index  = XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20, xai_index = i)\n",
    "    \n",
    "    print(masked_feature)\n",
    "        \n",
    "    for indexs in col_index:\n",
    "        train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature, total_colnames, col_index  = XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "        bandwidth = 20, xai_index = i)\n",
    "    \n",
    "        test_history_node_feature = torch.from_numpy(train_history_node_feature).float().cuda()\n",
    "        test_soft_node_feature = torch.from_numpy(train_soft_node_feature).float().cuda()\n",
    "        test_loan_adj = torch.from_numpy(train_loan_adj).float().cuda()\n",
    "        test_history_adj = torch.from_numpy(train_history_adj).float().cuda()\n",
    "        test_soft_adj = torch.from_numpy(train_soft_adj).float().cuda()\n",
    "     \n",
    "        ### -0.1\n",
    "        col_index1 = indexs - 1\n",
    "        print('                %s'%total_colnames[col_index1])\n",
    "        train_loan_node_feature1 = train_loan_node_feature\n",
    "        train_loan_node_feature1[:,indexs] = train_loan_node_feature1[:,indexs] - 0.1\n",
    "        test_loan_node_feature = torch.from_numpy(train_loan_node_feature1).float().cuda()\n",
    "        net.eval()\n",
    "        outputs1 = net(test_loan_node_feature, test_history_node_feature, test_soft_node_feature, \n",
    "                       test_loan_adj, test_history_adj, test_soft_adj)\n",
    "        \n",
    "        loss = criterion(outputs, outputs1)\n",
    "        print('        MAE_loss: {:.4f}'.format(loss))\n",
    "\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_history_feature = np.load('xai_history_feature.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distance matrix to Adjancency matrix and Node feature matrix Generation\n",
    "def XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, bandwidth = 10, xai_index = 0):\n",
    "    ### Transformation for Train dataset\n",
    "    loan_feature_adjacency_matrix_train = Distance_Weight(loan_feature_distance_matrix_train, bandwidth)\n",
    "    history_feature_adjacency_matrix_train = Distance_Weight(history_feature_distance_matrix_train, bandwidth)\n",
    "    soft_feature_adjacency_matrix_train = Distance_Weight(soft_feature_distance_matrix_train, bandwidth)\n",
    "    \n",
    "    ### Transformation for Test dataset\n",
    "    loan_feature_adjacency_matrix_test = Distance_Weight_Test(loan_feature_distance_matrix_test, bandwidth)\n",
    "    history_feature_adjacency_matrix_test = Distance_Weight_Test(history_feature_distance_matrix_test, bandwidth)\n",
    "    soft_feature_adjacency_matrix_test = Distance_Weight_Test(soft_feature_distance_matrix_test, bandwidth)\n",
    "\n",
    "    ### Normalize for train dataset\n",
    "    train_loan_adj = normalize(loan_feature_adjacency_matrix_train + sp.eye(loan_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_history_adj = normalize(history_feature_adjacency_matrix_train + sp.eye(history_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_soft_adj = normalize(soft_feature_adjacency_matrix_train + sp.eye(soft_feature_adjacency_matrix_train.shape[0]))\n",
    "\n",
    "    ### One-hot Encoding and Train X, Train Y   \n",
    "    temp_dummies = pd.get_dummies(train_dataset)\n",
    "    train_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "    base_category = temp_dummies.columns[[11, 13, 18, 19, 21, 24, 26, 29, 32, 34, 40, 43]]\n",
    "    train_x = Model_matrix(train_dataset, base_category)\n",
    "    train_x = train_x.drop('loan_status_Charged Off', axis = 1)\n",
    "\n",
    "    ### Train Node feature Matrix\n",
    "    loan_features = train_dataset.columns[0:7]\n",
    "    train_loan_features_dataset = train_dataset[loan_features] \n",
    "    history_features = train_dataset.columns[7:16]\n",
    "    train_history_features_dataset = train_dataset[history_features] \n",
    "    masked_feature = history_features[i]\n",
    "    if i == 1:\n",
    "        numeric_feature = history_features[5]\n",
    "    else:\n",
    "        numeric_feature = history_features[1]\n",
    "    soft_features = train_dataset.columns[16:21]\n",
    "    train_soft_features_dataset = train_dataset[soft_features] \n",
    "    \n",
    "    loan_x = Model_matrix(train_loan_features_dataset, base_category)\n",
    "    #history_x = Model_matrix(train_history_features_dataset, base_category)\n",
    "    history_x, colnames, total_colnames = Model_matrix_without_one_feature(train_history_features_dataset, base_category, masked_feature, numeric_feature)\n",
    "    col_index = [list(history_x.columns).index(x) + 1 for x in colnames]\n",
    "    \n",
    "    soft_x = Model_matrix(train_soft_features_dataset, base_category)\n",
    "    DN_count_loan = Default_Neighbor_count(loan_feature_adjacency_matrix_train, train_y)\n",
    "    train_loan_node_feature = np.concatenate((DN_count_loan, loan_x), axis= 1)\n",
    "    DN_count_history = Default_Neighbor_count(history_feature_adjacency_matrix_train, train_y)\n",
    "    train_history_node_feature = np.concatenate((DN_count_history, history_x), axis= 1)\n",
    "    DN_count_soft = Default_Neighbor_count(soft_feature_adjacency_matrix_train, train_y)\n",
    "    train_soft_node_feature = np.concatenate((DN_count_soft, soft_x), axis= 1)\n",
    "\n",
    "    \n",
    "    \n",
    "    return(train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature, total_colnames, col_index)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(xai_history_feature)):\n",
    "    train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, xai_history_feature[i], soft_feature_distance_matrix_exercise, \n",
    "            cv = 5)\n",
    "    col_order = train_dataset.columns\n",
    "\n",
    "    ### normalize\n",
    "    test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "    train_dataset = Numerical_min_max(train_dataset)\n",
    "    train_dataset = train_dataset[col_order]\n",
    "    test_dataset = test_dataset[col_order]\n",
    "    del col_order\n",
    "    \n",
    "    train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature, total_colnames, col_index  = XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20, xai_index = i)\n",
    "    \n",
    "    print(masked_feature)\n",
    "        \n",
    "    for indexs in col_index:\n",
    "        train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature, total_colnames, col_index  = XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "        bandwidth = 20, xai_index = i)\n",
    "   \n",
    "        test_loan_node_feature = torch.from_numpy(train_loan_node_feature).float().cuda()\n",
    "        test_soft_node_feature = torch.from_numpy(train_soft_node_feature).float().cuda()\n",
    "        test_loan_adj = torch.from_numpy(train_loan_adj).float().cuda()\n",
    "        test_history_adj = torch.from_numpy(train_history_adj).float().cuda()\n",
    "        test_soft_adj = torch.from_numpy(train_soft_adj).float().cuda()\n",
    "    \n",
    "        ### -0.1\n",
    "        col_index1 = indexs - 1\n",
    "        print('                %s'%total_colnames[col_index1])\n",
    "        train_history_node_feature1 = train_history_node_feature\n",
    "        train_history_node_feature1[:,indexs] = train_history_node_feature1[:,indexs] - 0.1\n",
    "        test_history_node_feature = torch.from_numpy(train_history_node_feature1).float().cuda()\n",
    "        net.eval()\n",
    "        outputs1 = net(test_loan_node_feature, test_history_node_feature, test_soft_node_feature, \n",
    "                       test_loan_adj, test_history_adj, test_soft_adj)\n",
    "        \n",
    "        loss = criterion(outputs, outputs1)\n",
    "        print('        MAE_loss: {:.4f}'.format(loss))\n",
    "\n",
    "    print('--------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xai_soft_feature = np.load('xai_soft_feature.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distance matrix to Adjancency matrix and Node feature matrix Generation\n",
    "def XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, bandwidth = 10, xai_index = 0):\n",
    "    ### Transformation for Train dataset\n",
    "    loan_feature_adjacency_matrix_train = Distance_Weight(loan_feature_distance_matrix_train, bandwidth)\n",
    "    history_feature_adjacency_matrix_train = Distance_Weight(history_feature_distance_matrix_train, bandwidth)\n",
    "    soft_feature_adjacency_matrix_train = Distance_Weight(soft_feature_distance_matrix_train, bandwidth)\n",
    "    \n",
    "    ### Transformation for Test dataset\n",
    "    loan_feature_adjacency_matrix_test = Distance_Weight_Test(loan_feature_distance_matrix_test, bandwidth)\n",
    "    history_feature_adjacency_matrix_test = Distance_Weight_Test(history_feature_distance_matrix_test, bandwidth)\n",
    "    soft_feature_adjacency_matrix_test = Distance_Weight_Test(soft_feature_distance_matrix_test, bandwidth)\n",
    "\n",
    "    ### Normalize for train dataset\n",
    "    train_loan_adj = normalize(loan_feature_adjacency_matrix_train + sp.eye(loan_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_history_adj = normalize(history_feature_adjacency_matrix_train + sp.eye(history_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_soft_adj = normalize(soft_feature_adjacency_matrix_train + sp.eye(soft_feature_adjacency_matrix_train.shape[0]))\n",
    "\n",
    "    ### One-hot Encoding and Train X, Train Y   \n",
    "    temp_dummies = pd.get_dummies(train_dataset)\n",
    "    train_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "    base_category = temp_dummies.columns[[11, 13, 18, 19, 21, 24, 26, 29, 32, 34, 40, 43]]\n",
    "    train_x = Model_matrix(train_dataset, base_category)\n",
    "    train_x = train_x.drop('loan_status_Charged Off', axis = 1)\n",
    "\n",
    "    ### Train Node feature Matrix\n",
    "    loan_features = train_dataset.columns[0:7]\n",
    "    train_loan_features_dataset = train_dataset[loan_features] \n",
    "    history_features = train_dataset.columns[7:16]\n",
    "    train_history_features_dataset = train_dataset[history_features] \n",
    "    soft_features = train_dataset.columns[16:21]\n",
    "    masked_feature = soft_features[i]\n",
    "    numeric_feature = soft_features[2]\n",
    "    train_soft_features_dataset = train_dataset[soft_features] \n",
    "    \n",
    "    loan_x = Model_matrix(train_loan_features_dataset, base_category)\n",
    "    history_x = Model_matrix(train_history_features_dataset, base_category)\n",
    "    #soft_x = Model_matrix(train_soft_features_dataset, base_category)\n",
    "    soft_x, colnames, total_colnames = Model_matrix_without_one_feature(train_soft_features_dataset, base_category, masked_feature, numeric_feature)\n",
    "    col_index = [list(soft_x.columns).index(x) + 1 for x in colnames]\n",
    "    DN_count_loan = Default_Neighbor_count(loan_feature_adjacency_matrix_train, train_y)\n",
    "    train_loan_node_feature = np.concatenate((DN_count_loan, loan_x), axis= 1)\n",
    "    DN_count_history = Default_Neighbor_count(history_feature_adjacency_matrix_train, train_y)\n",
    "    train_history_node_feature = np.concatenate((DN_count_history, history_x), axis= 1)\n",
    "    DN_count_soft = Default_Neighbor_count(soft_feature_adjacency_matrix_train, train_y)\n",
    "    train_soft_node_feature = np.concatenate((DN_count_soft, soft_x), axis= 1)\n",
    "\n",
    "    return(train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature, total_colnames, col_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,3,4]:\n",
    "    train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, xai_soft_feature[i], \n",
    "            cv = 5)\n",
    "    col_order = train_dataset.columns\n",
    "\n",
    "    ### normalize\n",
    "    test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "    train_dataset = Numerical_min_max(train_dataset)\n",
    "    train_dataset = train_dataset[col_order]\n",
    "    test_dataset = test_dataset[col_order]\n",
    "    del col_order\n",
    "    \n",
    "    train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature, total_colnames, col_index  = XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20, xai_index = i)\n",
    "    \n",
    "    print(masked_feature)\n",
    "        \n",
    "    for indexs in col_index:\n",
    "        ### -0.1\n",
    "        train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature, total_colnames, col_index  = XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "        bandwidth = 20, xai_index = i)\n",
    "    \n",
    "        test_loan_node_feature = torch.from_numpy(train_loan_node_feature).float().cuda()\n",
    "        test_history_node_feature = torch.from_numpy(train_history_node_feature).float().cuda()\n",
    "        test_loan_adj = torch.from_numpy(train_loan_adj).float().cuda()\n",
    "        test_history_adj = torch.from_numpy(train_history_adj).float().cuda()\n",
    "        test_soft_adj = torch.from_numpy(train_soft_adj).float().cuda()\n",
    "    \n",
    "        col_index1 = indexs - 1\n",
    "        print('                %s'%total_colnames[col_index1])\n",
    "        train_soft_node_feature1 = train_soft_node_feature\n",
    "        train_soft_node_feature1[:,indexs] = train_soft_node_feature1[:,indexs] - 0.1\n",
    "        test_soft_node_feature = torch.from_numpy(train_soft_node_feature1).float().cuda()\n",
    "        net.eval()\n",
    "        outputs1 = net(test_loan_node_feature, test_history_node_feature, test_soft_node_feature, \n",
    "                       test_loan_adj, test_history_adj, test_soft_adj)\n",
    "        \n",
    "        loss = criterion(outputs, outputs1)\n",
    "        print('        MAE_loss: {:.4f}'.format(loss))\n",
    "\n",
    "    print('--------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distance matrix to Adjancency matrix and Node feature matrix Generation\n",
    "def XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, bandwidth = 10, xai_index = 0):\n",
    "    ### Transformation for Train dataset\n",
    "    loan_feature_adjacency_matrix_train = Distance_Weight(loan_feature_distance_matrix_train, bandwidth)\n",
    "    history_feature_adjacency_matrix_train = Distance_Weight(history_feature_distance_matrix_train, bandwidth)\n",
    "    soft_feature_adjacency_matrix_train = Distance_Weight(soft_feature_distance_matrix_train, bandwidth)\n",
    "    \n",
    "    ### Transformation for Test dataset\n",
    "    loan_feature_adjacency_matrix_test = Distance_Weight_Test(loan_feature_distance_matrix_test, bandwidth)\n",
    "    history_feature_adjacency_matrix_test = Distance_Weight_Test(history_feature_distance_matrix_test, bandwidth)\n",
    "    soft_feature_adjacency_matrix_test = Distance_Weight_Test(soft_feature_distance_matrix_test, bandwidth)\n",
    "\n",
    "    ### Normalize for train dataset\n",
    "    train_loan_adj = normalize(loan_feature_adjacency_matrix_train + sp.eye(loan_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_history_adj = normalize(history_feature_adjacency_matrix_train + sp.eye(history_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_soft_adj = normalize(soft_feature_adjacency_matrix_train + sp.eye(soft_feature_adjacency_matrix_train.shape[0]))\n",
    "\n",
    "    ### One-hot Encoding and Train X, Train Y   \n",
    "    temp_dummies = pd.get_dummies(train_dataset)\n",
    "    train_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "    base_category = temp_dummies.columns[[11, 13, 18, 19, 21, 24, 26, 29, 32, 34, 40, 43]]\n",
    "    train_x = Model_matrix(train_dataset, base_category)\n",
    "    train_x = train_x.drop('loan_status_Charged Off', axis = 1)\n",
    "\n",
    "    ### Train Node feature Matrix\n",
    "    loan_features = train_dataset.columns[0:7]\n",
    "    train_loan_features_dataset = train_dataset[loan_features] \n",
    "    history_features = train_dataset.columns[7:16]\n",
    "    train_history_features_dataset = train_dataset[history_features] \n",
    "    soft_features = train_dataset.columns[16:21]\n",
    "    masked_feature = soft_features[i]\n",
    "    train_soft_features_dataset = train_dataset[soft_features] \n",
    "    \n",
    "    loan_x = Model_matrix(train_loan_features_dataset, base_category)\n",
    "    history_x = Model_matrix(train_history_features_dataset, base_category)\n",
    "    soft_x = Model_matrix(train_soft_features_dataset, base_category)\n",
    "    soft_x['annual_inc'] = soft_x['annual_inc'] - 0.1\n",
    "    DN_count_loan = Default_Neighbor_count(loan_feature_adjacency_matrix_train, train_y)\n",
    "    train_loan_node_feature = np.concatenate((DN_count_loan, loan_x), axis= 1)\n",
    "    DN_count_history = Default_Neighbor_count(history_feature_adjacency_matrix_train, train_y)\n",
    "    train_history_node_feature = np.concatenate((DN_count_history, history_x), axis= 1)\n",
    "    DN_count_soft = Default_Neighbor_count(soft_feature_adjacency_matrix_train, train_y)\n",
    "    train_soft_node_feature = np.concatenate((DN_count_soft, soft_x), axis= 1)\n",
    "\n",
    "    return(train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [2]:\n",
    "    train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, xai_soft_feature[i], \n",
    "            cv = 5)\n",
    "    col_order = train_dataset.columns\n",
    "\n",
    "    ### normalize\n",
    "    test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "    train_dataset = Numerical_min_max(train_dataset)\n",
    "    train_dataset = train_dataset[col_order]\n",
    "    test_dataset = test_dataset[col_order]\n",
    "    del col_order\n",
    "    \n",
    "    train_loan_adj, train_history_adj, train_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft, masked_feature = XAI_Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20, xai_index = i)\n",
    "    \n",
    "    test_loan_node_feature = torch.from_numpy(train_loan_node_feature).float().cuda()\n",
    "    test_history_node_feature = torch.from_numpy(train_history_node_feature).float().cuda()\n",
    "    test_soft_node_feature = torch.from_numpy(train_soft_node_feature).float().cuda()\n",
    "    test_loan_adj = torch.from_numpy(train_loan_adj).float().cuda()\n",
    "    test_history_adj = torch.from_numpy(train_history_adj).float().cuda()\n",
    "    test_soft_adj = torch.from_numpy(train_soft_adj).float().cuda()\n",
    "    \n",
    "    print(masked_feature)\n",
    "    net.eval()\n",
    "    outputs1 = net(test_loan_node_feature, test_history_node_feature, test_soft_node_feature, \n",
    "                   test_loan_adj, test_history_adj, test_soft_adj)\n",
    "        \n",
    "    loss = criterion(outputs, outputs1)\n",
    "    print('        MAE_loss: {:.4f}'.format(loss))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distance matrix to Adjancency matrix and Node feature matrix Generation\n",
    "def Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, bandwidth = 10):\n",
    "    ### Transformation for Train dataset\n",
    "    loan_feature_adjacency_matrix_train = Distance_Weight(loan_feature_distance_matrix_train, bandwidth)\n",
    "    history_feature_adjacency_matrix_train = Distance_Weight(history_feature_distance_matrix_train, bandwidth)\n",
    "    soft_feature_adjacency_matrix_train = Distance_Weight(soft_feature_distance_matrix_train, bandwidth)\n",
    "    \n",
    "    ### Transformation for Test dataset\n",
    "    loan_feature_adjacency_matrix_test = Distance_Weight_Test(loan_feature_distance_matrix_test, bandwidth)\n",
    "    history_feature_adjacency_matrix_test = Distance_Weight_Test(history_feature_distance_matrix_test, bandwidth)\n",
    "    soft_feature_adjacency_matrix_test = Distance_Weight_Test(soft_feature_distance_matrix_test, bandwidth)\n",
    "\n",
    "    ### Normalize for train dataset\n",
    "    train_loan_adj = normalize(loan_feature_adjacency_matrix_train + sp.eye(loan_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_history_adj = normalize(history_feature_adjacency_matrix_train + sp.eye(history_feature_adjacency_matrix_train.shape[0]))\n",
    "    train_soft_adj = normalize(soft_feature_adjacency_matrix_train + sp.eye(soft_feature_adjacency_matrix_train.shape[0]))\n",
    "\n",
    "    ### Matrix generation for Test dataset\n",
    "    loan_feature_adjacency_matrix_test = np.concatenate([loan_feature_adjacency_matrix_train,loan_feature_adjacency_matrix_test])\n",
    "    history_feature_adjacency_matrix_test = np.concatenate([history_feature_adjacency_matrix_train,history_feature_adjacency_matrix_test])\n",
    "    soft_feature_adjacency_matrix_test = np.concatenate([soft_feature_adjacency_matrix_train,soft_feature_adjacency_matrix_test])\n",
    "\n",
    "    loan_feature_adjacency_matrix_test = np.concatenate((loan_feature_adjacency_matrix_test, np.zeros((loan_feature_adjacency_matrix_test.shape[0], loan_feature_distance_matrix_test.shape[0]))), axis = 1)\n",
    "    history_feature_adjacency_matrix_test = np.concatenate((history_feature_adjacency_matrix_test, np.zeros((history_feature_adjacency_matrix_test.shape[0], history_feature_distance_matrix_test.shape[0]))), axis = 1)\n",
    "    soft_feature_adjacency_matrix_test = np.concatenate((soft_feature_adjacency_matrix_test, np.zeros((soft_feature_adjacency_matrix_test.shape[0], soft_feature_distance_matrix_test.shape[0]))), axis = 1)\n",
    "\n",
    "    test_loan_adj = normalize(loan_feature_adjacency_matrix_test + sp.eye(loan_feature_adjacency_matrix_test.shape[0]))\n",
    "    test_history_adj = normalize(history_feature_adjacency_matrix_test + sp.eye(history_feature_adjacency_matrix_test.shape[0]))\n",
    "    test_soft_adj = normalize(soft_feature_adjacency_matrix_test + sp.eye(soft_feature_adjacency_matrix_test.shape[0]))\n",
    "\n",
    "\n",
    "    ### One-hot Encoding and Train X, Train Y   \n",
    "    temp_dummies = pd.get_dummies(train_dataset)\n",
    "    train_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "    base_category = temp_dummies.columns[[11, 13, 18, 19, 21, 24, 26, 29, 32, 34, 40, 43]]\n",
    "    train_x = Model_matrix(train_dataset, base_category)\n",
    "    train_x = train_x.drop('loan_status_Charged Off', axis = 1)\n",
    "\n",
    "    ### Train Node feature Matrix\n",
    "    loan_features = train_dataset.columns[0:7]\n",
    "    train_loan_features_dataset = train_dataset[loan_features] \n",
    "    history_features = train_dataset.columns[7:16]\n",
    "    train_history_features_dataset = train_dataset[history_features] \n",
    "    soft_features = train_dataset.columns[16:21]\n",
    "    train_soft_features_dataset = train_dataset[soft_features] \n",
    "    \n",
    "    loan_x = Model_matrix(train_loan_features_dataset, base_category)\n",
    "    history_x = Model_matrix(train_history_features_dataset, base_category)\n",
    "    soft_x = Model_matrix(train_soft_features_dataset, base_category)\n",
    "    DN_count_loan = Default_Neighbor_count(loan_feature_adjacency_matrix_train, train_y)\n",
    "    train_loan_node_feature = np.concatenate((DN_count_loan, loan_x), axis= 1)\n",
    "    DN_count_history = Default_Neighbor_count(history_feature_adjacency_matrix_train, train_y)\n",
    "    train_history_node_feature = np.concatenate((DN_count_history, history_x), axis= 1)\n",
    "    DN_count_soft = Default_Neighbor_count(soft_feature_adjacency_matrix_train, train_y)\n",
    "    train_soft_node_feature = np.concatenate((DN_count_soft, soft_x), axis= 1)\n",
    "\n",
    "    all_dataset = pd.concat([train_dataset, test_dataset])\n",
    "    all_dataset = all_dataset.reset_index()\n",
    "    all_dataset = all_dataset.drop(columns=['index'])\n",
    "\n",
    "    temp_dummies = pd.get_dummies(all_dataset)\n",
    "    test_y = list(temp_dummies['loan_status_Charged Off'])\n",
    "\n",
    "    ### Test Node feature matrix\n",
    "    loan_features = all_dataset.columns[0:7]\n",
    "    test_loan_features_dataset = all_dataset[loan_features] \n",
    "    history_features = all_dataset.columns[7:16]\n",
    "    test_history_features_dataset = all_dataset[history_features] \n",
    "    soft_features = all_dataset.columns[16:21]\n",
    "    test_soft_features_dataset = all_dataset[soft_features] \n",
    "    \n",
    "    loan_x = Model_matrix(test_loan_features_dataset, base_category)\n",
    "    history_x = Model_matrix(test_history_features_dataset, base_category)\n",
    "    soft_x = Model_matrix(test_soft_features_dataset, base_category)\n",
    "    \n",
    "    DN_count_loan = Default_Neighbor_count(loan_feature_adjacency_matrix_test, test_y)\n",
    "    test_loan_node_feature = np.concatenate((DN_count_loan, loan_x), axis= 1)\n",
    "    \n",
    "    DN_count_history = Default_Neighbor_count(history_feature_adjacency_matrix_test, test_y)\n",
    "    test_history_node_feature = np.concatenate((DN_count_history, history_x), axis= 1)\n",
    "    \n",
    "    DN_count_soft = Default_Neighbor_count(soft_feature_adjacency_matrix_test, test_y)\n",
    "    test_soft_node_feature = np.concatenate((DN_count_soft, soft_x), axis= 1)\n",
    "    \n",
    "    return(train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test = CV_dataset_Extraction(sample_dataset, loan_feature_distance_matrix_exercise, history_feature_distance_matrix_exercise, soft_feature_distance_matrix_exercise, \n",
    "            cv = 5)\n",
    "col_order = train_dataset.columns\n",
    "\n",
    "    ### normalize\n",
    "test_dataset = Numerical_min_max_for_test(train_dataset, test_dataset)\n",
    "train_dataset = Numerical_min_max(train_dataset)\n",
    "train_dataset = train_dataset[col_order]\n",
    "test_dataset = test_dataset[col_order]\n",
    "del col_order\n",
    "    \n",
    "train_loan_adj, train_history_adj, train_soft_adj, test_loan_adj, test_history_adj, test_soft_adj, train_loan_node_feature, train_history_node_feature, train_soft_node_feature, test_loan_node_feature, test_history_node_feature, test_soft_node_feature, base_category, train_y, test_y, DN_count_loan, DN_count_history, DN_count_soft = Matrix_Generation(loan_feature_distance_matrix_train, history_feature_distance_matrix_train, soft_feature_distance_matrix_train, loan_feature_distance_matrix_test, history_feature_distance_matrix_test, soft_feature_distance_matrix_test, train_dataset, test_dataset, \n",
    "    bandwidth = 20)\n",
    "\n",
    "test_loan_adj = torch.from_numpy(train_loan_adj).float().cuda()\n",
    "test_history_adj = torch.from_numpy(train_history_adj).float().cuda()\n",
    "test_soft_adj = torch.from_numpy(train_soft_adj).float().cuda()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loan_node_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "    \n",
    "### loan_network\n",
    "print('loan_network')\n",
    "test_history_node_feature = torch.from_numpy(train_history_node_feature).float().cuda()\n",
    "test_soft_node_feature = torch.from_numpy(train_soft_node_feature).float().cuda()\n",
    "train_loan_node_feature1 = train_loan_node_feature\n",
    "train_loan_node_feature1[:,0] = train_loan_node_feature1[:,0] - 0.1\n",
    "test_loan_node_feature = torch.from_numpy(train_loan_node_feature1).float().cuda()\n",
    "\n",
    "\n",
    "net.eval()\n",
    "outputs1 = net(test_loan_node_feature, test_history_node_feature, test_soft_node_feature, \n",
    "               test_loan_adj, test_history_adj, test_soft_adj)\n",
    "loss = criterion(outputs, outputs1)\n",
    "print('        MAE_loss: {:.4f}'.format(loss))\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### history_network\n",
    "print('history_network')\n",
    "test_loan_node_feature = torch.from_numpy(train_loan_node_feature).float().cuda()\n",
    "test_soft_node_feature = torch.from_numpy(train_soft_node_feature).float().cuda()\n",
    "train_history_node_feature1 = train_history_node_feature\n",
    "train_history_node_feature1[:,0] = train_history_node_feature1[:,0] - 0.1\n",
    "test_history_node_feature = torch.from_numpy(train_history_node_feature1).float().cuda()\n",
    "\n",
    "\n",
    "net.eval()\n",
    "outputs1 = net(test_loan_node_feature, test_history_node_feature, test_soft_node_feature, \n",
    "               test_loan_adj, test_history_adj, test_soft_adj)\n",
    "        \n",
    "loss = criterion(outputs, outputs1)\n",
    "print('        MAE_loss: {:.4f}'.format(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loan_node_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history_node_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_soft_node_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### soft_network\n",
    "print('soft_network')\n",
    "test_history_node_feature = torch.from_numpy(train_history_node_feature).float().cuda()\n",
    "test_loan_node_feature = torch.from_numpy(train_loan_node_feature).float().cuda()\n",
    "train_soft_node_feature1 = train_soft_node_feature\n",
    "train_soft_node_feature1[:,0] = train_soft_node_feature1[:,0] - 0.1\n",
    "test_soft_node_feature = torch.from_numpy(train_soft_node_feature1).float().cuda()\n",
    "\n",
    "\n",
    "net.eval()\n",
    "outputs1 = net(test_loan_node_feature, test_history_node_feature, test_soft_node_feature, \n",
    "               test_loan_adj, test_history_adj, test_soft_adj)\n",
    "        \n",
    "loss = criterion(outputs, outputs1)\n",
    "print('        MAE_loss: {:.4f}'.format(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history_node_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_soft_node_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
