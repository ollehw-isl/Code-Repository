{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_sampling(Dataset, indices_Short, indices_Long,  sample_num, seed):\n",
    "    np.random.seed(seed)\n",
    "    Short_sample = np.random.choice(indices_Short, sample_num, replace=False)\n",
    "    Long_sample = np.random.choice(indices_Long, sample_num, replace=False)\n",
    "    Dataset_sample = pd.concat([Dataset.iloc[list(Short_sample),:], Dataset.iloc[list(Long_sample),:]])\n",
    "    Dataset_sample = Dataset_sample.reset_index()\n",
    "    Dataset_sample = Dataset_sample.drop(columns=['index'])\n",
    "    return Short_sample, Long_sample, Dataset_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patent = pd.read_csv('Patent_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patent[\"lag_label\"].tolist().index(\"Short\")\n",
    "indices_Short = [i for i, x in enumerate(Patent[\"lag_label\"].tolist()) if x == \"Short\"]\n",
    "indices_Long = [i for i, x in enumerate(Patent[\"lag_label\"].tolist()) if x == \"Long\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Short_sample, Long_sample, Patent_test = Test_sampling(Patent, indices_Short, indices_Long,  500, 7214)\n",
    "samples = list(Short_sample) + list(Long_sample)\n",
    "Patent_train = Patent.loc[set(Patent.index) - set(samples)]\n",
    "train_index = list(set(Patent.index) - set(samples))\n",
    "Patent_train = Patent_train.reset_index()\n",
    "Patent_train = Patent_train.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patent_train_x = Patent_train.iloc[:,  2:27]\n",
    "Patent_train_y = list(Patent_train['lag_label'])\n",
    "\n",
    "Patent_test_x = Patent_test.iloc[:,  2:27]\n",
    "Patent_test_x=(Patent_test_x-Patent_train_x.min())/(Patent_train_x.max()-Patent_train_x.min())  \n",
    "Patent_train_x=(Patent_train_x-Patent_train_x.min())/(Patent_train_x.max()-Patent_train_x.min())  \n",
    "Patent_test_y = list(Patent_test['lag_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics    \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(model, Test_x, Test_y):\n",
    "    pred = model.predict(Test_x)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Test_y, model.predict_proba(Test_x)[:, 0], pos_label=\"Long\")\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print(\"Accuracy = {}\".format(metrics.accuracy_score(Test_y, pred)))\n",
    "    print(\"Precision = {}\".format(metrics.precision_score(Test_y, pred, pos_label=\"Short\")))\n",
    "    print(\"Recall = {}\".format(metrics.recall_score(Test_y, pred, pos_label=\"Short\")))\n",
    "    print(\"F1 score = {}\".format(metrics.f1_score(Test_y, pred, pos_label=\"Short\")))\n",
    "    print(\"AUC = {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LR\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(Patent_train_x, Patent_train_y)\n",
    "Evaluation(lr_model, Patent_test_x, Patent_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 5, 5), random_state=1)\n",
    "mlp_model.fit(Patent_train_x, Patent_train_y)\n",
    "Evaluation(mlp_model, Patent_test_x, Patent_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVM\n",
    "svm_model = SVC(C = 1, kernel = 'rbf',gamma = 0.1, probability = True)\n",
    "svm_model.fit(Patent_train_x, Patent_train_y)\n",
    "Evaluation(svm_model, Patent_test_x, Patent_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RF\n",
    "rf_model = RandomForestClassifier(n_estimators=1000, max_depth = 3, max_features = 5, random_state = 748)\n",
    "rf_model.fit(Patent_train_x, Patent_train_y)\n",
    "Evaluation(rf_model, Patent_test_x, Patent_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.653\n",
      "Precision = 0.71671388101983\n",
      "Recall = 0.506\n",
      "F1 score = 0.5932004689331769\n",
      "AUC = 0.7051999999999999\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(n_estimator = 1000, learning_rate = 0.1, max_depth = 5, gamma = 0.1, seed = 1771)\n",
    "xgb_model.fit(Patent_train_x, Patent_train_y)\n",
    "Evaluation(xgb_model, Patent_test_x, Patent_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adj mat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paper - Paper\n",
    "Paper_Paper = pd.read_csv('Paper_Paper.csv')\n",
    "\n",
    "unique_list = list(Paper_Paper['Appln_ID'])\n",
    "unique_list = list(set(unique_list))\n",
    "unique_list.sort()\n",
    "\n",
    "orders = train_index + samples\n",
    "Appln_ID = list(Patent.iloc[orders, :]['Appln_ID'])\n",
    "Paper_Paper_adj = np.identity(len(Patent))\n",
    "\n",
    "\n",
    "for x in unique_list:\n",
    "    row_index = Appln_ID.index(x)\n",
    "    cited_Paper_list = list(Paper_Paper.loc[Paper_Paper['Appln_ID'] == x, 'cited_Appln_ID'])\n",
    "    for y in cited_Paper_list:\n",
    "        col_index = Appln_ID.index(y)\n",
    "        Paper_Paper_adj[row_index,col_index] = 1 / (len(cited_Paper_list)+1)\n",
    "        Paper_Paper_adj[row_index,row_index] = 1 / (len(cited_Paper_list)+1)\n",
    "    \n",
    "\n",
    "### Paper - Applicant\n",
    "\n",
    "Paper_Applicant = pd.read_csv('Paper_Applicant.csv')\n",
    "Applicant_dictionary = pd.read_csv('Applicant_dictionary.csv')\n",
    "Applicant_ID = list(Applicant_dictionary['Applicant_ID'])\n",
    "\n",
    "unique_list = list(Paper_Applicant['Appln_ID'])\n",
    "unique_list = list(set(unique_list))\n",
    "unique_list.sort()\n",
    "\n",
    "Paper_Applicant_adj = np.zeros([len(Patent),len(Applicant_ID)])\n",
    "\n",
    "for x in unique_list:\n",
    "    row_index = Appln_ID.index(x)\n",
    "    Applicant_list = list(Paper_Applicant.loc[Paper_Applicant['Appln_ID'] == x, 'Applicant_ID'])\n",
    "    for y in Applicant_list:\n",
    "        col_index = Applicant_ID.index(y)\n",
    "        Paper_Applicant_adj[row_index,col_index] = 1 /len(Applicant_list)\n",
    "\n",
    "### Paper - Inventor\n",
    "Paper_Inventor = pd.read_csv('Paper_Inventor.csv')\n",
    "Inventor_dictionary = pd.read_csv('Inventor_dictionary.csv')\n",
    "Inventor_ID = list(Inventor_dictionary['Inventor_ID'])\n",
    "\n",
    "unique_list = list(Paper_Inventor['Appln_ID'])\n",
    "unique_list = list(set(unique_list))\n",
    "unique_list.sort()\n",
    "\n",
    "Paper_Inventor_adj = np.zeros([len(Patent),len(Inventor_ID)])\n",
    "\n",
    "for x in unique_list:\n",
    "    row_index = Appln_ID.index(x)\n",
    "    Inventor_list = list(Paper_Inventor.loc[Paper_Inventor['Appln_ID'] == x, 'Inventor_ID'])\n",
    "    for y in Inventor_list:\n",
    "        col_index = Inventor_ID.index(y)\n",
    "        Paper_Inventor_adj[row_index,col_index] = 1 /len(Inventor_list)\n",
    "        \n",
    "### Paper - Field\n",
    "Paper_Field = pd.read_csv('Paper_Field.csv')\n",
    "Field_ID = list(Paper_Field['IPC'])\n",
    "Field_ID = list(set(Field_ID))\n",
    "Field_ID.sort()\n",
    "\n",
    "unique_list = list(Paper_Field['Appln_ID'])\n",
    "unique_list = list(set(unique_list))\n",
    "unique_list.sort()\n",
    "\n",
    "Paper_Field_adj = np.zeros([len(Patent),len(Field_ID)])\n",
    "\n",
    "for x in unique_list:\n",
    "    row_index = Appln_ID.index(x)\n",
    "    Field_list = list(Paper_Field.loc[Paper_Field['Appln_ID'] == x, 'IPC'])\n",
    "    for y in Field_list:\n",
    "        col_index = Field_ID.index(y)\n",
    "        Paper_Field_adj[row_index,col_index] = 1/len(Field_list)\n",
    "\n",
    "        \n",
    "### Co-Applicant\n",
    "Applicant_Applicant = pd.read_csv('Applicant_Applicant.csv')\n",
    "Applicant_Applicant['Weight'] = Applicant_Applicant['Count'] / Applicant_Applicant['total']\n",
    "\n",
    "Applicant_Applicant_adj = np.identity(len(Applicant_ID))\n",
    "\n",
    "for i in range(len(Applicant_Applicant)):\n",
    "    temp_df = Applicant_Applicant.iloc[i,:]\n",
    "    row_index = Applicant_ID.index(temp_df['Applicant_ID'])\n",
    "    col_index = Applicant_ID.index(temp_df['Applicant2'])\n",
    "    weight = temp_df['Weight']\n",
    "    Applicant_Applicant_adj[row_index,col_index] = weight\n",
    "\n",
    "### Co-Inventor\n",
    "Inventor_Inventor = pd.read_csv('Inventor_Inventor.csv')\n",
    "Inventor_Inventor['Weight'] = Inventor_Inventor['Count'] / Inventor_Inventor['total']\n",
    "\n",
    "Inventor_Inventor_adj = np.identity(len(Inventor_ID))\n",
    "\n",
    "for i in range(len(Inventor_Inventor)):\n",
    "    temp_df = Inventor_Inventor.iloc[i,:]\n",
    "    row_index = Inventor_ID.index(temp_df['Inventor_ID'])\n",
    "    col_index = Inventor_ID.index(temp_df['Inventor2'])\n",
    "    weight = temp_df['Weight']\n",
    "    Inventor_Inventor_adj[row_index,col_index] = weight \n",
    "    \n",
    "\n",
    "### Co-Field\n",
    "Field_Field = pd.read_csv('Field_Field.csv')\n",
    "Field_Field['Weight'] = Field_Field['Count'] / Field_Field['total']\n",
    "\n",
    "Field_Field_adj = np.identity(len(Field_ID))\n",
    "\n",
    "for i in range(len(Field_Field)):\n",
    "    temp_df = Field_Field.iloc[i,:]\n",
    "    row_index = Field_ID.index(temp_df['Field'])\n",
    "    col_index = Field_ID.index(temp_df['Field2'])\n",
    "    weight = temp_df['Weight']\n",
    "    Field_Field_adj[row_index,col_index] = weight \n",
    "\n",
    "    \n",
    "        \n",
    "### Total adj\n",
    "Total_adj = np.concatenate((Paper_Paper_adj, Paper_Applicant_adj, Paper_Inventor_adj, Paper_Field_adj), axis= 1)\n",
    "temp_adj = np.concatenate((Paper_Applicant_adj.T, Applicant_Applicant_adj, np.zeros((len(Applicant_ID), len(Inventor_ID))), np.zeros((len(Applicant_ID), len(Field_ID)))), axis = 1)\n",
    "Total_adj = np.concatenate((Total_adj, temp_adj), axis = 0)\n",
    "temp_adj = np.concatenate((Paper_Inventor_adj.T, np.zeros((len(Inventor_ID), len(Applicant_ID))), Inventor_Inventor_adj, np.zeros((len(Inventor_ID), len(Field_ID)))), axis = 1)\n",
    "Total_adj = np.concatenate((Total_adj, temp_adj), axis = 0)\n",
    "temp_adj = np.concatenate((Paper_Field_adj.T, np.zeros((len(Field_ID), len(Applicant_ID))), np.zeros((len(Field_ID), len(Inventor_ID))), Field_Field_adj), axis = 1)\n",
    "Total_adj = np.concatenate((Total_adj, temp_adj), axis = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9999999999999998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Total_adj[4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node feature mat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node_feature = Patent.iloc[orders, 2:27]\n",
    "Node_feature=(Node_feature-Node_feature.min())/(Node_feature.max()-Node_feature.min())  \n",
    "Node_feature = Node_feature.values\n",
    "\n",
    "Node_feature = np.concatenate((Node_feature, np.zeros((2413, 25))), axis = 0)\n",
    "Node_feature = np.concatenate((Node_feature, np.concatenate((np.zeros((11849, 2413)), np.identity(2413)), axis = 0)), axis = 1)\n",
    "def get_multiple_elements_in_list(in_list, in_indices):\n",
    "    return [in_list[i] for i in in_indices]\n",
    "\n",
    "### y값\n",
    "temp_dummies = pd.get_dummies(Patent)\n",
    "all_y = list(temp_dummies['lag_label_Short'])\n",
    "\n",
    "\n",
    "train_index = list(set(Patent.index) - set(samples))\n",
    "\n",
    "test_y = get_multiple_elements_in_list(all_y, samples)\n",
    "train_y = get_multiple_elements_in_list(all_y, train_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14262, 2438)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Node_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.39285714, 0.10714286, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.03571429, 0.03571429, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.05357143, 0.21428571, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Node_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "######## Model consturction\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, act=None, bn=False):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.use_bn = bn\n",
    "        self.linear1 = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        self.linear2 = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        self.linear3 = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear3.weight)\n",
    "        self.linear4 = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear4.weight)\n",
    "        self.bn =nn.BatchNorm1d(out_dim)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x, adj, c1, c2, c3):\n",
    "        index = [i for i in range(0,(11849+c1))] + [i for i in range(12162,(12162+c2))] + [i for i in range(12871,(12871+c3))]\n",
    "                \n",
    "        out1 = self.linear1(x[0:11849, : ])\n",
    "        out2 = self.linear2(x[11849:(11849+c1), : ])\n",
    "        out3 = self.linear3(x[(11849+c1):(11849+c1+c2), : ])\n",
    "        out4 = self.linear4(x[(11849+c1+c2):(11849+c1+c2+c3), : ])\n",
    "        \n",
    "        \n",
    "        out1 = torch.matmul(adj[:, 0:11849], out1)\n",
    "        out2 = torch.matmul(adj[:, 11849:(11849+c1)], out2)\n",
    "        out3 = torch.matmul(adj[:, (11849+c1):(11849+c1+c2)], out3)\n",
    "        out4 = torch.matmul(adj[:, (11849+c1+c2):(11849+c1+c2+c3)], out4)\n",
    "        \n",
    "        out = out1 + out2 + out3 + out4\n",
    "        \n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out, adj\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias = False)\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        out = in_x + out_x\n",
    "        return out\n",
    "\n",
    "\n",
    "class Highway(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Highway, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        z = self.gate_coefficient(in_x)\n",
    "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
    "        return out\n",
    "            \n",
    "    def gate_coefficient(self, in_x):\n",
    "        x1 = self.linear_coef_in(in_x)\n",
    "        return self.sigmoid(x1)\n",
    "\n",
    "\n",
    "\n",
    "class GatedSkipConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GatedSkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
    "        self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        z = self.gate_coefficient(in_x, out_x)\n",
    "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
    "        return out\n",
    "            \n",
    "    def gate_coefficient(self, in_x, out_x):\n",
    "        x1 = self.linear_coef_in(in_x)\n",
    "        x2 = self.linear_coef_out(out_x)\n",
    "        return self.sigmoid(x1+x2)\n",
    "\n",
    "\n",
    "\n",
    "class GCNBlock(nn.Module):\n",
    "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim, bn = True, sc = 'sc'):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,\n",
    "                                        hidden_dim if i==n_layer-1 else hidden_dim,\n",
    "                                        nn.ReLU(),\n",
    "                                        True if i!=n_layer-1 else False))\n",
    "        if sc=='sc':\n",
    "            self.sc = SkipConnection(in_dim, out_dim)\n",
    "        elif sc == 'no':\n",
    "            self.sc = None\n",
    "        elif sc == 'gc':\n",
    "            self.sc = GatedSkipConnection(in_dim, out_dim)\n",
    "        elif sc == 'hw':\n",
    "            self.sc = Highway(in_dim, out_dim)\n",
    "        else:\n",
    "            assert False, \"Wrong sc type.\"\n",
    "            \n",
    "    def forward(self, x, adj, c1, c2, c3):\n",
    "        residual = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out, adj = layer((x if i==0 else out), adj, c1, c2, c3)    \n",
    "        if self.sc != None:\n",
    "            out = self.sc(residual, out)\n",
    "        return out, adj\n",
    "\n",
    "            \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, act = None):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim, self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks.append(GCNBlock(args.n_layer,\n",
    "                                        args.input_dim if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,   ### output dim 따로\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        self.pred1 = Classifier(args.hidden_dim, args.pred_dim1, act = nn.ReLU())\n",
    "        self.pred2 = Classifier(args.pred_dim1, args.pred_dim2,act = nn.ReLU())\n",
    "        self.pred3 = Classifier(args.pred_dim2, args.pred_dim3,act = nn.ReLU())\n",
    "        self.pred4 = Classifier(args.pred_dim3, 1,act = nn.Sigmoid())\n",
    "\n",
    "    def forward(self, node_feature, adj, c1, c2, c3):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, adj = block((node_feature if i==0 else out), adj, c1, c2, c3)\n",
    "        out = self.pred1(out)\n",
    "        out = self.pred2(out)\n",
    "        out = self.pred3(out)\n",
    "        out = self.pred4(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, args, criterion):\n",
    "    t = time.time()\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(node_feature, Adj, c1, c2, c3)\n",
    "    loss = criterion(outputs[0:10849], train_y.float())\n",
    "    \n",
    "    pred = (outputs > 0.5).float()\n",
    "    pred_train = pred[0:10849]\n",
    "    pred_test = pred[10849:11849]\n",
    "    correct = (pred_train.transpose(0,1) == train_y.float()).float().sum()\n",
    "    acc_train = correct / len(train_y)\n",
    "    correct = (pred_test.transpose(0,1) == test_y.float()).float().sum()\n",
    "    acc_test = correct / len(test_y)\n",
    "    pred_list = pred_test.tolist()\n",
    "    test_y_list = test_y.tolist()\n",
    "    f1_score = metrics.f1_score(test_y_list, pred_list, pos_label= 1)\n",
    "    precision = metrics.precision_score(test_y_list, pred_list, pos_label=1)\n",
    "    recall = metrics.recall_score(test_y_list, pred_list, pos_label=1)\n",
    "    outputs_list = outputs.tolist()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_y_list, outputs_list[10849:11849], pos_label=1)\n",
    "\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'acc_train: {:.4f}'.format(acc_train),\n",
    "          'acc_test: {:.4f}'.format(acc_test),\n",
    "          'f1_test: {:.4f}'.format(f1_score),\n",
    "          'precision: {:.4f}'.format(precision),\n",
    "          'recall: {:.4f}'.format(recall)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 313\n",
    "c2 = 479\n",
    "# 340\n",
    "# 479\n",
    "# 709\n",
    "c3 = 431\n",
    "#c3 = 431\n",
    "#c3 = 835\n",
    "#c3 = 1391\n",
    "input_dim = 25 + c1 + c2 + c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i for i in range(0,(11849+c1))] + [i for i in range(12162,(12162+c2))] + [i for i in range(12871,(12871+c3))]\n",
    "Total_adj1 = Total_adj[index, :][:, index]       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13072, 13072)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Total_adj1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14285714, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.1       , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.11111111, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.3       ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Total_adj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2 = [i for i in range(0,(25+c1))] + [i for i in range(338,(338+c2))] + [i for i in range(1047,(1047+c3))]\n",
    "Node_feature1 = Node_feature[index, :][:, index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13072, 1248)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Node_feature1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1248"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({ \"seed\": 714, \"epochs\": 1000, \"lr\": 0.0001, \"weight_decay\": 5e-4, \"n_layer\": 1, \n",
    "                                         \"n_block\": 2 , \"input_dim\": input_dim , \"hidden_dim\": 400 , \"pred_dim1\": 100, \"pred_dim2\": 25, \n",
    "                                         \"pred_dim3\": 25, \"bn\": True ,\"sc\": \"gc\",\"cuda\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b1978c4850>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.manual_seed(710)\n",
    "torch.manual_seed(710)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seed: 347 Epoch: 584 acc_train: 0.7447 acc_test: 0.6800 f1_test: 0.6617 auc_test: 0.7055\n",
    "### Seed: 710 Epoch: 697 acc_train: 0.7435 acc_test: 0.6820 f1_test: 0.6755 auc_test: 0.7047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNNet(\n",
       "  (blocks): ModuleList(\n",
       "    (0): GCNBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (linear1): Linear(in_features=1248, out_features=400, bias=True)\n",
       "          (linear2): Linear(in_features=1248, out_features=400, bias=True)\n",
       "          (linear3): Linear(in_features=1248, out_features=400, bias=True)\n",
       "          (linear4): Linear(in_features=1248, out_features=400, bias=True)\n",
       "          (bn): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (sc): GatedSkipConnection(\n",
       "        (linear): Linear(in_features=1248, out_features=400, bias=False)\n",
       "        (linear_coef_in): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (linear_coef_out): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (1): GCNBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): GCNLayer(\n",
       "          (linear1): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (linear2): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (linear3): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (linear4): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (bn): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (sc): GatedSkipConnection(\n",
       "        (linear): Linear(in_features=400, out_features=400, bias=False)\n",
       "        (linear_coef_in): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (linear_coef_out): Linear(in_features=400, out_features=400, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pred1): Classifier(\n",
       "    (linear): Linear(in_features=400, out_features=100, bias=True)\n",
       "    (activation): ReLU()\n",
       "  )\n",
       "  (pred2): Classifier(\n",
       "    (linear): Linear(in_features=100, out_features=25, bias=True)\n",
       "    (activation): ReLU()\n",
       "  )\n",
       "  (pred3): Classifier(\n",
       "    (linear): Linear(in_features=25, out_features=25, bias=True)\n",
       "    (activation): ReLU()\n",
       "  )\n",
       "  (pred4): Classifier(\n",
       "    (linear): Linear(in_features=25, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.act = nn.ReLU()\n",
    "criterion = nn.BCELoss()\n",
    "net = GCNNet(args)  \n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "net.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj = torch.from_numpy(Total_adj1).float().cuda()\n",
    "train_y = torch.tensor(train_y, dtype=torch.long).cuda()\n",
    "node_feature = torch.from_numpy(Node_feature1).float().cuda()\n",
    "test_y = torch.tensor(test_y, dtype=torch.long).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adj = torch.from_numpy(Total_adj1).float().cuda()\n",
    "node_feature = torch.from_numpy(Node_feature1).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:512: UserWarning: Using a target size (torch.Size([10849])) that is different to the input size (torch.Size([10849, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 acc_train: 0.5042 acc_test: 0.4900 f1_test: 0.4283 precision: 0.4872 recall: 0.3820\n",
      "Epoch: 0002 acc_train: 0.5004 acc_test: 0.4980 f1_test: 0.4814 precision: 0.4979 recall: 0.4660\n",
      "Epoch: 0003 acc_train: 0.4852 acc_test: 0.5310 f1_test: 0.5995 precision: 0.5231 recall: 0.7020\n",
      "Epoch: 0004 acc_train: 0.4909 acc_test: 0.5330 f1_test: 0.5971 precision: 0.5250 recall: 0.6920\n",
      "Epoch: 0005 acc_train: 0.5075 acc_test: 0.5490 f1_test: 0.5948 precision: 0.5400 recall: 0.6620\n",
      "Epoch: 0006 acc_train: 0.5262 acc_test: 0.5520 f1_test: 0.5608 precision: 0.5500 recall: 0.5720\n",
      "Epoch: 0007 acc_train: 0.5519 acc_test: 0.5530 f1_test: 0.5209 precision: 0.5612 recall: 0.4860\n",
      "Epoch: 0008 acc_train: 0.5918 acc_test: 0.5510 f1_test: 0.4280 precision: 0.5895 recall: 0.3360\n",
      "Epoch: 0009 acc_train: 0.6041 acc_test: 0.5680 f1_test: 0.3881 precision: 0.6650 recall: 0.2740\n",
      "Epoch: 0010 acc_train: 0.6096 acc_test: 0.5590 f1_test: 0.3328 precision: 0.6832 recall: 0.2200\n",
      "Epoch: 0011 acc_train: 0.6038 acc_test: 0.5550 f1_test: 0.2834 precision: 0.7273 recall: 0.1760\n",
      "Epoch: 0012 acc_train: 0.5989 acc_test: 0.5470 f1_test: 0.2488 precision: 0.7282 recall: 0.1500\n",
      "Epoch: 0013 acc_train: 0.6002 acc_test: 0.5520 f1_test: 0.2656 precision: 0.7364 recall: 0.1620\n",
      "Epoch: 0014 acc_train: 0.6001 acc_test: 0.5550 f1_test: 0.2903 precision: 0.7165 recall: 0.1820\n",
      "Epoch: 0015 acc_train: 0.6007 acc_test: 0.5530 f1_test: 0.2871 precision: 0.7087 recall: 0.1800\n",
      "Epoch: 0016 acc_train: 0.6001 acc_test: 0.5580 f1_test: 0.3262 precision: 0.6859 recall: 0.2140\n",
      "Epoch: 0017 acc_train: 0.6008 acc_test: 0.5620 f1_test: 0.3323 precision: 0.6987 recall: 0.2180\n",
      "Epoch: 0018 acc_train: 0.6011 acc_test: 0.5610 f1_test: 0.3257 precision: 0.7020 recall: 0.2120\n",
      "Epoch: 0019 acc_train: 0.6007 acc_test: 0.5600 f1_test: 0.3210 precision: 0.7027 recall: 0.2080\n",
      "Epoch: 0020 acc_train: 0.6022 acc_test: 0.5560 f1_test: 0.2839 precision: 0.7333 recall: 0.1760\n",
      "Epoch: 0021 acc_train: 0.6014 acc_test: 0.5580 f1_test: 0.2894 precision: 0.7377 recall: 0.1800\n",
      "Epoch: 0022 acc_train: 0.6014 acc_test: 0.5580 f1_test: 0.2894 precision: 0.7377 recall: 0.1800\n",
      "Epoch: 0023 acc_train: 0.6029 acc_test: 0.5590 f1_test: 0.2921 precision: 0.7398 recall: 0.1820\n",
      "Epoch: 0024 acc_train: 0.6040 acc_test: 0.5590 f1_test: 0.2944 precision: 0.7360 recall: 0.1840\n",
      "Epoch: 0025 acc_train: 0.6040 acc_test: 0.5610 f1_test: 0.3065 precision: 0.7293 recall: 0.1940\n",
      "Epoch: 0026 acc_train: 0.6051 acc_test: 0.5600 f1_test: 0.3082 precision: 0.7206 recall: 0.1960\n",
      "Epoch: 0027 acc_train: 0.6037 acc_test: 0.5620 f1_test: 0.3482 precision: 0.6802 recall: 0.2340\n",
      "Epoch: 0028 acc_train: 0.6047 acc_test: 0.5640 f1_test: 0.3531 precision: 0.6839 recall: 0.2380\n",
      "Epoch: 0029 acc_train: 0.6060 acc_test: 0.5630 f1_test: 0.3526 precision: 0.6800 recall: 0.2380\n",
      "Epoch: 0030 acc_train: 0.6084 acc_test: 0.5640 f1_test: 0.3434 precision: 0.6951 recall: 0.2280\n",
      "Epoch: 0031 acc_train: 0.6079 acc_test: 0.5600 f1_test: 0.3210 precision: 0.7027 recall: 0.2080\n",
      "Epoch: 0032 acc_train: 0.6078 acc_test: 0.5600 f1_test: 0.3210 precision: 0.7027 recall: 0.2080\n",
      "Epoch: 0033 acc_train: 0.6078 acc_test: 0.5600 f1_test: 0.3210 precision: 0.7027 recall: 0.2080\n",
      "Epoch: 0034 acc_train: 0.6084 acc_test: 0.5590 f1_test: 0.3247 precision: 0.6928 recall: 0.2120\n",
      "Epoch: 0035 acc_train: 0.6086 acc_test: 0.5580 f1_test: 0.3242 precision: 0.6883 recall: 0.2120\n",
      "Epoch: 0036 acc_train: 0.6095 acc_test: 0.5590 f1_test: 0.3267 precision: 0.6903 recall: 0.2140\n",
      "Epoch: 0037 acc_train: 0.6106 acc_test: 0.5660 f1_test: 0.3522 precision: 0.6941 recall: 0.2360\n",
      "Epoch: 0038 acc_train: 0.6073 acc_test: 0.5650 f1_test: 0.3668 precision: 0.6738 recall: 0.2520\n",
      "Epoch: 0039 acc_train: 0.6105 acc_test: 0.5650 f1_test: 0.3536 precision: 0.6879 recall: 0.2380\n",
      "Epoch: 0040 acc_train: 0.6096 acc_test: 0.5590 f1_test: 0.3308 precision: 0.6855 recall: 0.2180\n",
      "Epoch: 0041 acc_train: 0.6095 acc_test: 0.5590 f1_test: 0.3267 precision: 0.6903 recall: 0.2140\n",
      "Epoch: 0042 acc_train: 0.6099 acc_test: 0.5580 f1_test: 0.3242 precision: 0.6883 recall: 0.2120\n",
      "Epoch: 0043 acc_train: 0.6104 acc_test: 0.5580 f1_test: 0.3242 precision: 0.6883 recall: 0.2120\n",
      "Epoch: 0044 acc_train: 0.6103 acc_test: 0.5630 f1_test: 0.3369 precision: 0.6981 recall: 0.2220\n",
      "Epoch: 0045 acc_train: 0.6101 acc_test: 0.5610 f1_test: 0.3359 precision: 0.6894 recall: 0.2220\n",
      "Epoch: 0046 acc_train: 0.6105 acc_test: 0.5610 f1_test: 0.3359 precision: 0.6894 recall: 0.2220\n",
      "Epoch: 0047 acc_train: 0.6102 acc_test: 0.5610 f1_test: 0.3379 precision: 0.6871 recall: 0.2240\n",
      "Epoch: 0048 acc_train: 0.6105 acc_test: 0.5600 f1_test: 0.3373 precision: 0.6829 recall: 0.2240\n",
      "Epoch: 0049 acc_train: 0.6107 acc_test: 0.5590 f1_test: 0.3368 precision: 0.6788 recall: 0.2240\n",
      "Epoch: 0050 acc_train: 0.6113 acc_test: 0.5600 f1_test: 0.3393 precision: 0.6807 recall: 0.2260\n",
      "Epoch: 0051 acc_train: 0.6116 acc_test: 0.5600 f1_test: 0.3393 precision: 0.6807 recall: 0.2260\n",
      "Epoch: 0052 acc_train: 0.6119 acc_test: 0.5600 f1_test: 0.3393 precision: 0.6807 recall: 0.2260\n",
      "Epoch: 0053 acc_train: 0.6131 acc_test: 0.5600 f1_test: 0.3413 precision: 0.6786 recall: 0.2280\n",
      "Epoch: 0054 acc_train: 0.6117 acc_test: 0.5600 f1_test: 0.3393 precision: 0.6807 recall: 0.2260\n",
      "Epoch: 0055 acc_train: 0.6120 acc_test: 0.5600 f1_test: 0.3393 precision: 0.6807 recall: 0.2260\n",
      "Epoch: 0056 acc_train: 0.6120 acc_test: 0.5600 f1_test: 0.3393 precision: 0.6807 recall: 0.2260\n",
      "Epoch: 0057 acc_train: 0.6124 acc_test: 0.5600 f1_test: 0.3393 precision: 0.6807 recall: 0.2260\n",
      "Epoch: 0058 acc_train: 0.6131 acc_test: 0.5640 f1_test: 0.3493 precision: 0.6882 recall: 0.2340\n",
      "Epoch: 0059 acc_train: 0.6134 acc_test: 0.5650 f1_test: 0.3517 precision: 0.6901 recall: 0.2360\n",
      "Epoch: 0060 acc_train: 0.6139 acc_test: 0.5640 f1_test: 0.3512 precision: 0.6860 recall: 0.2360\n",
      "Epoch: 0061 acc_train: 0.6138 acc_test: 0.5630 f1_test: 0.3507 precision: 0.6821 recall: 0.2360\n",
      "Epoch: 0062 acc_train: 0.6146 acc_test: 0.5650 f1_test: 0.3594 precision: 0.6816 recall: 0.2440\n",
      "Epoch: 0063 acc_train: 0.6155 acc_test: 0.5660 f1_test: 0.3618 precision: 0.6833 recall: 0.2460\n",
      "Epoch: 0064 acc_train: 0.6168 acc_test: 0.5660 f1_test: 0.3618 precision: 0.6833 recall: 0.2460\n",
      "Epoch: 0065 acc_train: 0.6171 acc_test: 0.5680 f1_test: 0.3666 precision: 0.6868 recall: 0.2500\n",
      "Epoch: 0066 acc_train: 0.6174 acc_test: 0.5680 f1_test: 0.3666 precision: 0.6868 recall: 0.2500\n",
      "Epoch: 0067 acc_train: 0.6178 acc_test: 0.5690 f1_test: 0.3690 precision: 0.6885 recall: 0.2520\n",
      "Epoch: 0068 acc_train: 0.6185 acc_test: 0.5720 f1_test: 0.3761 precision: 0.6935 recall: 0.2580\n",
      "Epoch: 0069 acc_train: 0.6195 acc_test: 0.5720 f1_test: 0.3761 precision: 0.6935 recall: 0.2580\n",
      "Epoch: 0070 acc_train: 0.6197 acc_test: 0.5720 f1_test: 0.3761 precision: 0.6935 recall: 0.2580\n",
      "Epoch: 0071 acc_train: 0.6201 acc_test: 0.5720 f1_test: 0.3761 precision: 0.6935 recall: 0.2580\n",
      "Epoch: 0072 acc_train: 0.6200 acc_test: 0.5720 f1_test: 0.3779 precision: 0.6915 recall: 0.2600\n",
      "Epoch: 0073 acc_train: 0.6203 acc_test: 0.5720 f1_test: 0.3779 precision: 0.6915 recall: 0.2600\n",
      "Epoch: 0074 acc_train: 0.6205 acc_test: 0.5720 f1_test: 0.3797 precision: 0.6895 recall: 0.2620\n",
      "Epoch: 0075 acc_train: 0.6218 acc_test: 0.5720 f1_test: 0.3815 precision: 0.6875 recall: 0.2640\n",
      "Epoch: 0076 acc_train: 0.6207 acc_test: 0.5710 f1_test: 0.3792 precision: 0.6859 recall: 0.2620\n",
      "Epoch: 0077 acc_train: 0.6211 acc_test: 0.5720 f1_test: 0.3815 precision: 0.6875 recall: 0.2640\n",
      "Epoch: 0078 acc_train: 0.6213 acc_test: 0.5730 f1_test: 0.3838 precision: 0.6891 recall: 0.2660\n",
      "Epoch: 0079 acc_train: 0.6213 acc_test: 0.5740 f1_test: 0.3862 precision: 0.6907 recall: 0.2680\n",
      "Epoch: 0080 acc_train: 0.6210 acc_test: 0.5730 f1_test: 0.3821 precision: 0.6911 recall: 0.2640\n",
      "Epoch: 0081 acc_train: 0.6209 acc_test: 0.5730 f1_test: 0.3821 precision: 0.6911 recall: 0.2640\n",
      "Epoch: 0082 acc_train: 0.6221 acc_test: 0.5730 f1_test: 0.3838 precision: 0.6891 recall: 0.2660\n",
      "Epoch: 0083 acc_train: 0.6216 acc_test: 0.5760 f1_test: 0.3890 precision: 0.6959 recall: 0.2700\n",
      "Epoch: 0084 acc_train: 0.6214 acc_test: 0.5750 f1_test: 0.3885 precision: 0.6923 recall: 0.2700\n",
      "Epoch: 0085 acc_train: 0.6225 acc_test: 0.5750 f1_test: 0.3902 precision: 0.6904 recall: 0.2720\n",
      "Epoch: 0086 acc_train: 0.6233 acc_test: 0.5740 f1_test: 0.3914 precision: 0.6850 recall: 0.2740\n",
      "Epoch: 0087 acc_train: 0.6228 acc_test: 0.5760 f1_test: 0.3960 precision: 0.6881 recall: 0.2780\n",
      "Epoch: 0088 acc_train: 0.6238 acc_test: 0.5770 f1_test: 0.3983 precision: 0.6897 recall: 0.2800\n",
      "Epoch: 0089 acc_train: 0.6240 acc_test: 0.5770 f1_test: 0.3983 precision: 0.6897 recall: 0.2800\n",
      "Epoch: 0090 acc_train: 0.6245 acc_test: 0.5770 f1_test: 0.3983 precision: 0.6897 recall: 0.2800\n",
      "Epoch: 0091 acc_train: 0.6259 acc_test: 0.5770 f1_test: 0.4000 precision: 0.6878 recall: 0.2820\n",
      "Epoch: 0092 acc_train: 0.6270 acc_test: 0.5760 f1_test: 0.3994 precision: 0.6845 recall: 0.2820\n",
      "Epoch: 0093 acc_train: 0.6270 acc_test: 0.5770 f1_test: 0.4000 precision: 0.6878 recall: 0.2820\n",
      "Epoch: 0094 acc_train: 0.6274 acc_test: 0.5750 f1_test: 0.3989 precision: 0.6812 recall: 0.2820\n",
      "Epoch: 0095 acc_train: 0.6277 acc_test: 0.5760 f1_test: 0.4028 precision: 0.6810 recall: 0.2860\n",
      "Epoch: 0096 acc_train: 0.6288 acc_test: 0.5760 f1_test: 0.4028 precision: 0.6810 recall: 0.2860\n",
      "Epoch: 0097 acc_train: 0.6285 acc_test: 0.5750 f1_test: 0.4023 precision: 0.6777 recall: 0.2860\n",
      "Epoch: 0098 acc_train: 0.6293 acc_test: 0.5770 f1_test: 0.4051 precision: 0.6825 recall: 0.2880\n",
      "Epoch: 0099 acc_train: 0.6296 acc_test: 0.5770 f1_test: 0.4051 precision: 0.6825 recall: 0.2880\n",
      "Epoch: 0100 acc_train: 0.6307 acc_test: 0.5780 f1_test: 0.4073 precision: 0.6840 recall: 0.2900\n",
      "Epoch: 0101 acc_train: 0.6303 acc_test: 0.5790 f1_test: 0.4095 precision: 0.6854 recall: 0.2920\n",
      "Epoch: 0102 acc_train: 0.6313 acc_test: 0.5820 f1_test: 0.4162 precision: 0.6898 recall: 0.2980\n",
      "Epoch: 0103 acc_train: 0.6301 acc_test: 0.5790 f1_test: 0.4112 precision: 0.6837 recall: 0.2940\n",
      "Epoch: 0104 acc_train: 0.6322 acc_test: 0.5950 f1_test: 0.4549 precision: 0.6955 recall: 0.3380\n",
      "Epoch: 0105 acc_train: 0.6295 acc_test: 0.5790 f1_test: 0.4095 precision: 0.6854 recall: 0.2920\n",
      "Epoch: 0106 acc_train: 0.6331 acc_test: 0.5870 f1_test: 0.4350 precision: 0.6883 recall: 0.3180\n",
      "Epoch: 0107 acc_train: 0.6340 acc_test: 0.5920 f1_test: 0.4486 precision: 0.6917 recall: 0.3320\n",
      "Epoch: 0108 acc_train: 0.6299 acc_test: 0.5790 f1_test: 0.4128 precision: 0.6820 recall: 0.2960\n",
      "Epoch: 0109 acc_train: 0.6345 acc_test: 0.5920 f1_test: 0.4426 precision: 0.6983 recall: 0.3240\n",
      "Epoch: 0110 acc_train: 0.6357 acc_test: 0.5940 f1_test: 0.4499 precision: 0.6975 recall: 0.3320\n",
      "Epoch: 0111 acc_train: 0.6311 acc_test: 0.5810 f1_test: 0.4172 precision: 0.6849 recall: 0.3000\n",
      "Epoch: 0112 acc_train: 0.6360 acc_test: 0.5930 f1_test: 0.4493 precision: 0.6946 recall: 0.3320\n",
      "Epoch: 0113 acc_train: 0.6359 acc_test: 0.5880 f1_test: 0.4356 precision: 0.6913 recall: 0.3180\n",
      "Epoch: 0114 acc_train: 0.6331 acc_test: 0.5810 f1_test: 0.4205 precision: 0.6816 recall: 0.3040\n",
      "Epoch: 0115 acc_train: 0.6376 acc_test: 0.5940 f1_test: 0.4514 precision: 0.6958 recall: 0.3340\n",
      "Epoch: 0116 acc_train: 0.6358 acc_test: 0.5840 f1_test: 0.4270 precision: 0.6858 recall: 0.3100\n",
      "Epoch: 0117 acc_train: 0.6368 acc_test: 0.5870 f1_test: 0.4335 precision: 0.6900 recall: 0.3160\n",
      "Epoch: 0118 acc_train: 0.6385 acc_test: 0.5960 f1_test: 0.4555 precision: 0.6983 recall: 0.3380\n",
      "Epoch: 0119 acc_train: 0.6370 acc_test: 0.5870 f1_test: 0.4288 precision: 0.6951 recall: 0.3100\n",
      "Epoch: 0120 acc_train: 0.6390 acc_test: 0.5940 f1_test: 0.4484 precision: 0.6992 recall: 0.3300\n",
      "Epoch: 0121 acc_train: 0.6389 acc_test: 0.5940 f1_test: 0.4484 precision: 0.6992 recall: 0.3300\n",
      "Epoch: 0122 acc_train: 0.6384 acc_test: 0.5870 f1_test: 0.4319 precision: 0.6916 recall: 0.3140\n",
      "Epoch: 0123 acc_train: 0.6417 acc_test: 0.5970 f1_test: 0.4576 precision: 0.6996 recall: 0.3400\n",
      "Epoch: 0124 acc_train: 0.6396 acc_test: 0.5920 f1_test: 0.4426 precision: 0.6983 recall: 0.3240\n",
      "Epoch: 0125 acc_train: 0.6408 acc_test: 0.5940 f1_test: 0.4484 precision: 0.6992 recall: 0.3300\n",
      "Epoch: 0126 acc_train: 0.6405 acc_test: 0.5940 f1_test: 0.4484 precision: 0.6992 recall: 0.3300\n",
      "Epoch: 0127 acc_train: 0.6396 acc_test: 0.5910 f1_test: 0.4405 precision: 0.6970 recall: 0.3220\n",
      "Epoch: 0128 acc_train: 0.6421 acc_test: 0.5970 f1_test: 0.4605 precision: 0.6964 recall: 0.3440\n",
      "Epoch: 0129 acc_train: 0.6394 acc_test: 0.5900 f1_test: 0.4384 precision: 0.6957 recall: 0.3200\n",
      "Epoch: 0130 acc_train: 0.6423 acc_test: 0.5960 f1_test: 0.4613 precision: 0.6920 recall: 0.3460\n",
      "Epoch: 0131 acc_train: 0.6406 acc_test: 0.5890 f1_test: 0.4393 precision: 0.6910 recall: 0.3220\n",
      "Epoch: 0132 acc_train: 0.6425 acc_test: 0.5960 f1_test: 0.4599 precision: 0.6935 recall: 0.3440\n",
      "Epoch: 0133 acc_train: 0.6413 acc_test: 0.5970 f1_test: 0.4576 precision: 0.6996 recall: 0.3400\n",
      "Epoch: 0134 acc_train: 0.6423 acc_test: 0.5940 f1_test: 0.4514 precision: 0.6958 recall: 0.3340\n",
      "Epoch: 0135 acc_train: 0.6437 acc_test: 0.5950 f1_test: 0.4593 precision: 0.6908 recall: 0.3440\n",
      "Epoch: 0136 acc_train: 0.6416 acc_test: 0.5920 f1_test: 0.4472 precision: 0.6933 recall: 0.3300\n",
      "Epoch: 0137 acc_train: 0.6436 acc_test: 0.5970 f1_test: 0.4676 precision: 0.6887 recall: 0.3540\n",
      "Epoch: 0138 acc_train: 0.6407 acc_test: 0.5920 f1_test: 0.4441 precision: 0.6966 recall: 0.3260\n",
      "Epoch: 0139 acc_train: 0.6449 acc_test: 0.5970 f1_test: 0.4787 precision: 0.6777 recall: 0.3700\n",
      "Epoch: 0140 acc_train: 0.6380 acc_test: 0.5840 f1_test: 0.4190 precision: 0.6944 recall: 0.3000\n",
      "Epoch: 0141 acc_train: 0.6460 acc_test: 0.5990 f1_test: 0.4852 precision: 0.6774 recall: 0.3780\n",
      "Epoch: 0142 acc_train: 0.6437 acc_test: 0.5940 f1_test: 0.4543 precision: 0.6926 recall: 0.3380\n",
      "Epoch: 0143 acc_train: 0.6442 acc_test: 0.5950 f1_test: 0.4549 precision: 0.6955 recall: 0.3380\n",
      "Epoch: 0144 acc_train: 0.6472 acc_test: 0.6000 f1_test: 0.4859 precision: 0.6799 recall: 0.3780\n",
      "Epoch: 0145 acc_train: 0.6438 acc_test: 0.5930 f1_test: 0.4478 precision: 0.6962 recall: 0.3300\n",
      "Epoch: 0146 acc_train: 0.6476 acc_test: 0.5980 f1_test: 0.4738 precision: 0.6856 recall: 0.3620\n",
      "Epoch: 0147 acc_train: 0.6484 acc_test: 0.5980 f1_test: 0.4724 precision: 0.6870 recall: 0.3600\n",
      "Epoch: 0148 acc_train: 0.6455 acc_test: 0.5940 f1_test: 0.4528 precision: 0.6942 recall: 0.3360\n",
      "Epoch: 0149 acc_train: 0.6503 acc_test: 0.6050 f1_test: 0.4929 precision: 0.6882 recall: 0.3840\n",
      "Epoch: 0150 acc_train: 0.6498 acc_test: 0.6000 f1_test: 0.4709 precision: 0.6953 recall: 0.3560\n",
      "Epoch: 0151 acc_train: 0.6525 acc_test: 0.5980 f1_test: 0.4779 precision: 0.6815 recall: 0.3680\n",
      "Epoch: 0152 acc_train: 0.6552 acc_test: 0.6100 f1_test: 0.5113 precision: 0.6846 recall: 0.4080\n",
      "Epoch: 0153 acc_train: 0.6513 acc_test: 0.6020 f1_test: 0.4804 precision: 0.6917 recall: 0.3680\n",
      "Epoch: 0154 acc_train: 0.6553 acc_test: 0.6190 f1_test: 0.5302 precision: 0.6913 recall: 0.4300\n",
      "Epoch: 0155 acc_train: 0.6537 acc_test: 0.6070 f1_test: 0.4955 precision: 0.6918 recall: 0.3860\n",
      "Epoch: 0156 acc_train: 0.6565 acc_test: 0.6150 f1_test: 0.5205 precision: 0.6898 recall: 0.4180\n",
      "Epoch: 0157 acc_train: 0.6579 acc_test: 0.6140 f1_test: 0.5163 precision: 0.6913 recall: 0.4120\n",
      "Epoch: 0158 acc_train: 0.6574 acc_test: 0.6150 f1_test: 0.5157 precision: 0.6949 recall: 0.4100\n",
      "Epoch: 0159 acc_train: 0.6579 acc_test: 0.6200 f1_test: 0.5309 precision: 0.6935 recall: 0.4300\n",
      "Epoch: 0160 acc_train: 0.6560 acc_test: 0.6060 f1_test: 0.4974 precision: 0.6866 recall: 0.3900\n",
      "Epoch: 0161 acc_train: 0.6585 acc_test: 0.6220 f1_test: 0.5401 precision: 0.6894 recall: 0.4440\n",
      "Epoch: 0162 acc_train: 0.6548 acc_test: 0.6060 f1_test: 0.4910 precision: 0.6934 recall: 0.3800\n",
      "Epoch: 0163 acc_train: 0.6585 acc_test: 0.6200 f1_test: 0.5455 precision: 0.6786 recall: 0.4560\n",
      "Epoch: 0164 acc_train: 0.6538 acc_test: 0.6030 f1_test: 0.4810 precision: 0.6943 recall: 0.3680\n",
      "Epoch: 0165 acc_train: 0.6592 acc_test: 0.6210 f1_test: 0.5483 precision: 0.6785 recall: 0.4600\n",
      "Epoch: 0166 acc_train: 0.6584 acc_test: 0.6130 f1_test: 0.5120 precision: 0.6928 recall: 0.4060\n",
      "Epoch: 0167 acc_train: 0.6604 acc_test: 0.6190 f1_test: 0.5302 precision: 0.6913 recall: 0.4300\n",
      "Epoch: 0168 acc_train: 0.6606 acc_test: 0.6240 f1_test: 0.5502 precision: 0.6845 recall: 0.4600\n",
      "Epoch: 0169 acc_train: 0.6591 acc_test: 0.6020 f1_test: 0.4845 precision: 0.6875 recall: 0.3740\n",
      "Epoch: 0170 acc_train: 0.6608 acc_test: 0.6280 f1_test: 0.5613 precision: 0.6839 recall: 0.4760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0171 acc_train: 0.6611 acc_test: 0.6160 f1_test: 0.5212 precision: 0.6921 recall: 0.4180\n",
      "Epoch: 0172 acc_train: 0.6633 acc_test: 0.6250 f1_test: 0.5455 precision: 0.6923 recall: 0.4500\n",
      "Epoch: 0173 acc_train: 0.6636 acc_test: 0.6260 f1_test: 0.5483 precision: 0.6921 recall: 0.4540\n",
      "Epoch: 0174 acc_train: 0.6618 acc_test: 0.6160 f1_test: 0.5188 precision: 0.6946 recall: 0.4140\n",
      "Epoch: 0175 acc_train: 0.6611 acc_test: 0.6280 f1_test: 0.5592 precision: 0.6860 recall: 0.4720\n",
      "Epoch: 0176 acc_train: 0.6590 acc_test: 0.6010 f1_test: 0.4838 precision: 0.6850 recall: 0.3740\n",
      "Epoch: 0177 acc_train: 0.6622 acc_test: 0.6270 f1_test: 0.5658 precision: 0.6769 recall: 0.4860\n",
      "Epoch: 0178 acc_train: 0.6628 acc_test: 0.6180 f1_test: 0.5213 precision: 0.6980 recall: 0.4160\n",
      "Epoch: 0179 acc_train: 0.6651 acc_test: 0.6210 f1_test: 0.5406 precision: 0.6862 recall: 0.4460\n",
      "Epoch: 0180 acc_train: 0.6637 acc_test: 0.6290 f1_test: 0.5630 precision: 0.6848 recall: 0.4780\n",
      "Epoch: 0181 acc_train: 0.6630 acc_test: 0.6170 f1_test: 0.5230 precision: 0.6931 recall: 0.4200\n",
      "Epoch: 0182 acc_train: 0.6638 acc_test: 0.6230 f1_test: 0.5591 precision: 0.6732 recall: 0.4780\n",
      "Epoch: 0183 acc_train: 0.6643 acc_test: 0.6160 f1_test: 0.5306 precision: 0.6824 recall: 0.4340\n",
      "Epoch: 0184 acc_train: 0.6655 acc_test: 0.6180 f1_test: 0.5398 precision: 0.6788 recall: 0.4480\n",
      "Epoch: 0185 acc_train: 0.6636 acc_test: 0.6270 f1_test: 0.5586 precision: 0.6841 recall: 0.4720\n",
      "Epoch: 0186 acc_train: 0.6635 acc_test: 0.6160 f1_test: 0.5259 precision: 0.6871 recall: 0.4260\n",
      "Epoch: 0187 acc_train: 0.6643 acc_test: 0.6290 f1_test: 0.5630 precision: 0.6848 recall: 0.4780\n",
      "Epoch: 0188 acc_train: 0.6657 acc_test: 0.6200 f1_test: 0.5377 precision: 0.6863 recall: 0.4420\n",
      "Epoch: 0189 acc_train: 0.6656 acc_test: 0.6260 f1_test: 0.5505 precision: 0.6898 recall: 0.4580\n",
      "Epoch: 0190 acc_train: 0.6661 acc_test: 0.6280 f1_test: 0.5613 precision: 0.6839 recall: 0.4760\n",
      "Epoch: 0191 acc_train: 0.6673 acc_test: 0.6190 f1_test: 0.5337 precision: 0.6877 recall: 0.4360\n",
      "Epoch: 0192 acc_train: 0.6666 acc_test: 0.6260 f1_test: 0.5579 precision: 0.6821 recall: 0.4720\n",
      "Epoch: 0193 acc_train: 0.6684 acc_test: 0.6250 f1_test: 0.5530 precision: 0.6844 recall: 0.4640\n",
      "Epoch: 0194 acc_train: 0.6684 acc_test: 0.6230 f1_test: 0.5496 precision: 0.6825 recall: 0.4600\n",
      "Epoch: 0195 acc_train: 0.6672 acc_test: 0.6250 f1_test: 0.5583 precision: 0.6791 recall: 0.4740\n",
      "Epoch: 0196 acc_train: 0.6677 acc_test: 0.6160 f1_test: 0.5328 precision: 0.6801 recall: 0.4380\n",
      "Epoch: 0197 acc_train: 0.6676 acc_test: 0.6230 f1_test: 0.5580 precision: 0.6742 recall: 0.4760\n",
      "Epoch: 0198 acc_train: 0.6678 acc_test: 0.6160 f1_test: 0.5328 precision: 0.6801 recall: 0.4380\n",
      "Epoch: 0199 acc_train: 0.6688 acc_test: 0.6230 f1_test: 0.5538 precision: 0.6783 recall: 0.4680\n",
      "Epoch: 0200 acc_train: 0.6699 acc_test: 0.6210 f1_test: 0.5493 precision: 0.6774 recall: 0.4620\n",
      "Epoch: 0201 acc_train: 0.6681 acc_test: 0.6180 f1_test: 0.5398 precision: 0.6788 recall: 0.4480\n",
      "Epoch: 0202 acc_train: 0.6686 acc_test: 0.6230 f1_test: 0.5601 precision: 0.6723 recall: 0.4800\n",
      "Epoch: 0203 acc_train: 0.6691 acc_test: 0.6180 f1_test: 0.5341 precision: 0.6844 recall: 0.4380\n",
      "Epoch: 0204 acc_train: 0.6690 acc_test: 0.6250 f1_test: 0.5634 precision: 0.6741 recall: 0.4840\n",
      "Epoch: 0205 acc_train: 0.6693 acc_test: 0.6190 f1_test: 0.5371 precision: 0.6842 recall: 0.4420\n",
      "Epoch: 0206 acc_train: 0.6713 acc_test: 0.6250 f1_test: 0.5604 precision: 0.6771 recall: 0.4780\n",
      "Epoch: 0207 acc_train: 0.6709 acc_test: 0.6250 f1_test: 0.5583 precision: 0.6791 recall: 0.4740\n",
      "Epoch: 0208 acc_train: 0.6698 acc_test: 0.6220 f1_test: 0.5468 precision: 0.6826 recall: 0.4560\n",
      "Epoch: 0209 acc_train: 0.6701 acc_test: 0.6240 f1_test: 0.5638 precision: 0.6713 recall: 0.4860\n",
      "Epoch: 0210 acc_train: 0.6699 acc_test: 0.6220 f1_test: 0.5435 precision: 0.6860 recall: 0.4500\n",
      "Epoch: 0211 acc_train: 0.6708 acc_test: 0.6230 f1_test: 0.5621 precision: 0.6704 recall: 0.4840\n",
      "Epoch: 0212 acc_train: 0.6716 acc_test: 0.6240 f1_test: 0.5524 precision: 0.6824 recall: 0.4640\n",
      "Epoch: 0213 acc_train: 0.6708 acc_test: 0.6230 f1_test: 0.5570 precision: 0.6752 recall: 0.4740\n",
      "Epoch: 0214 acc_train: 0.6702 acc_test: 0.6220 f1_test: 0.5574 precision: 0.6723 recall: 0.4760\n",
      "Epoch: 0215 acc_train: 0.6705 acc_test: 0.6200 f1_test: 0.5444 precision: 0.6796 recall: 0.4540\n",
      "Epoch: 0216 acc_train: 0.6710 acc_test: 0.6220 f1_test: 0.5594 precision: 0.6704 recall: 0.4800\n",
      "Epoch: 0217 acc_train: 0.6704 acc_test: 0.6210 f1_test: 0.5450 precision: 0.6817 recall: 0.4540\n",
      "Epoch: 0218 acc_train: 0.6721 acc_test: 0.6240 f1_test: 0.5607 precision: 0.6742 recall: 0.4800\n",
      "Epoch: 0219 acc_train: 0.6722 acc_test: 0.6230 f1_test: 0.5549 precision: 0.6772 recall: 0.4700\n",
      "Epoch: 0220 acc_train: 0.6730 acc_test: 0.6220 f1_test: 0.5563 precision: 0.6733 recall: 0.4740\n",
      "Epoch: 0221 acc_train: 0.6731 acc_test: 0.6240 f1_test: 0.5597 precision: 0.6751 recall: 0.4780\n",
      "Epoch: 0222 acc_train: 0.6728 acc_test: 0.6220 f1_test: 0.5500 precision: 0.6794 recall: 0.4620\n",
      "Epoch: 0223 acc_train: 0.6734 acc_test: 0.6240 f1_test: 0.5628 precision: 0.6722 recall: 0.4840\n",
      "Epoch: 0224 acc_train: 0.6724 acc_test: 0.6220 f1_test: 0.5446 precision: 0.6848 recall: 0.4520\n",
      "Epoch: 0225 acc_train: 0.6731 acc_test: 0.6280 f1_test: 0.5714 precision: 0.6739 recall: 0.4960\n",
      "Epoch: 0226 acc_train: 0.6723 acc_test: 0.6220 f1_test: 0.5401 precision: 0.6894 recall: 0.4440\n",
      "Epoch: 0227 acc_train: 0.6749 acc_test: 0.6260 f1_test: 0.5681 precision: 0.6721 recall: 0.4920\n",
      "Epoch: 0228 acc_train: 0.6740 acc_test: 0.6270 f1_test: 0.5586 precision: 0.6841 recall: 0.4720\n",
      "Epoch: 0229 acc_train: 0.6742 acc_test: 0.6260 f1_test: 0.5600 precision: 0.6800 recall: 0.4760\n",
      "Epoch: 0230 acc_train: 0.6744 acc_test: 0.6250 f1_test: 0.5675 precision: 0.6703 recall: 0.4920\n",
      "Epoch: 0231 acc_train: 0.6734 acc_test: 0.6230 f1_test: 0.5463 precision: 0.6858 recall: 0.4540\n",
      "Epoch: 0232 acc_train: 0.6734 acc_test: 0.6290 f1_test: 0.5770 precision: 0.6711 recall: 0.5060\n",
      "Epoch: 0233 acc_train: 0.6731 acc_test: 0.6250 f1_test: 0.5487 precision: 0.6888 recall: 0.4560\n",
      "Epoch: 0234 acc_train: 0.6766 acc_test: 0.6270 f1_test: 0.5678 precision: 0.6749 recall: 0.4900\n",
      "Epoch: 0235 acc_train: 0.6755 acc_test: 0.6260 f1_test: 0.5641 precision: 0.6760 recall: 0.4840\n",
      "Epoch: 0236 acc_train: 0.6738 acc_test: 0.6250 f1_test: 0.5552 precision: 0.6822 recall: 0.4680\n",
      "Epoch: 0237 acc_train: 0.6751 acc_test: 0.6300 f1_test: 0.5795 precision: 0.6711 recall: 0.5100\n",
      "Epoch: 0238 acc_train: 0.6748 acc_test: 0.6270 f1_test: 0.5554 precision: 0.6873 recall: 0.4660\n",
      "Epoch: 0239 acc_train: 0.6770 acc_test: 0.6290 f1_test: 0.5750 precision: 0.6729 recall: 0.5020\n",
      "Epoch: 0240 acc_train: 0.6776 acc_test: 0.6310 f1_test: 0.5694 precision: 0.6835 recall: 0.4880\n",
      "Epoch: 0241 acc_train: 0.6767 acc_test: 0.6300 f1_test: 0.5667 precision: 0.6836 recall: 0.4840\n",
      "Epoch: 0242 acc_train: 0.6772 acc_test: 0.6310 f1_test: 0.5773 precision: 0.6756 recall: 0.5040\n",
      "Epoch: 0243 acc_train: 0.6761 acc_test: 0.6270 f1_test: 0.5596 precision: 0.6830 recall: 0.4740\n",
      "Epoch: 0244 acc_train: 0.6773 acc_test: 0.6320 f1_test: 0.5799 precision: 0.6755 recall: 0.5080\n",
      "Epoch: 0245 acc_train: 0.6786 acc_test: 0.6310 f1_test: 0.5664 precision: 0.6866 recall: 0.4820\n",
      "Epoch: 0246 acc_train: 0.6777 acc_test: 0.6290 f1_test: 0.5701 precision: 0.6777 recall: 0.4920\n",
      "Epoch: 0247 acc_train: 0.6779 acc_test: 0.6300 f1_test: 0.5727 precision: 0.6776 recall: 0.4960\n",
      "Epoch: 0248 acc_train: 0.6783 acc_test: 0.6310 f1_test: 0.5674 precision: 0.6856 recall: 0.4840\n",
      "Epoch: 0249 acc_train: 0.6784 acc_test: 0.6300 f1_test: 0.5757 precision: 0.6747 recall: 0.5020\n",
      "Epoch: 0250 acc_train: 0.6774 acc_test: 0.6290 f1_test: 0.5630 precision: 0.6848 recall: 0.4780\n",
      "Epoch: 0251 acc_train: 0.6791 acc_test: 0.6310 f1_test: 0.5763 precision: 0.6765 recall: 0.5020\n",
      "Epoch: 0252 acc_train: 0.6791 acc_test: 0.6310 f1_test: 0.5694 precision: 0.6835 recall: 0.4880\n",
      "Epoch: 0253 acc_train: 0.6793 acc_test: 0.6310 f1_test: 0.5724 precision: 0.6804 recall: 0.4940\n",
      "Epoch: 0254 acc_train: 0.6794 acc_test: 0.6290 f1_test: 0.5711 precision: 0.6767 recall: 0.4940\n",
      "Epoch: 0255 acc_train: 0.6798 acc_test: 0.6310 f1_test: 0.5694 precision: 0.6835 recall: 0.4880\n",
      "Epoch: 0256 acc_train: 0.6800 acc_test: 0.6330 f1_test: 0.5796 precision: 0.6783 recall: 0.5060\n",
      "Epoch: 0257 acc_train: 0.6787 acc_test: 0.6340 f1_test: 0.5714 precision: 0.6893 recall: 0.4880\n",
      "Epoch: 0258 acc_train: 0.6798 acc_test: 0.6330 f1_test: 0.5815 precision: 0.6764 recall: 0.5100\n",
      "Epoch: 0259 acc_train: 0.6777 acc_test: 0.6300 f1_test: 0.5626 precision: 0.6879 recall: 0.4760\n",
      "Epoch: 0260 acc_train: 0.6784 acc_test: 0.6340 f1_test: 0.5850 precision: 0.6754 recall: 0.5160\n",
      "Epoch: 0261 acc_train: 0.6767 acc_test: 0.6290 f1_test: 0.5557 precision: 0.6925 recall: 0.4640\n",
      "Epoch: 0262 acc_train: 0.6766 acc_test: 0.6360 f1_test: 0.5901 precision: 0.6753 recall: 0.5240\n",
      "Epoch: 0263 acc_train: 0.6762 acc_test: 0.6280 f1_test: 0.5561 precision: 0.6893 recall: 0.4660\n",
      "Epoch: 0264 acc_train: 0.6803 acc_test: 0.6350 f1_test: 0.5857 precision: 0.6772 recall: 0.5160\n",
      "Epoch: 0265 acc_train: 0.6797 acc_test: 0.6300 f1_test: 0.5678 precision: 0.6826 recall: 0.4860\n",
      "Epoch: 0266 acc_train: 0.6806 acc_test: 0.6330 f1_test: 0.5728 precision: 0.6852 recall: 0.4920\n",
      "Epoch: 0267 acc_train: 0.6802 acc_test: 0.6350 f1_test: 0.5857 precision: 0.6772 recall: 0.5160\n",
      "Epoch: 0268 acc_train: 0.6774 acc_test: 0.6280 f1_test: 0.5592 precision: 0.6860 recall: 0.4720\n",
      "Epoch: 0269 acc_train: 0.6797 acc_test: 0.6360 f1_test: 0.5882 precision: 0.6771 recall: 0.5200\n",
      "Epoch: 0270 acc_train: 0.6797 acc_test: 0.6290 f1_test: 0.5620 precision: 0.6859 recall: 0.4760\n",
      "Epoch: 0271 acc_train: 0.6807 acc_test: 0.6340 f1_test: 0.5793 precision: 0.6811 recall: 0.5040\n",
      "Epoch: 0272 acc_train: 0.6817 acc_test: 0.6330 f1_test: 0.5786 precision: 0.6792 recall: 0.5040\n",
      "Epoch: 0273 acc_train: 0.6793 acc_test: 0.6290 f1_test: 0.5630 precision: 0.6848 recall: 0.4780\n",
      "Epoch: 0274 acc_train: 0.6811 acc_test: 0.6380 f1_test: 0.5905 precision: 0.6797 recall: 0.5220\n",
      "Epoch: 0275 acc_train: 0.6795 acc_test: 0.6290 f1_test: 0.5609 precision: 0.6870 recall: 0.4740\n",
      "Epoch: 0276 acc_train: 0.6822 acc_test: 0.6360 f1_test: 0.5873 precision: 0.6780 recall: 0.5180\n",
      "Epoch: 0277 acc_train: 0.6816 acc_test: 0.6310 f1_test: 0.5684 precision: 0.6845 recall: 0.4860\n",
      "Epoch: 0278 acc_train: 0.6821 acc_test: 0.6330 f1_test: 0.5796 precision: 0.6783 recall: 0.5060\n",
      "Epoch: 0279 acc_train: 0.6815 acc_test: 0.6330 f1_test: 0.5786 precision: 0.6792 recall: 0.5040\n",
      "Epoch: 0280 acc_train: 0.6822 acc_test: 0.6310 f1_test: 0.5704 precision: 0.6825 recall: 0.4900\n",
      "Epoch: 0281 acc_train: 0.6831 acc_test: 0.6380 f1_test: 0.5905 precision: 0.6797 recall: 0.5220\n",
      "Epoch: 0282 acc_train: 0.6803 acc_test: 0.6290 f1_test: 0.5640 precision: 0.6838 recall: 0.4800\n",
      "Epoch: 0283 acc_train: 0.6812 acc_test: 0.6370 f1_test: 0.5926 precision: 0.6752 recall: 0.5280\n",
      "Epoch: 0284 acc_train: 0.6802 acc_test: 0.6320 f1_test: 0.5650 precision: 0.6908 recall: 0.4780\n",
      "Epoch: 0285 acc_train: 0.6815 acc_test: 0.6360 f1_test: 0.5910 precision: 0.6744 recall: 0.5260\n",
      "Epoch: 0286 acc_train: 0.6808 acc_test: 0.6310 f1_test: 0.5684 precision: 0.6845 recall: 0.4860\n",
      "Epoch: 0287 acc_train: 0.6839 acc_test: 0.6360 f1_test: 0.5892 precision: 0.6762 recall: 0.5220\n",
      "Epoch: 0288 acc_train: 0.6828 acc_test: 0.6320 f1_test: 0.5751 precision: 0.6803 recall: 0.4980\n",
      "Epoch: 0289 acc_train: 0.6833 acc_test: 0.6340 f1_test: 0.5831 precision: 0.6772 recall: 0.5120\n",
      "Epoch: 0290 acc_train: 0.6833 acc_test: 0.6340 f1_test: 0.5812 precision: 0.6791 recall: 0.5080\n",
      "Epoch: 0291 acc_train: 0.6839 acc_test: 0.6350 f1_test: 0.5790 precision: 0.6839 recall: 0.5020\n",
      "Epoch: 0292 acc_train: 0.6848 acc_test: 0.6360 f1_test: 0.5873 precision: 0.6780 recall: 0.5180\n",
      "Epoch: 0293 acc_train: 0.6833 acc_test: 0.6330 f1_test: 0.5738 precision: 0.6842 recall: 0.4940\n",
      "Epoch: 0294 acc_train: 0.6842 acc_test: 0.6380 f1_test: 0.5960 precision: 0.6742 recall: 0.5340\n",
      "Epoch: 0295 acc_train: 0.6814 acc_test: 0.6360 f1_test: 0.5708 precision: 0.6954 recall: 0.4840\n",
      "Epoch: 0296 acc_train: 0.6828 acc_test: 0.6380 f1_test: 0.6031 precision: 0.6675 recall: 0.5500\n",
      "Epoch: 0297 acc_train: 0.6805 acc_test: 0.6250 f1_test: 0.5466 precision: 0.6911 recall: 0.4520\n",
      "Epoch: 0298 acc_train: 0.6829 acc_test: 0.6350 f1_test: 0.6002 precision: 0.6634 recall: 0.5480\n",
      "Epoch: 0299 acc_train: 0.6837 acc_test: 0.6360 f1_test: 0.5758 precision: 0.6899 recall: 0.4940\n",
      "Epoch: 0300 acc_train: 0.6849 acc_test: 0.6320 f1_test: 0.5770 precision: 0.6784 recall: 0.5020\n",
      "Epoch: 0301 acc_train: 0.6843 acc_test: 0.6370 f1_test: 0.5998 precision: 0.6683 recall: 0.5440\n",
      "Epoch: 0302 acc_train: 0.6828 acc_test: 0.6340 f1_test: 0.5674 precision: 0.6936 recall: 0.4800\n",
      "Epoch: 0303 acc_train: 0.6861 acc_test: 0.6410 f1_test: 0.6007 precision: 0.6767 recall: 0.5400\n",
      "Epoch: 0304 acc_train: 0.6884 acc_test: 0.6370 f1_test: 0.5917 precision: 0.6761 recall: 0.5260\n",
      "Epoch: 0305 acc_train: 0.6837 acc_test: 0.6330 f1_test: 0.5728 precision: 0.6852 recall: 0.4920\n",
      "Epoch: 0306 acc_train: 0.6850 acc_test: 0.6370 f1_test: 0.5980 precision: 0.6700 recall: 0.5400\n",
      "Epoch: 0307 acc_train: 0.6845 acc_test: 0.6310 f1_test: 0.5724 precision: 0.6804 recall: 0.4940\n",
      "Epoch: 0308 acc_train: 0.6873 acc_test: 0.6380 f1_test: 0.5914 precision: 0.6788 recall: 0.5240\n",
      "Epoch: 0309 acc_train: 0.6886 acc_test: 0.6380 f1_test: 0.5933 precision: 0.6769 recall: 0.5280\n",
      "Epoch: 0310 acc_train: 0.6855 acc_test: 0.6350 f1_test: 0.5751 precision: 0.6880 recall: 0.4940\n",
      "Epoch: 0311 acc_train: 0.6871 acc_test: 0.6420 f1_test: 0.6022 precision: 0.6775 recall: 0.5420\n",
      "Epoch: 0312 acc_train: 0.6877 acc_test: 0.6370 f1_test: 0.5851 precision: 0.6827 recall: 0.5120\n",
      "Epoch: 0313 acc_train: 0.6890 acc_test: 0.6360 f1_test: 0.5873 precision: 0.6780 recall: 0.5180\n",
      "Epoch: 0314 acc_train: 0.6883 acc_test: 0.6400 f1_test: 0.5973 precision: 0.6777 recall: 0.5340\n",
      "Epoch: 0315 acc_train: 0.6856 acc_test: 0.6310 f1_test: 0.5744 precision: 0.6785 recall: 0.4980\n",
      "Epoch: 0316 acc_train: 0.6896 acc_test: 0.6380 f1_test: 0.5942 precision: 0.6760 recall: 0.5300\n",
      "Epoch: 0317 acc_train: 0.6879 acc_test: 0.6350 f1_test: 0.5829 precision: 0.6800 recall: 0.5100\n",
      "Epoch: 0318 acc_train: 0.6901 acc_test: 0.6340 f1_test: 0.5841 precision: 0.6763 recall: 0.5140\n",
      "Epoch: 0319 acc_train: 0.6905 acc_test: 0.6380 f1_test: 0.5942 precision: 0.6760 recall: 0.5300\n",
      "Epoch: 0320 acc_train: 0.6862 acc_test: 0.6340 f1_test: 0.5783 precision: 0.6821 recall: 0.5020\n",
      "Epoch: 0321 acc_train: 0.6899 acc_test: 0.6380 f1_test: 0.5951 precision: 0.6751 recall: 0.5320\n",
      "Epoch: 0322 acc_train: 0.6896 acc_test: 0.6330 f1_test: 0.5806 precision: 0.6773 recall: 0.5080\n",
      "Epoch: 0323 acc_train: 0.6894 acc_test: 0.6400 f1_test: 0.5955 precision: 0.6795 recall: 0.5300\n",
      "Epoch: 0324 acc_train: 0.6910 acc_test: 0.6380 f1_test: 0.5905 precision: 0.6797 recall: 0.5220\n",
      "Epoch: 0325 acc_train: 0.6903 acc_test: 0.6380 f1_test: 0.5905 precision: 0.6797 recall: 0.5220\n",
      "Epoch: 0326 acc_train: 0.6897 acc_test: 0.6390 f1_test: 0.5966 precision: 0.6759 recall: 0.5340\n",
      "Epoch: 0327 acc_train: 0.6893 acc_test: 0.6350 f1_test: 0.5848 precision: 0.6781 recall: 0.5140\n",
      "Epoch: 0328 acc_train: 0.6913 acc_test: 0.6390 f1_test: 0.5984 precision: 0.6742 recall: 0.5380\n",
      "Epoch: 0329 acc_train: 0.6882 acc_test: 0.6340 f1_test: 0.5803 precision: 0.6801 recall: 0.5060\n",
      "Epoch: 0330 acc_train: 0.6911 acc_test: 0.6390 f1_test: 0.5993 precision: 0.6733 recall: 0.5400\n",
      "Epoch: 0331 acc_train: 0.6887 acc_test: 0.6380 f1_test: 0.5829 precision: 0.6875 recall: 0.5060\n",
      "Epoch: 0332 acc_train: 0.6903 acc_test: 0.6390 f1_test: 0.5993 precision: 0.6733 recall: 0.5400\n",
      "Epoch: 0333 acc_train: 0.6885 acc_test: 0.6340 f1_test: 0.5783 precision: 0.6821 recall: 0.5020\n",
      "Epoch: 0334 acc_train: 0.6897 acc_test: 0.6360 f1_test: 0.5965 precision: 0.6692 recall: 0.5380\n",
      "Epoch: 0335 acc_train: 0.6882 acc_test: 0.6350 f1_test: 0.5761 precision: 0.6870 recall: 0.4960\n",
      "Epoch: 0336 acc_train: 0.6900 acc_test: 0.6390 f1_test: 0.6020 precision: 0.6708 recall: 0.5460\n",
      "Epoch: 0337 acc_train: 0.6873 acc_test: 0.6310 f1_test: 0.5684 precision: 0.6845 recall: 0.4860\n",
      "Epoch: 0338 acc_train: 0.6887 acc_test: 0.6380 f1_test: 0.6031 precision: 0.6675 recall: 0.5500\n",
      "Epoch: 0339 acc_train: 0.6875 acc_test: 0.6330 f1_test: 0.5718 precision: 0.6863 recall: 0.4900\n",
      "Epoch: 0340 acc_train: 0.6886 acc_test: 0.6400 f1_test: 0.6026 precision: 0.6724 recall: 0.5460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0341 acc_train: 0.6894 acc_test: 0.6380 f1_test: 0.5858 precision: 0.6845 recall: 0.5120\n",
      "Epoch: 0342 acc_train: 0.6920 acc_test: 0.6370 f1_test: 0.5962 precision: 0.6717 recall: 0.5360\n",
      "Epoch: 0343 acc_train: 0.6905 acc_test: 0.6380 f1_test: 0.5923 precision: 0.6778 recall: 0.5260\n",
      "Epoch: 0344 acc_train: 0.6914 acc_test: 0.6390 f1_test: 0.5930 precision: 0.6796 recall: 0.5260\n",
      "Epoch: 0345 acc_train: 0.6928 acc_test: 0.6390 f1_test: 0.5993 precision: 0.6733 recall: 0.5400\n",
      "Epoch: 0346 acc_train: 0.6908 acc_test: 0.6400 f1_test: 0.5890 precision: 0.6862 recall: 0.5160\n",
      "Epoch: 0347 acc_train: 0.6903 acc_test: 0.6430 f1_test: 0.6081 precision: 0.6740 recall: 0.5540\n",
      "Epoch: 0348 acc_train: 0.6869 acc_test: 0.6320 f1_test: 0.5681 precision: 0.6875 recall: 0.4840\n",
      "Epoch: 0349 acc_train: 0.6908 acc_test: 0.6400 f1_test: 0.6104 precision: 0.6651 recall: 0.5640\n",
      "Epoch: 0350 acc_train: 0.6863 acc_test: 0.6330 f1_test: 0.5647 precision: 0.6939 recall: 0.4760\n",
      "Epoch: 0351 acc_train: 0.6899 acc_test: 0.6420 f1_test: 0.6117 precision: 0.6682 recall: 0.5640\n",
      "Epoch: 0352 acc_train: 0.6907 acc_test: 0.6410 f1_test: 0.5888 precision: 0.6890 recall: 0.5140\n",
      "Epoch: 0353 acc_train: 0.6925 acc_test: 0.6420 f1_test: 0.5996 precision: 0.6802 recall: 0.5360\n",
      "Epoch: 0354 acc_train: 0.6919 acc_test: 0.6430 f1_test: 0.6081 precision: 0.6740 recall: 0.5540\n",
      "Epoch: 0355 acc_train: 0.6877 acc_test: 0.6380 f1_test: 0.5791 precision: 0.6917 recall: 0.4980\n",
      "Epoch: 0356 acc_train: 0.6908 acc_test: 0.6430 f1_test: 0.6132 precision: 0.6690 recall: 0.5660\n",
      "Epoch: 0357 acc_train: 0.6885 acc_test: 0.6360 f1_test: 0.5748 precision: 0.6910 recall: 0.4920\n",
      "Epoch: 0358 acc_train: 0.6932 acc_test: 0.6430 f1_test: 0.6081 precision: 0.6740 recall: 0.5540\n",
      "Epoch: 0359 acc_train: 0.6920 acc_test: 0.6410 f1_test: 0.5971 precision: 0.6803 recall: 0.5320\n",
      "Epoch: 0360 acc_train: 0.6933 acc_test: 0.6390 f1_test: 0.5921 precision: 0.6805 recall: 0.5240\n",
      "Epoch: 0361 acc_train: 0.6938 acc_test: 0.6440 f1_test: 0.6105 precision: 0.6739 recall: 0.5580\n",
      "Epoch: 0362 acc_train: 0.6891 acc_test: 0.6360 f1_test: 0.5777 precision: 0.6878 recall: 0.4980\n",
      "Epoch: 0363 acc_train: 0.6929 acc_test: 0.6410 f1_test: 0.6077 precision: 0.6699 recall: 0.5560\n",
      "Epoch: 0364 acc_train: 0.6933 acc_test: 0.6430 f1_test: 0.5901 precision: 0.6927 recall: 0.5140\n",
      "Epoch: 0365 acc_train: 0.6949 acc_test: 0.6420 f1_test: 0.6031 precision: 0.6766 recall: 0.5440\n",
      "Epoch: 0366 acc_train: 0.6962 acc_test: 0.6380 f1_test: 0.5969 precision: 0.6734 recall: 0.5360\n",
      "Epoch: 0367 acc_train: 0.6937 acc_test: 0.6370 f1_test: 0.5889 precision: 0.6789 recall: 0.5200\n",
      "Epoch: 0368 acc_train: 0.6941 acc_test: 0.6440 f1_test: 0.6105 precision: 0.6739 recall: 0.5580\n",
      "Epoch: 0369 acc_train: 0.6920 acc_test: 0.6400 f1_test: 0.5833 precision: 0.6923 recall: 0.5040\n",
      "Epoch: 0370 acc_train: 0.6950 acc_test: 0.6430 f1_test: 0.6090 precision: 0.6731 recall: 0.5560\n",
      "Epoch: 0371 acc_train: 0.6934 acc_test: 0.6390 f1_test: 0.5865 precision: 0.6863 recall: 0.5120\n",
      "Epoch: 0372 acc_train: 0.6972 acc_test: 0.6450 f1_test: 0.6077 precision: 0.6790 recall: 0.5500\n",
      "Epoch: 0373 acc_train: 0.6953 acc_test: 0.6390 f1_test: 0.5948 precision: 0.6777 recall: 0.5300\n",
      "Epoch: 0374 acc_train: 0.6970 acc_test: 0.6380 f1_test: 0.5960 precision: 0.6742 recall: 0.5340\n",
      "Epoch: 0375 acc_train: 0.6972 acc_test: 0.6410 f1_test: 0.5989 precision: 0.6785 recall: 0.5360\n",
      "Epoch: 0376 acc_train: 0.6958 acc_test: 0.6380 f1_test: 0.5933 precision: 0.6769 recall: 0.5280\n",
      "Epoch: 0377 acc_train: 0.6975 acc_test: 0.6420 f1_test: 0.6040 precision: 0.6757 recall: 0.5460\n",
      "Epoch: 0378 acc_train: 0.6954 acc_test: 0.6380 f1_test: 0.5886 precision: 0.6816 recall: 0.5180\n",
      "Epoch: 0379 acc_train: 0.6969 acc_test: 0.6400 f1_test: 0.6087 precision: 0.6667 recall: 0.5600\n",
      "Epoch: 0380 acc_train: 0.6925 acc_test: 0.6360 f1_test: 0.5748 precision: 0.6910 recall: 0.4920\n",
      "Epoch: 0381 acc_train: 0.6963 acc_test: 0.6460 f1_test: 0.6218 precision: 0.6674 recall: 0.5820\n",
      "Epoch: 0382 acc_train: 0.6926 acc_test: 0.6350 f1_test: 0.5639 precision: 0.7003 recall: 0.4720\n",
      "Epoch: 0383 acc_train: 0.6960 acc_test: 0.6420 f1_test: 0.6183 precision: 0.6621 recall: 0.5800\n",
      "Epoch: 0384 acc_train: 0.6951 acc_test: 0.6440 f1_test: 0.5851 precision: 0.7011 recall: 0.5020\n",
      "Epoch: 0385 acc_train: 0.6973 acc_test: 0.6400 f1_test: 0.5991 precision: 0.6759 recall: 0.5380\n",
      "Epoch: 0386 acc_train: 0.6983 acc_test: 0.6430 f1_test: 0.6124 precision: 0.6698 recall: 0.5640\n",
      "Epoch: 0387 acc_train: 0.6941 acc_test: 0.6380 f1_test: 0.5771 precision: 0.6938 recall: 0.4940\n",
      "Epoch: 0388 acc_train: 0.6964 acc_test: 0.6480 f1_test: 0.6231 precision: 0.6705 recall: 0.5820\n",
      "Epoch: 0389 acc_train: 0.6952 acc_test: 0.6430 f1_test: 0.5882 precision: 0.6948 recall: 0.5100\n",
      "Epoch: 0390 acc_train: 0.6965 acc_test: 0.6420 f1_test: 0.6031 precision: 0.6766 recall: 0.5440\n",
      "Epoch: 0391 acc_train: 0.6989 acc_test: 0.6420 f1_test: 0.6083 precision: 0.6715 recall: 0.5560\n",
      "Epoch: 0392 acc_train: 0.6936 acc_test: 0.6380 f1_test: 0.5800 precision: 0.6906 recall: 0.5000\n",
      "Epoch: 0393 acc_train: 0.6979 acc_test: 0.6470 f1_test: 0.6225 precision: 0.6690 recall: 0.5820\n",
      "Epoch: 0394 acc_train: 0.6952 acc_test: 0.6390 f1_test: 0.5827 precision: 0.6904 recall: 0.5040\n",
      "Epoch: 0395 acc_train: 0.6991 acc_test: 0.6400 f1_test: 0.6044 precision: 0.6707 recall: 0.5500\n",
      "Epoch: 0396 acc_train: 0.6972 acc_test: 0.6410 f1_test: 0.6042 precision: 0.6732 recall: 0.5480\n",
      "Epoch: 0397 acc_train: 0.6981 acc_test: 0.6430 f1_test: 0.5929 precision: 0.6897 recall: 0.5200\n",
      "Epoch: 0398 acc_train: 0.6990 acc_test: 0.6380 f1_test: 0.6074 precision: 0.6635 recall: 0.5600\n",
      "Epoch: 0399 acc_train: 0.6938 acc_test: 0.6340 f1_test: 0.5774 precision: 0.6831 recall: 0.5000\n",
      "Epoch: 0400 acc_train: 0.6991 acc_test: 0.6380 f1_test: 0.6082 precision: 0.6627 recall: 0.5620\n",
      "Epoch: 0401 acc_train: 0.6964 acc_test: 0.6450 f1_test: 0.5961 precision: 0.6913 recall: 0.5240\n",
      "Epoch: 0402 acc_train: 0.6993 acc_test: 0.6410 f1_test: 0.6042 precision: 0.6732 recall: 0.5480\n",
      "Epoch: 0403 acc_train: 0.6994 acc_test: 0.6390 f1_test: 0.6002 precision: 0.6725 recall: 0.5420\n",
      "Epoch: 0404 acc_train: 0.6970 acc_test: 0.6390 f1_test: 0.5930 precision: 0.6796 recall: 0.5260\n",
      "Epoch: 0405 acc_train: 0.6992 acc_test: 0.6430 f1_test: 0.6141 precision: 0.6682 recall: 0.5680\n",
      "Epoch: 0406 acc_train: 0.6969 acc_test: 0.6410 f1_test: 0.5869 precision: 0.6911 recall: 0.5100\n",
      "Epoch: 0407 acc_train: 0.6997 acc_test: 0.6430 f1_test: 0.6115 precision: 0.6706 recall: 0.5620\n",
      "Epoch: 0408 acc_train: 0.6974 acc_test: 0.6390 f1_test: 0.5884 precision: 0.6844 recall: 0.5160\n",
      "Epoch: 0409 acc_train: 0.7013 acc_test: 0.6420 f1_test: 0.6083 precision: 0.6715 recall: 0.5560\n",
      "Epoch: 0410 acc_train: 0.6990 acc_test: 0.6390 f1_test: 0.5966 precision: 0.6759 recall: 0.5340\n",
      "Epoch: 0411 acc_train: 0.6985 acc_test: 0.6380 f1_test: 0.5987 precision: 0.6716 recall: 0.5400\n",
      "Epoch: 0412 acc_train: 0.6993 acc_test: 0.6390 f1_test: 0.6020 precision: 0.6708 recall: 0.5460\n",
      "Epoch: 0413 acc_train: 0.6990 acc_test: 0.6400 f1_test: 0.5964 precision: 0.6786 recall: 0.5320\n",
      "Epoch: 0414 acc_train: 0.7009 acc_test: 0.6420 f1_test: 0.6100 precision: 0.6699 recall: 0.5600\n",
      "Epoch: 0415 acc_train: 0.6981 acc_test: 0.6350 f1_test: 0.5819 precision: 0.6810 recall: 0.5080\n",
      "Epoch: 0416 acc_train: 0.7020 acc_test: 0.6410 f1_test: 0.6127 precision: 0.6651 recall: 0.5680\n",
      "Epoch: 0417 acc_train: 0.6955 acc_test: 0.6390 f1_test: 0.5758 precision: 0.6980 recall: 0.4900\n",
      "Epoch: 0418 acc_train: 0.7012 acc_test: 0.6420 f1_test: 0.6208 precision: 0.6599 recall: 0.5860\n",
      "Epoch: 0419 acc_train: 0.6956 acc_test: 0.6370 f1_test: 0.5653 precision: 0.7045 recall: 0.4720\n",
      "Epoch: 0420 acc_train: 0.7019 acc_test: 0.6430 f1_test: 0.6206 precision: 0.6621 recall: 0.5840\n",
      "Epoch: 0421 acc_train: 0.7002 acc_test: 0.6400 f1_test: 0.5918 precision: 0.6832 recall: 0.5220\n",
      "Epoch: 0422 acc_train: 0.7006 acc_test: 0.6390 f1_test: 0.5957 precision: 0.6768 recall: 0.5320\n",
      "Epoch: 0423 acc_train: 0.7016 acc_test: 0.6430 f1_test: 0.6190 precision: 0.6636 recall: 0.5800\n",
      "Epoch: 0424 acc_train: 0.6964 acc_test: 0.6400 f1_test: 0.5785 precision: 0.6977 recall: 0.4940\n",
      "Epoch: 0425 acc_train: 0.7028 acc_test: 0.6410 f1_test: 0.6185 precision: 0.6599 recall: 0.5820\n",
      "Epoch: 0426 acc_train: 0.6994 acc_test: 0.6390 f1_test: 0.5836 precision: 0.6894 recall: 0.5060\n",
      "Epoch: 0427 acc_train: 0.7014 acc_test: 0.6390 f1_test: 0.6020 precision: 0.6708 recall: 0.5460\n",
      "Epoch: 0428 acc_train: 0.7024 acc_test: 0.6410 f1_test: 0.6102 precision: 0.6675 recall: 0.5620\n",
      "Epoch: 0429 acc_train: 0.6988 acc_test: 0.6380 f1_test: 0.5820 precision: 0.6885 recall: 0.5040\n",
      "Epoch: 0430 acc_train: 0.7034 acc_test: 0.6430 f1_test: 0.6190 precision: 0.6636 recall: 0.5800\n",
      "Epoch: 0431 acc_train: 0.7005 acc_test: 0.6380 f1_test: 0.5820 precision: 0.6885 recall: 0.5040\n",
      "Epoch: 0432 acc_train: 0.7034 acc_test: 0.6460 f1_test: 0.6169 precision: 0.6722 recall: 0.5700\n",
      "Epoch: 0433 acc_train: 0.7018 acc_test: 0.6380 f1_test: 0.6013 precision: 0.6691 recall: 0.5460\n",
      "Epoch: 0434 acc_train: 0.7020 acc_test: 0.6410 f1_test: 0.6024 precision: 0.6749 recall: 0.5440\n",
      "Epoch: 0435 acc_train: 0.7046 acc_test: 0.6440 f1_test: 0.6156 precision: 0.6690 recall: 0.5700\n",
      "Epoch: 0436 acc_train: 0.7016 acc_test: 0.6340 f1_test: 0.5783 precision: 0.6821 recall: 0.5020\n",
      "Epoch: 0437 acc_train: 0.7053 acc_test: 0.6410 f1_test: 0.6169 precision: 0.6613 recall: 0.5780\n",
      "Epoch: 0438 acc_train: 0.6992 acc_test: 0.6330 f1_test: 0.5718 precision: 0.6863 recall: 0.4900\n",
      "Epoch: 0439 acc_train: 0.7043 acc_test: 0.6440 f1_test: 0.6237 precision: 0.6614 recall: 0.5900\n",
      "Epoch: 0440 acc_train: 0.7002 acc_test: 0.6360 f1_test: 0.5777 precision: 0.6878 recall: 0.4980\n",
      "Epoch: 0441 acc_train: 0.7059 acc_test: 0.6420 f1_test: 0.6159 precision: 0.6644 recall: 0.5740\n",
      "Epoch: 0442 acc_train: 0.7039 acc_test: 0.6410 f1_test: 0.5980 precision: 0.6794 recall: 0.5340\n",
      "Epoch: 0443 acc_train: 0.7046 acc_test: 0.6420 f1_test: 0.6075 precision: 0.6723 recall: 0.5540\n",
      "Epoch: 0444 acc_train: 0.7058 acc_test: 0.6400 f1_test: 0.6061 precision: 0.6691 recall: 0.5540\n",
      "Epoch: 0445 acc_train: 0.7045 acc_test: 0.6390 f1_test: 0.5930 precision: 0.6796 recall: 0.5260\n",
      "Epoch: 0446 acc_train: 0.7066 acc_test: 0.6410 f1_test: 0.6160 precision: 0.6621 recall: 0.5760\n",
      "Epoch: 0447 acc_train: 0.7023 acc_test: 0.6370 f1_test: 0.5754 precision: 0.6930 recall: 0.4920\n",
      "Epoch: 0448 acc_train: 0.7053 acc_test: 0.6450 f1_test: 0.6259 precision: 0.6615 recall: 0.5940\n",
      "Epoch: 0449 acc_train: 0.7000 acc_test: 0.6360 f1_test: 0.5667 precision: 0.7000 recall: 0.4760\n",
      "Epoch: 0450 acc_train: 0.7057 acc_test: 0.6460 f1_test: 0.6289 precision: 0.6608 recall: 0.6000\n",
      "Epoch: 0451 acc_train: 0.7034 acc_test: 0.6420 f1_test: 0.5828 precision: 0.6983 recall: 0.5000\n",
      "Epoch: 0452 acc_train: 0.7048 acc_test: 0.6440 f1_test: 0.6147 precision: 0.6698 recall: 0.5680\n",
      "Epoch: 0453 acc_train: 0.7061 acc_test: 0.6390 f1_test: 0.6080 precision: 0.6651 recall: 0.5600\n",
      "Epoch: 0454 acc_train: 0.7030 acc_test: 0.6390 f1_test: 0.5836 precision: 0.6894 recall: 0.5060\n",
      "Epoch: 0455 acc_train: 0.7057 acc_test: 0.6440 f1_test: 0.6245 precision: 0.6607 recall: 0.5920\n",
      "Epoch: 0456 acc_train: 0.7024 acc_test: 0.6330 f1_test: 0.5667 precision: 0.6916 recall: 0.4800\n",
      "Epoch: 0457 acc_train: 0.7078 acc_test: 0.6450 f1_test: 0.6187 precision: 0.6682 recall: 0.5760\n",
      "Epoch: 0458 acc_train: 0.7060 acc_test: 0.6420 f1_test: 0.6022 precision: 0.6775 recall: 0.5420\n",
      "Epoch: 0459 acc_train: 0.7066 acc_test: 0.6440 f1_test: 0.6036 precision: 0.6809 recall: 0.5420\n",
      "Epoch: 0460 acc_train: 0.7075 acc_test: 0.6450 f1_test: 0.6211 precision: 0.6659 recall: 0.5820\n",
      "Epoch: 0461 acc_train: 0.7025 acc_test: 0.6360 f1_test: 0.5758 precision: 0.6899 recall: 0.4940\n",
      "Epoch: 0462 acc_train: 0.7064 acc_test: 0.6460 f1_test: 0.6266 precision: 0.6629 recall: 0.5940\n",
      "Epoch: 0463 acc_train: 0.7036 acc_test: 0.6380 f1_test: 0.5771 precision: 0.6938 recall: 0.4940\n",
      "Epoch: 0464 acc_train: 0.7093 acc_test: 0.6420 f1_test: 0.6126 precision: 0.6675 recall: 0.5660\n",
      "Epoch: 0465 acc_train: 0.7076 acc_test: 0.6450 f1_test: 0.6112 precision: 0.6755 recall: 0.5580\n",
      "Epoch: 0466 acc_train: 0.7060 acc_test: 0.6420 f1_test: 0.5941 precision: 0.6859 recall: 0.5240\n",
      "Epoch: 0467 acc_train: 0.7079 acc_test: 0.6410 f1_test: 0.6193 precision: 0.6591 recall: 0.5840\n",
      "Epoch: 0468 acc_train: 0.7034 acc_test: 0.6350 f1_test: 0.5721 precision: 0.6912 recall: 0.4880\n",
      "Epoch: 0469 acc_train: 0.7071 acc_test: 0.6450 f1_test: 0.6243 precision: 0.6629 recall: 0.5900\n",
      "Epoch: 0470 acc_train: 0.7078 acc_test: 0.6400 f1_test: 0.5890 precision: 0.6862 recall: 0.5160\n",
      "Epoch: 0471 acc_train: 0.7091 acc_test: 0.6460 f1_test: 0.6127 precision: 0.6763 recall: 0.5600\n",
      "Epoch: 0472 acc_train: 0.7101 acc_test: 0.6430 f1_test: 0.6141 precision: 0.6682 recall: 0.5680\n",
      "Epoch: 0473 acc_train: 0.7079 acc_test: 0.6360 f1_test: 0.5816 precision: 0.6838 recall: 0.5060\n",
      "Epoch: 0474 acc_train: 0.7072 acc_test: 0.6440 f1_test: 0.6221 precision: 0.6629 recall: 0.5860\n",
      "Epoch: 0475 acc_train: 0.7065 acc_test: 0.6330 f1_test: 0.5728 precision: 0.6852 recall: 0.4920\n",
      "Epoch: 0476 acc_train: 0.7119 acc_test: 0.6440 f1_test: 0.6180 precision: 0.6667 recall: 0.5760\n",
      "Epoch: 0477 acc_train: 0.7096 acc_test: 0.6500 f1_test: 0.6094 precision: 0.6894 recall: 0.5460\n",
      "Epoch: 0478 acc_train: 0.7104 acc_test: 0.6440 f1_test: 0.6027 precision: 0.6818 recall: 0.5400\n",
      "Epoch: 0479 acc_train: 0.7113 acc_test: 0.6460 f1_test: 0.6194 precision: 0.6698 recall: 0.5760\n",
      "Epoch: 0480 acc_train: 0.7056 acc_test: 0.6340 f1_test: 0.5754 precision: 0.6851 recall: 0.4960\n",
      "Epoch: 0481 acc_train: 0.7100 acc_test: 0.6410 f1_test: 0.6169 precision: 0.6613 recall: 0.5780\n",
      "Epoch: 0482 acc_train: 0.7089 acc_test: 0.6340 f1_test: 0.5793 precision: 0.6811 recall: 0.5040\n",
      "Epoch: 0483 acc_train: 0.7108 acc_test: 0.6480 f1_test: 0.6199 precision: 0.6737 recall: 0.5740\n",
      "Epoch: 0484 acc_train: 0.7111 acc_test: 0.6390 f1_test: 0.5984 precision: 0.6742 recall: 0.5380\n",
      "Epoch: 0485 acc_train: 0.7102 acc_test: 0.6430 f1_test: 0.6020 precision: 0.6801 recall: 0.5400\n",
      "Epoch: 0486 acc_train: 0.7119 acc_test: 0.6470 f1_test: 0.6192 precision: 0.6721 recall: 0.5740\n",
      "Epoch: 0487 acc_train: 0.7099 acc_test: 0.6370 f1_test: 0.5832 precision: 0.6846 recall: 0.5080\n",
      "Epoch: 0488 acc_train: 0.7098 acc_test: 0.6420 f1_test: 0.6191 precision: 0.6614 recall: 0.5820\n",
      "Epoch: 0489 acc_train: 0.7073 acc_test: 0.6330 f1_test: 0.5687 precision: 0.6895 recall: 0.4840\n",
      "Epoch: 0490 acc_train: 0.7111 acc_test: 0.6430 f1_test: 0.6222 precision: 0.6607 recall: 0.5880\n",
      "Epoch: 0491 acc_train: 0.7076 acc_test: 0.6310 f1_test: 0.5664 precision: 0.6866 recall: 0.4820\n",
      "Epoch: 0492 acc_train: 0.7123 acc_test: 0.6420 f1_test: 0.6191 precision: 0.6614 recall: 0.5820\n",
      "Epoch: 0493 acc_train: 0.7122 acc_test: 0.6380 f1_test: 0.5886 precision: 0.6816 recall: 0.5180\n",
      "Epoch: 0494 acc_train: 0.7130 acc_test: 0.6460 f1_test: 0.6127 precision: 0.6763 recall: 0.5600\n",
      "Epoch: 0495 acc_train: 0.7142 acc_test: 0.6490 f1_test: 0.6189 precision: 0.6770 recall: 0.5700\n",
      "Epoch: 0496 acc_train: 0.7131 acc_test: 0.6390 f1_test: 0.5912 precision: 0.6815 recall: 0.5220\n",
      "Epoch: 0497 acc_train: 0.7124 acc_test: 0.6480 f1_test: 0.6247 precision: 0.6689 recall: 0.5860\n",
      "Epoch: 0498 acc_train: 0.7091 acc_test: 0.6340 f1_test: 0.5684 precision: 0.6925 recall: 0.4820\n",
      "Epoch: 0499 acc_train: 0.7103 acc_test: 0.6490 f1_test: 0.6301 precision: 0.6659 recall: 0.5980\n",
      "Epoch: 0500 acc_train: 0.7093 acc_test: 0.6330 f1_test: 0.5657 precision: 0.6928 recall: 0.4780\n",
      "Epoch: 0501 acc_train: 0.7117 acc_test: 0.6490 f1_test: 0.6301 precision: 0.6659 recall: 0.5980\n",
      "Epoch: 0502 acc_train: 0.7108 acc_test: 0.6360 f1_test: 0.5777 precision: 0.6878 recall: 0.4980\n",
      "Epoch: 0503 acc_train: 0.7144 acc_test: 0.6490 f1_test: 0.6197 precision: 0.6761 recall: 0.5720\n",
      "Epoch: 0504 acc_train: 0.7144 acc_test: 0.6510 f1_test: 0.6186 precision: 0.6819 recall: 0.5660\n",
      "Epoch: 0505 acc_train: 0.7130 acc_test: 0.6450 f1_test: 0.5998 precision: 0.6873 recall: 0.5320\n",
      "Epoch: 0506 acc_train: 0.7138 acc_test: 0.6440 f1_test: 0.6221 precision: 0.6629 recall: 0.5860\n",
      "Epoch: 0507 acc_train: 0.7100 acc_test: 0.6360 f1_test: 0.5718 precision: 0.6943 recall: 0.4860\n",
      "Epoch: 0508 acc_train: 0.7120 acc_test: 0.6490 f1_test: 0.6301 precision: 0.6659 recall: 0.5980\n",
      "Epoch: 0509 acc_train: 0.7101 acc_test: 0.6330 f1_test: 0.5687 precision: 0.6895 recall: 0.4840\n",
      "Epoch: 0510 acc_train: 0.7158 acc_test: 0.6460 f1_test: 0.6210 precision: 0.6682 recall: 0.5800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0511 acc_train: 0.7130 acc_test: 0.6400 f1_test: 0.5955 precision: 0.6795 recall: 0.5300\n",
      "Epoch: 0512 acc_train: 0.7153 acc_test: 0.6430 f1_test: 0.6064 precision: 0.6757 recall: 0.5500\n",
      "Epoch: 0513 acc_train: 0.7162 acc_test: 0.6520 f1_test: 0.6250 precision: 0.6776 recall: 0.5800\n",
      "Epoch: 0514 acc_train: 0.7132 acc_test: 0.6380 f1_test: 0.5849 precision: 0.6855 recall: 0.5100\n",
      "Epoch: 0515 acc_train: 0.7150 acc_test: 0.6450 f1_test: 0.6243 precision: 0.6629 recall: 0.5900\n",
      "Epoch: 0516 acc_train: 0.7125 acc_test: 0.6350 f1_test: 0.5691 precision: 0.6945 recall: 0.4820\n",
      "Epoch: 0517 acc_train: 0.7144 acc_test: 0.6460 f1_test: 0.6250 precision: 0.6644 recall: 0.5900\n",
      "Epoch: 0518 acc_train: 0.7129 acc_test: 0.6380 f1_test: 0.5810 precision: 0.6896 recall: 0.5020\n",
      "Epoch: 0519 acc_train: 0.7158 acc_test: 0.6470 f1_test: 0.6176 precision: 0.6738 recall: 0.5700\n",
      "Epoch: 0520 acc_train: 0.7167 acc_test: 0.6460 f1_test: 0.6110 precision: 0.6780 recall: 0.5560\n",
      "Epoch: 0521 acc_train: 0.7166 acc_test: 0.6450 f1_test: 0.6051 precision: 0.6817 recall: 0.5440\n",
      "Epoch: 0522 acc_train: 0.7170 acc_test: 0.6480 f1_test: 0.6223 precision: 0.6713 recall: 0.5800\n",
      "Epoch: 0523 acc_train: 0.7120 acc_test: 0.6350 f1_test: 0.5731 precision: 0.6901 recall: 0.4900\n",
      "Epoch: 0524 acc_train: 0.7132 acc_test: 0.6490 f1_test: 0.6317 precision: 0.6645 recall: 0.6020\n",
      "Epoch: 0525 acc_train: 0.7110 acc_test: 0.6370 f1_test: 0.5673 precision: 0.7021 recall: 0.4760\n",
      "Epoch: 0526 acc_train: 0.7164 acc_test: 0.6490 f1_test: 0.6270 precision: 0.6689 recall: 0.5900\n",
      "Epoch: 0527 acc_train: 0.7167 acc_test: 0.6380 f1_test: 0.5877 precision: 0.6825 recall: 0.5160\n",
      "Epoch: 0528 acc_train: 0.7187 acc_test: 0.6520 f1_test: 0.6193 precision: 0.6836 recall: 0.5660\n",
      "Epoch: 0529 acc_train: 0.7179 acc_test: 0.6510 f1_test: 0.6227 precision: 0.6776 recall: 0.5760\n",
      "Epoch: 0530 acc_train: 0.7142 acc_test: 0.6390 f1_test: 0.5836 precision: 0.6894 recall: 0.5060\n",
      "Epoch: 0531 acc_train: 0.7152 acc_test: 0.6490 f1_test: 0.6309 precision: 0.6652 recall: 0.6000\n",
      "Epoch: 0532 acc_train: 0.7125 acc_test: 0.6340 f1_test: 0.5643 precision: 0.6971 recall: 0.4740\n",
      "Epoch: 0533 acc_train: 0.7153 acc_test: 0.6510 f1_test: 0.6315 precision: 0.6689 recall: 0.5980\n",
      "Epoch: 0534 acc_train: 0.7157 acc_test: 0.6330 f1_test: 0.5767 precision: 0.6812 recall: 0.5000\n",
      "Epoch: 0535 acc_train: 0.7184 acc_test: 0.6470 f1_test: 0.6150 precision: 0.6763 recall: 0.5640\n",
      "Epoch: 0536 acc_train: 0.7200 acc_test: 0.6490 f1_test: 0.6205 precision: 0.6753 recall: 0.5740\n",
      "Epoch: 0537 acc_train: 0.7163 acc_test: 0.6380 f1_test: 0.5820 precision: 0.6885 recall: 0.5040\n",
      "Epoch: 0538 acc_train: 0.7163 acc_test: 0.6480 f1_test: 0.6287 precision: 0.6652 recall: 0.5960\n",
      "Epoch: 0539 acc_train: 0.7130 acc_test: 0.6360 f1_test: 0.5677 precision: 0.6988 recall: 0.4780\n",
      "Epoch: 0540 acc_train: 0.7179 acc_test: 0.6500 f1_test: 0.6316 precision: 0.6667 recall: 0.6000\n",
      "Epoch: 0541 acc_train: 0.7177 acc_test: 0.6410 f1_test: 0.5897 precision: 0.6880 recall: 0.5160\n",
      "Epoch: 0542 acc_train: 0.7201 acc_test: 0.6430 f1_test: 0.6073 precision: 0.6748 recall: 0.5520\n",
      "Epoch: 0543 acc_train: 0.7211 acc_test: 0.6500 f1_test: 0.6261 precision: 0.6720 recall: 0.5860\n",
      "Epoch: 0544 acc_train: 0.7142 acc_test: 0.6370 f1_test: 0.5744 precision: 0.6941 recall: 0.4900\n",
      "Epoch: 0545 acc_train: 0.7167 acc_test: 0.6520 f1_test: 0.6383 precision: 0.6645 recall: 0.6140\n",
      "Epoch: 0546 acc_train: 0.7133 acc_test: 0.6420 f1_test: 0.5738 precision: 0.7088 recall: 0.4820\n",
      "Epoch: 0547 acc_train: 0.7210 acc_test: 0.6460 f1_test: 0.6202 precision: 0.6690 recall: 0.5780\n",
      "Epoch: 0548 acc_train: 0.7228 acc_test: 0.6450 f1_test: 0.6095 precision: 0.6773 recall: 0.5540\n",
      "Epoch: 0549 acc_train: 0.7173 acc_test: 0.6430 f1_test: 0.5844 precision: 0.6992 recall: 0.5020\n",
      "Epoch: 0550 acc_train: 0.7203 acc_test: 0.6470 f1_test: 0.6319 precision: 0.6601 recall: 0.6060\n",
      "Epoch: 0551 acc_train: 0.7161 acc_test: 0.6320 f1_test: 0.5741 precision: 0.6813 recall: 0.4960\n",
      "Epoch: 0552 acc_train: 0.7222 acc_test: 0.6520 f1_test: 0.6258 precision: 0.6767 recall: 0.5820\n",
      "Epoch: 0553 acc_train: 0.7244 acc_test: 0.6530 f1_test: 0.6191 precision: 0.6861 recall: 0.5640\n",
      "Epoch: 0554 acc_train: 0.7225 acc_test: 0.6400 f1_test: 0.5918 precision: 0.6832 recall: 0.5220\n",
      "Epoch: 0555 acc_train: 0.7213 acc_test: 0.6530 f1_test: 0.6297 precision: 0.6751 recall: 0.5900\n",
      "Epoch: 0556 acc_train: 0.7186 acc_test: 0.6350 f1_test: 0.5741 precision: 0.6891 recall: 0.4920\n",
      "Epoch: 0557 acc_train: 0.7235 acc_test: 0.6510 f1_test: 0.6259 precision: 0.6744 recall: 0.5840\n",
      "Epoch: 0558 acc_train: 0.7221 acc_test: 0.6390 f1_test: 0.5957 precision: 0.6768 recall: 0.5320\n",
      "Epoch: 0559 acc_train: 0.7250 acc_test: 0.6460 f1_test: 0.6110 precision: 0.6780 recall: 0.5560\n",
      "Epoch: 0560 acc_train: 0.7262 acc_test: 0.6480 f1_test: 0.6223 precision: 0.6713 recall: 0.5800\n",
      "Epoch: 0561 acc_train: 0.7203 acc_test: 0.6410 f1_test: 0.5897 precision: 0.6880 recall: 0.5160\n",
      "Epoch: 0562 acc_train: 0.7245 acc_test: 0.6520 f1_test: 0.6306 precision: 0.6719 recall: 0.5940\n",
      "Epoch: 0563 acc_train: 0.7200 acc_test: 0.6370 f1_test: 0.5813 precision: 0.6866 recall: 0.5040\n",
      "Epoch: 0564 acc_train: 0.7239 acc_test: 0.6490 f1_test: 0.6222 precision: 0.6737 recall: 0.5780\n",
      "Epoch: 0565 acc_train: 0.7241 acc_test: 0.6410 f1_test: 0.5989 precision: 0.6785 recall: 0.5360\n",
      "Epoch: 0566 acc_train: 0.7259 acc_test: 0.6470 f1_test: 0.6134 precision: 0.6780 recall: 0.5600\n",
      "Epoch: 0567 acc_train: 0.7257 acc_test: 0.6480 f1_test: 0.6207 precision: 0.6729 recall: 0.5760\n",
      "Epoch: 0568 acc_train: 0.7258 acc_test: 0.6370 f1_test: 0.5908 precision: 0.6770 recall: 0.5240\n",
      "Epoch: 0569 acc_train: 0.7251 acc_test: 0.6470 f1_test: 0.6233 precision: 0.6682 recall: 0.5840\n",
      "Epoch: 0570 acc_train: 0.7192 acc_test: 0.6380 f1_test: 0.5781 precision: 0.6927 recall: 0.4960\n",
      "Epoch: 0571 acc_train: 0.7240 acc_test: 0.6500 f1_test: 0.6347 precision: 0.6638 recall: 0.6080\n",
      "Epoch: 0572 acc_train: 0.7204 acc_test: 0.6380 f1_test: 0.5771 precision: 0.6938 recall: 0.4940\n",
      "Epoch: 0573 acc_train: 0.7267 acc_test: 0.6480 f1_test: 0.6247 precision: 0.6689 recall: 0.5860\n",
      "Epoch: 0574 acc_train: 0.7270 acc_test: 0.6430 f1_test: 0.6038 precision: 0.6783 recall: 0.5440\n",
      "Epoch: 0575 acc_train: 0.7271 acc_test: 0.6440 f1_test: 0.6079 precision: 0.6765 recall: 0.5520\n",
      "Epoch: 0576 acc_train: 0.7285 acc_test: 0.6480 f1_test: 0.6231 precision: 0.6705 recall: 0.5820\n",
      "Epoch: 0577 acc_train: 0.7256 acc_test: 0.6420 f1_test: 0.5904 precision: 0.6898 recall: 0.5160\n",
      "Epoch: 0578 acc_train: 0.7256 acc_test: 0.6470 f1_test: 0.6280 precision: 0.6637 recall: 0.5960\n",
      "Epoch: 0579 acc_train: 0.7202 acc_test: 0.6370 f1_test: 0.5724 precision: 0.6963 recall: 0.4860\n",
      "Epoch: 0580 acc_train: 0.7243 acc_test: 0.6490 f1_test: 0.6317 precision: 0.6645 recall: 0.6020\n",
      "Epoch: 0581 acc_train: 0.7227 acc_test: 0.6410 f1_test: 0.5840 precision: 0.6942 recall: 0.5040\n",
      "Epoch: 0582 acc_train: 0.7286 acc_test: 0.6490 f1_test: 0.6238 precision: 0.6721 recall: 0.5820\n",
      "Epoch: 0583 acc_train: 0.7285 acc_test: 0.6450 f1_test: 0.6103 precision: 0.6764 recall: 0.5560\n",
      "Epoch: 0584 acc_train: 0.7292 acc_test: 0.6420 f1_test: 0.6049 precision: 0.6749 recall: 0.5480\n",
      "Epoch: 0585 acc_train: 0.7297 acc_test: 0.6490 f1_test: 0.6246 precision: 0.6713 recall: 0.5840\n",
      "Epoch: 0586 acc_train: 0.7238 acc_test: 0.6440 f1_test: 0.5917 precision: 0.6935 recall: 0.5160\n",
      "Epoch: 0587 acc_train: 0.7262 acc_test: 0.6510 f1_test: 0.6307 precision: 0.6697 recall: 0.5960\n",
      "Epoch: 0588 acc_train: 0.7228 acc_test: 0.6400 f1_test: 0.5804 precision: 0.6955 recall: 0.4980\n",
      "Epoch: 0589 acc_train: 0.7285 acc_test: 0.6490 f1_test: 0.6262 precision: 0.6697 recall: 0.5880\n",
      "Epoch: 0590 acc_train: 0.7276 acc_test: 0.6430 f1_test: 0.5957 precision: 0.6867 recall: 0.5260\n",
      "Epoch: 0591 acc_train: 0.7291 acc_test: 0.6500 f1_test: 0.6196 precision: 0.6786 recall: 0.5700\n",
      "Epoch: 0592 acc_train: 0.7317 acc_test: 0.6460 f1_test: 0.6152 precision: 0.6738 recall: 0.5660\n",
      "Epoch: 0593 acc_train: 0.7301 acc_test: 0.6420 f1_test: 0.6013 precision: 0.6784 recall: 0.5400\n",
      "Epoch: 0594 acc_train: 0.7299 acc_test: 0.6480 f1_test: 0.6215 precision: 0.6721 recall: 0.5780\n",
      "Epoch: 0595 acc_train: 0.7264 acc_test: 0.6350 f1_test: 0.5800 precision: 0.6829 recall: 0.5040\n",
      "Epoch: 0596 acc_train: 0.7270 acc_test: 0.6520 f1_test: 0.6345 precision: 0.6681 recall: 0.6040\n",
      "Epoch: 0597 acc_train: 0.7248 acc_test: 0.6370 f1_test: 0.5734 precision: 0.6952 recall: 0.4880\n",
      "Epoch: 0598 acc_train: 0.7286 acc_test: 0.6490 f1_test: 0.6309 precision: 0.6652 recall: 0.6000\n",
      "Epoch: 0599 acc_train: 0.7263 acc_test: 0.6400 f1_test: 0.5804 precision: 0.6955 recall: 0.4980\n",
      "Epoch: 0600 acc_train: 0.7316 acc_test: 0.6470 f1_test: 0.6249 precision: 0.6667 recall: 0.5880\n",
      "Epoch: 0601 acc_train: 0.7297 acc_test: 0.6450 f1_test: 0.6042 precision: 0.6826 recall: 0.5420\n",
      "Epoch: 0602 acc_train: 0.7323 acc_test: 0.6460 f1_test: 0.6135 precision: 0.6755 recall: 0.5620\n",
      "Epoch: 0603 acc_train: 0.7320 acc_test: 0.6460 f1_test: 0.6185 precision: 0.6706 recall: 0.5740\n",
      "Epoch: 0604 acc_train: 0.7295 acc_test: 0.6400 f1_test: 0.5937 precision: 0.6813 recall: 0.5260\n",
      "Epoch: 0605 acc_train: 0.7326 acc_test: 0.6480 f1_test: 0.6255 precision: 0.6682 recall: 0.5880\n",
      "Epoch: 0606 acc_train: 0.7273 acc_test: 0.6410 f1_test: 0.5830 precision: 0.6953 recall: 0.5020\n",
      "Epoch: 0607 acc_train: 0.7303 acc_test: 0.6490 f1_test: 0.6332 precision: 0.6630 recall: 0.6060\n",
      "Epoch: 0608 acc_train: 0.7247 acc_test: 0.6400 f1_test: 0.5724 precision: 0.7047 recall: 0.4820\n",
      "Epoch: 0609 acc_train: 0.7298 acc_test: 0.6480 f1_test: 0.6341 precision: 0.6602 recall: 0.6100\n",
      "Epoch: 0610 acc_train: 0.7275 acc_test: 0.6410 f1_test: 0.5801 precision: 0.6986 recall: 0.4960\n",
      "Epoch: 0611 acc_train: 0.7351 acc_test: 0.6510 f1_test: 0.6251 precision: 0.6752 recall: 0.5820\n",
      "Epoch: 0612 acc_train: 0.7363 acc_test: 0.6430 f1_test: 0.6115 precision: 0.6706 recall: 0.5620\n",
      "Epoch: 0613 acc_train: 0.7320 acc_test: 0.6440 f1_test: 0.6000 precision: 0.6846 recall: 0.5340\n",
      "Epoch: 0614 acc_train: 0.7334 acc_test: 0.6500 f1_test: 0.6331 precision: 0.6652 recall: 0.6040\n",
      "Epoch: 0615 acc_train: 0.7281 acc_test: 0.6370 f1_test: 0.5724 precision: 0.6963 recall: 0.4860\n",
      "Epoch: 0616 acc_train: 0.7325 acc_test: 0.6500 f1_test: 0.6308 precision: 0.6674 recall: 0.5980\n",
      "Epoch: 0617 acc_train: 0.7314 acc_test: 0.6360 f1_test: 0.5758 precision: 0.6899 recall: 0.4940\n",
      "Epoch: 0618 acc_train: 0.7355 acc_test: 0.6480 f1_test: 0.6223 precision: 0.6713 recall: 0.5800\n",
      "Epoch: 0619 acc_train: 0.7370 acc_test: 0.6410 f1_test: 0.6111 precision: 0.6667 recall: 0.5640\n",
      "Epoch: 0620 acc_train: 0.7356 acc_test: 0.6420 f1_test: 0.5987 precision: 0.6811 recall: 0.5340\n",
      "Epoch: 0621 acc_train: 0.7356 acc_test: 0.6490 f1_test: 0.6286 precision: 0.6674 recall: 0.5940\n",
      "Epoch: 0622 acc_train: 0.7312 acc_test: 0.6360 f1_test: 0.5767 precision: 0.6889 recall: 0.4960\n",
      "Epoch: 0623 acc_train: 0.7326 acc_test: 0.6500 f1_test: 0.6354 precision: 0.6630 recall: 0.6100\n",
      "Epoch: 0624 acc_train: 0.7304 acc_test: 0.6370 f1_test: 0.5724 precision: 0.6963 recall: 0.4860\n",
      "Epoch: 0625 acc_train: 0.7377 acc_test: 0.6480 f1_test: 0.6247 precision: 0.6689 recall: 0.5860\n",
      "Epoch: 0626 acc_train: 0.7375 acc_test: 0.6370 f1_test: 0.5998 precision: 0.6683 recall: 0.5440\n",
      "Epoch: 0627 acc_train: 0.7355 acc_test: 0.6410 f1_test: 0.5998 precision: 0.6776 recall: 0.5380\n",
      "Epoch: 0628 acc_train: 0.7376 acc_test: 0.6500 f1_test: 0.6300 precision: 0.6682 recall: 0.5960\n",
      "Epoch: 0629 acc_train: 0.7304 acc_test: 0.6390 f1_test: 0.5758 precision: 0.6980 recall: 0.4900\n",
      "Epoch: 0630 acc_train: 0.7279 acc_test: 0.6420 f1_test: 0.6354 precision: 0.6473 recall: 0.6240\n",
      "Epoch: 0631 acc_train: 0.7275 acc_test: 0.6310 f1_test: 0.5538 precision: 0.7003 recall: 0.4580\n",
      "Epoch: 0632 acc_train: 0.7371 acc_test: 0.6470 f1_test: 0.6184 precision: 0.6729 recall: 0.5720\n",
      "Epoch: 0633 acc_train: 0.7399 acc_test: 0.6510 f1_test: 0.6251 precision: 0.6752 recall: 0.5820\n",
      "Epoch: 0634 acc_train: 0.7308 acc_test: 0.6370 f1_test: 0.5784 precision: 0.6898 recall: 0.4980\n",
      "Epoch: 0635 acc_train: 0.7337 acc_test: 0.6540 f1_test: 0.6373 precision: 0.6696 recall: 0.6080\n",
      "Epoch: 0636 acc_train: 0.7363 acc_test: 0.6400 f1_test: 0.5991 precision: 0.6759 recall: 0.5380\n",
      "Epoch: 0637 acc_train: 0.7366 acc_test: 0.6450 f1_test: 0.6086 precision: 0.6781 recall: 0.5520\n",
      "Epoch: 0638 acc_train: 0.7390 acc_test: 0.6550 f1_test: 0.6349 precision: 0.6742 recall: 0.6000\n",
      "Epoch: 0639 acc_train: 0.7334 acc_test: 0.6310 f1_test: 0.5674 precision: 0.6856 recall: 0.4840\n",
      "Epoch: 0640 acc_train: 0.7377 acc_test: 0.6480 f1_test: 0.6247 precision: 0.6689 recall: 0.5860\n",
      "Epoch: 0641 acc_train: 0.7411 acc_test: 0.6360 f1_test: 0.5928 precision: 0.6726 recall: 0.5300\n",
      "Epoch: 0642 acc_train: 0.7391 acc_test: 0.6310 f1_test: 0.5877 precision: 0.6658 recall: 0.5260\n",
      "Epoch: 0643 acc_train: 0.7387 acc_test: 0.6480 f1_test: 0.6263 precision: 0.6674 recall: 0.5900\n",
      "Epoch: 0644 acc_train: 0.7366 acc_test: 0.6430 f1_test: 0.5863 precision: 0.6970 recall: 0.5060\n",
      "Epoch: 0645 acc_train: 0.7362 acc_test: 0.6480 f1_test: 0.6279 precision: 0.6659 recall: 0.5940\n",
      "Epoch: 0646 acc_train: 0.7370 acc_test: 0.6430 f1_test: 0.5920 precision: 0.6907 recall: 0.5180\n",
      "Epoch: 0647 acc_train: 0.7408 acc_test: 0.6460 f1_test: 0.6152 precision: 0.6738 recall: 0.5660\n",
      "Epoch: 0648 acc_train: 0.7412 acc_test: 0.6390 f1_test: 0.6029 precision: 0.6699 recall: 0.5480\n",
      "Epoch: 0649 acc_train: 0.7407 acc_test: 0.6410 f1_test: 0.5953 precision: 0.6822 recall: 0.5280\n",
      "Epoch: 0650 acc_train: 0.7404 acc_test: 0.6480 f1_test: 0.6271 precision: 0.6667 recall: 0.5920\n",
      "Epoch: 0651 acc_train: 0.7376 acc_test: 0.6290 f1_test: 0.5701 precision: 0.6777 recall: 0.4920\n",
      "Epoch: 0652 acc_train: 0.7422 acc_test: 0.6490 f1_test: 0.6286 precision: 0.6674 recall: 0.5940\n",
      "Epoch: 0653 acc_train: 0.7391 acc_test: 0.6360 f1_test: 0.5826 precision: 0.6828 recall: 0.5080\n",
      "Epoch: 0654 acc_train: 0.7438 acc_test: 0.6430 f1_test: 0.6190 precision: 0.6636 recall: 0.5800\n",
      "Epoch: 0655 acc_train: 0.7432 acc_test: 0.6380 f1_test: 0.5933 precision: 0.6769 recall: 0.5280\n",
      "Epoch: 0656 acc_train: 0.7437 acc_test: 0.6410 f1_test: 0.6094 precision: 0.6683 recall: 0.5600\n",
      "Epoch: 0657 acc_train: 0.7450 acc_test: 0.6420 f1_test: 0.6100 precision: 0.6699 recall: 0.5600\n",
      "Epoch: 0658 acc_train: 0.7435 acc_test: 0.6330 f1_test: 0.5881 precision: 0.6701 recall: 0.5240\n",
      "Epoch: 0659 acc_train: 0.7431 acc_test: 0.6440 f1_test: 0.6205 precision: 0.6644 recall: 0.5820\n",
      "Epoch: 0660 acc_train: 0.7379 acc_test: 0.6360 f1_test: 0.5728 precision: 0.6932 recall: 0.4880\n",
      "Epoch: 0661 acc_train: 0.7307 acc_test: 0.6320 f1_test: 0.6342 precision: 0.6304 recall: 0.6380\n",
      "Epoch: 0662 acc_train: 0.7249 acc_test: 0.6080 f1_test: 0.5063 precision: 0.6837 recall: 0.4020\n",
      "Epoch: 0663 acc_train: 0.7342 acc_test: 0.6400 f1_test: 0.6334 precision: 0.6452 recall: 0.6220\n",
      "Epoch: 0664 acc_train: 0.7438 acc_test: 0.6420 f1_test: 0.6049 precision: 0.6749 recall: 0.5480\n",
      "Epoch: 0665 acc_train: 0.7317 acc_test: 0.6280 f1_test: 0.5474 precision: 0.6988 recall: 0.4500\n",
      "Epoch: 0666 acc_train: 0.7355 acc_test: 0.6420 f1_test: 0.6279 precision: 0.6537 recall: 0.6040\n",
      "Epoch: 0667 acc_train: 0.7415 acc_test: 0.6340 f1_test: 0.5987 precision: 0.6626 recall: 0.5460\n",
      "Epoch: 0668 acc_train: 0.7364 acc_test: 0.6280 f1_test: 0.5674 precision: 0.6778 recall: 0.4880\n",
      "Epoch: 0669 acc_train: 0.7390 acc_test: 0.6510 f1_test: 0.6353 precision: 0.6652 recall: 0.6080\n",
      "Epoch: 0670 acc_train: 0.7457 acc_test: 0.6420 f1_test: 0.6109 precision: 0.6690 recall: 0.5620\n",
      "Epoch: 0671 acc_train: 0.7392 acc_test: 0.6290 f1_test: 0.5731 precision: 0.6748 recall: 0.4980\n",
      "Epoch: 0672 acc_train: 0.7419 acc_test: 0.6490 f1_test: 0.6317 precision: 0.6645 recall: 0.6020\n",
      "Epoch: 0673 acc_train: 0.7486 acc_test: 0.6370 f1_test: 0.5971 precision: 0.6708 recall: 0.5380\n",
      "Epoch: 0674 acc_train: 0.7408 acc_test: 0.6260 f1_test: 0.5681 precision: 0.6721 recall: 0.4920\n",
      "Epoch: 0675 acc_train: 0.7426 acc_test: 0.6480 f1_test: 0.6318 precision: 0.6623 recall: 0.6040\n",
      "Epoch: 0676 acc_train: 0.7418 acc_test: 0.6290 f1_test: 0.5817 precision: 0.6667 recall: 0.5160\n",
      "Epoch: 0677 acc_train: 0.7437 acc_test: 0.6330 f1_test: 0.5853 precision: 0.6727 recall: 0.5180\n",
      "Epoch: 0678 acc_train: 0.7452 acc_test: 0.6440 f1_test: 0.6268 precision: 0.6586 recall: 0.5980\n",
      "Epoch: 0679 acc_train: 0.7404 acc_test: 0.6390 f1_test: 0.5797 precision: 0.6936 recall: 0.4980\n",
      "Epoch: 0680 acc_train: 0.7464 acc_test: 0.6390 f1_test: 0.6172 precision: 0.6569 recall: 0.5820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0681 acc_train: 0.7480 acc_test: 0.6310 f1_test: 0.5914 precision: 0.6625 recall: 0.5340\n",
      "Epoch: 0682 acc_train: 0.7473 acc_test: 0.6390 f1_test: 0.5966 precision: 0.6759 recall: 0.5340\n",
      "Epoch: 0683 acc_train: 0.7441 acc_test: 0.6420 f1_test: 0.6200 precision: 0.6606 recall: 0.5840\n",
      "Epoch: 0684 acc_train: 0.7466 acc_test: 0.6370 f1_test: 0.5870 precision: 0.6807 recall: 0.5160\n",
      "Epoch: 0685 acc_train: 0.7518 acc_test: 0.6410 f1_test: 0.6144 precision: 0.6636 recall: 0.5720\n",
      "Epoch: 0686 acc_train: 0.7483 acc_test: 0.6390 f1_test: 0.6072 precision: 0.6659 recall: 0.5580\n",
      "Epoch: 0687 acc_train: 0.7488 acc_test: 0.6400 f1_test: 0.5982 precision: 0.6768 recall: 0.5360\n",
      "Epoch: 0688 acc_train: 0.7476 acc_test: 0.6440 f1_test: 0.6237 precision: 0.6614 recall: 0.5900\n",
      "Epoch: 0689 acc_train: 0.7458 acc_test: 0.6340 f1_test: 0.5793 precision: 0.6811 recall: 0.5040\n",
      "Epoch: 0690 acc_train: 0.7484 acc_test: 0.6400 f1_test: 0.6178 precision: 0.6584 recall: 0.5820\n",
      "Epoch: 0691 acc_train: 0.7488 acc_test: 0.6330 f1_test: 0.5834 precision: 0.6745 recall: 0.5140\n",
      "Epoch: 0692 acc_train: 0.7501 acc_test: 0.6350 f1_test: 0.6037 precision: 0.6603 recall: 0.5560\n",
      "Epoch: 0693 acc_train: 0.7508 acc_test: 0.6360 f1_test: 0.6018 precision: 0.6643 recall: 0.5500\n",
      "Epoch: 0694 acc_train: 0.7508 acc_test: 0.6320 f1_test: 0.5965 precision: 0.6602 recall: 0.5440\n",
      "Epoch: 0695 acc_train: 0.7525 acc_test: 0.6360 f1_test: 0.6103 precision: 0.6567 recall: 0.5700\n",
      "Epoch: 0696 acc_train: 0.7495 acc_test: 0.6350 f1_test: 0.5857 precision: 0.6772 recall: 0.5160\n",
      "Epoch: 0697 acc_train: 0.7486 acc_test: 0.6410 f1_test: 0.6249 precision: 0.6543 recall: 0.5980\n",
      "Epoch: 0698 acc_train: 0.7433 acc_test: 0.6300 f1_test: 0.5595 precision: 0.6912 recall: 0.4700\n",
      "Epoch: 0699 acc_train: 0.7421 acc_test: 0.6360 f1_test: 0.6278 precision: 0.6423 recall: 0.6140\n",
      "Epoch: 0700 acc_train: 0.7407 acc_test: 0.6250 f1_test: 0.5520 precision: 0.6855 recall: 0.4620\n",
      "Epoch: 0701 acc_train: 0.7487 acc_test: 0.6470 f1_test: 0.6280 precision: 0.6637 recall: 0.5960\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    train(epoch, args, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
